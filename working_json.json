[[{"repo_name":"lisroach\/PyRESTCONF","path":"rest_calls\/tests\/unit\/test_xml_mock.py","copies":"1","size":"2194","content":"import unittest\r\nimport requests\r\nimport mock\r\n\r\nfrom rest_calls.xmlRestClient import XMLRestCalls\r\n\r\nusername = 'admin'\r\npassword = 'admin'\r\nip = '127.0.0.1'\r\nport = 80\r\nendpoint = 'Cisco-IOS-XR-ip-static-cfg:router-static'\r\nsm_url = '{scheme}:\/\/{ip}:{port}{basePath}\/'.format(\r\n    scheme='http',\r\n    ip=ip,\r\n    port=port,\r\n    basePath='\/restconf\/data'\r\n)\r\nurl = sm_url + endpoint\r\ncontents = '{test: test}'\r\n\r\n\r\nclass XMLRestCallsCase(unittest.TestCase):\r\n    def setUp(self):\r\n        self.classObject = XMLRestCalls(\r\n            ip,\r\n            port,\r\n            username,\r\n            password\r\n        )\r\n\r\n    def test__init__(self):\r\n        \"\"\"Does constructor create a proper object\"\"\"\r\n        session = requests.Session()\r\n        session.headers.update({\r\n            'Accept': ','.join([\r\n                'application\/yang.data+xml',\r\n                'application\/yang.errors+xml',\r\n                ]),\r\n            'Content-Type': 'application\/yang.data+xml',\r\n        })\r\n        self.assertEqual(self.classObject._session.headers,\r\n                         session.headers)\r\n        self.assertEqual(self.classObject._host, sm_url)\r\n\r\n    @mock.patch('requests.sessions.Session.get')\r\n    def test_get(self, mock_get):\r\n        self.classObject.get(endpoint)\r\n        mock_get.assert_called_once_with(url, params={})\r\n\r\n    @mock.patch('requests.sessions.Session.put')\r\n    def test_put(self, mock_put):\r\n        self.classObject.put(contents, endpoint)\r\n        mock_put.assert_called_once_with(url, data=contents)\r\n\r\n    @mock.patch('requests.sessions.Session.patch')\r\n    def test_patch(self, mock_patch):\r\n        self.classObject.patch(contents, endpoint)\r\n        mock_patch.assert_called_once_with(url, data=contents)\r\n\r\n    @mock.patch('requests.sessions.Session.post')\r\n    def test_post(self, mock_post):\r\n        self.classObject.post(contents, endpoint)\r\n        mock_post.assert_called_once_with(url, data=contents)\r\n\r\n    @mock.patch('requests.sessions.Session.delete')\r\n    def test_delete(self, mock_delete):\r\n        self.classObject.delete(endpoint)\r\n        mock_delete.assert_called_once_with(url)\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n","license":"apache-2.0","hash":5605707288026969549,"line_mean":28.9014084507,"line_max":62,"alpha_frac":0.6020966272,"autogenerated":false},
{"repo_name":"igatoolsProject\/igatools","path":"source\/geometry\/grid.inst.py","copies":"1","size":"2551","content":"#-+--------------------------------------------------------------------\n# Igatools a general purpose Isogeometric analysis library.\n# Copyright (C) 2012-2016  by the igatools authors (see authors.txt).\n#\n# This file is part of the igatools library.\n#\n# The igatools library is free software: you can use it, redistribute\n# it and\/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, either\n# version 3 of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#-+--------------------------------------------------------------------\n\nfrom init_instantiation_data import *\n\ninclude_files = ['geometry\/grid_element.h']\ndata = Instantiation(include_files)\n(f, inst) = (data.file_output, data.inst)\n\nsub_dim_members = ['Grid<dim>::template BoundaryNormals<k> ' +\n                   'Grid<dim>::get_boundary_normals<k>(const int s_id) const;',\n                   'std::shared_ptr<Grid<k>> Grid<dim>::'+\n                   'get_sub_grid<k>(const int sub_elem_id, typename Grid<dim>::template SubGridMap<k> &elem_map) const;']\n\ngrids = [] \nfor dim in inst.sub_domain_dims:\n    grid = 'Grid<%d>' %(dim)\n    grids.append(grid)\n    f.write('template class %s; \\n' % (grid))\n    for fun in sub_dim_members:\n        for k in range(0,max(dim-1,0)+1):\n#        k = dim\n            s = fun.replace('k', '%d' % (k)).replace('dim', '%d' % (dim));\n            f.write('template ' + s + '\\n')  \n    \nfor dim in inst.domain_dims:\n    grid = 'Grid<%d>' %(dim)   \n    grids.append(grid)\n    f.write('template class %s; \\n' % (grid))\n    for fun in sub_dim_members:\n        for k in range(0,max(dim-1,0)+1):\n#        for k in inst.sub_dims(dim):\n            s = fun.replace('k', '%d' % (k)).replace('dim', '%d' % (dim));\n            f.write('template ' + s + '\\n')\n       \n\n\n\n#---------------------------------------------------\nf.write('#ifdef IGATOOLS_WITH_SERIALIZATION\\n')\narchives = ['OArchive','IArchive']\n\nfor grid in unique(grids):\n    for ar in archives:\n        f.write('template void %s::serialize(%s&);\\n' %(grid,ar))\nf.write('#endif \/\/ IGATOOLS_WITH_SERIALIZATION\\n')\n#---------------------------------------------------\n","license":"gpl-3.0","hash":-6941081620431806906,"line_mean":38.859375,"line_max":121,"alpha_frac":0.5711485692,"autogenerated":false},
{"repo_name":"thundernet8\/Plog","path":"app\/core\/main\/views.py","copies":"1","size":"4256","content":"# coding=utf-8\n\nfrom datetime import datetime\nimport json\n\nfrom flask import current_app\nfrom flask import send_file\nfrom flask import request\nfrom flask import render_template\nfrom flask import session\nfrom flask import redirect\nfrom flask import abort\nfrom flask.ext.login import current_user\n\nfrom . import main\nfrom app.core.models.helpers.redis_cache_decorator import redis_cached\nfrom app.core.models.settings import Setting\nfrom app.core.models.posts import Post\nfrom app.core.models.posts import Posts\nfrom app.core.models.users import User\nfrom app.core.models.tags import Tag\nfrom .forms import CommentForm\n\n\n@main.route('\/favicon.ico')\ndef favicon():\n    \"\"\"\n    \u6536\u85cf\u5939\u680f\u56fe\u6807\n    :return:\n    \"\"\"\n    return send_file('static\/dist\/images\/favicon.ico', as_attachment=False)\n\n\n@main.route('\/kill-ie.html')\ndef kill_ie():\n    \"\"\"\n    kill ie\n    :return:\n    \"\"\"\n    return render_template('utils\/kill-ie.html', blog_name=Setting.get_setting('blog_name', 'Plog'))\n\n\n# \u641c\u7d22\n@main.route('\/search', methods=['GET', 'POST'])\ndef search():\n    if request.method == 'POST':\n        s = request.form.get('s')\n        return redirect('\/search?s='+s)\n    else:\n        s = request.args.get('s')\n    return s  # TODO search\n\n\n# \u9996\u9875\n@main.route('\/')\n@redis_cached(timeout=30, key_prefix='home_html')  # TODO \u7f13\u5b58\u65f6\u95f4\ndef index():\n    pagenation = Posts(filters={'status': 'published'}).pagination()\n    posts = pagenation.items if pagenation else []\n    return render_template('home.html', posts=posts, pagenation=pagenation)\n\n\n# \u9996\u9875(\u5e26\u5206\u9875)\n@main.route('\/page\/<int:page>')\n@redis_cached(timeout=30, key_prefix='home_html_%s')\ndef index_paged(page):\n    pagenation = Posts(filters={'status': 'published'}).pagination(page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('home.html', posts=posts, pagenation=pagenation)\n\n\n# \u6587\u7ae0\u8be6\u60c5\u9875\n@main.route('\/article\/<int:post_id>.html')\n@redis_cached(timeout=30, key_prefix='article_%s')\ndef article_detail(post_id):\n    post = Post.get_post(post_id)\n    if not post or not post.post_id or post.status != 'published':\n        abort(404)\n    author = User(user_id=post.author_id)\n    comment_form = CommentForm()\n    return render_template('article.html', post=post, author=author, comment_form=comment_form)\n\n\n# \u7528\u6237\/\u4f5c\u8005\u4e3b\u9875\n@main.route('\/author\/<int:user_id>')\n@redis_cached(timeout=300, key_prefix='author_%s')\ndef user_homepage(user_id):\n    author = User.get_user_by_id(user_id)\n    if not author:\n        abort(404)\n    pagenation = User.get_user_posts_pagenation(user_id)\n    posts = pagenation.items if pagenation else []\n    return render_template('author.html', author=author, posts=posts, pagenation=pagenation)  # TODO \/\u8003\u8651\u4f7f\u7528\u7528\u6237\u540d\u6216\u6635\u79f0\u66ff\u4ee3\u7528\u6237 id \u4f5c\u4e3a\u94fe\u63a5\u6807\u8bc6\n\n\n# \u7528\u6237\/\u4f5c\u8005\u4e3b\u9875(\u5e26\u5206\u9875)\n@main.route('\/author\/<int:user_id>\/page\/<int:page>')\n@redis_cached(timeout=300, key_prefix='author_%s')\ndef user_homepage_paged(user_id, page):\n    author = User.get_user_by_id(user_id)\n    if not author:\n        abort(404)\n    pagenation = User.get_user_posts_pagenation(user_id, page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('author.html', author=author, posts=posts, pagenation=pagenation)\n\n\n# RSS\n@main.route('\/rss')\n@redis_cached(timeout=600, key_prefix='rss')\ndef rss():\n    return 'rss'  # TODO rss\n\n\n# TAG\n@main.route('\/tag\/<int:tag_id>')\n@redis_cached(timeout=600, key_prefix='tag_%s')\ndef tag(tag_id):\n    tag = Tag.get_tag_by_id(tag_id)\n    if not tag:\n        abort(404)\n    pagenation = Tag.get_tag_posts(tag_id)\n    posts = pagenation.items if pagenation else []\n    return render_template('tag.html', tag=tag, posts=posts, pagenation=pagenation)\n\n\n# TAG(\u5e26\u5206\u9875)\n@main.route('\/tag\/<int:tag_id>\/page\/<int:page>')\n@redis_cached(timeout=600, key_prefix='tag_%s')\ndef tag_paged(tag_id, page):\n    tag = Tag.get_tag_by_id(tag_id)\n    if not tag:\n        abort(404)\n    pagenation = Tag.get_tag_posts(tag_id, page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('tag.html', tag=tag, posts=posts, pagenation=pagenation)\n\n\n# 404\n@main.errorhandler(404)\ndef main_404(e):\n    return render_template('error_pages\/404.html'), 404\n\n\n\n\n\n","license":"gpl-3.0","hash":2699200856079543198,"line_mean":27.1088435374,"line_max":126,"alpha_frac":0.6878025169,"autogenerated":false},
{"repo_name":"lmazuel\/azure-sdk-for-python","path":"azure-mgmt-datalake-analytics\/azure\/mgmt\/datalake\/analytics\/account\/models\/compute_policy.py","copies":"1","size":"2664","content":"# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom .sub_resource import SubResource\n\n\nclass ComputePolicy(SubResource):\n    \"\"\"Data Lake Analytics compute policy information.\n\n    Variables are only populated by the server, and will be ignored when\n    sending a request.\n\n    :ivar id: The resource identifier.\n    :vartype id: str\n    :ivar name: The resource name.\n    :vartype name: str\n    :ivar type: The resource type.\n    :vartype type: str\n    :ivar object_id: The AAD object identifier for the entity to create a\n     policy for.\n    :vartype object_id: str\n    :ivar object_type: The type of AAD object the object identifier refers to.\n     Possible values include: 'User', 'Group', 'ServicePrincipal'\n    :vartype object_type: str or\n     ~azure.mgmt.datalake.analytics.account.models.AADObjectType\n    :ivar max_degree_of_parallelism_per_job: The maximum degree of parallelism\n     per job this user can use to submit jobs.\n    :vartype max_degree_of_parallelism_per_job: int\n    :ivar min_priority_per_job: The minimum priority per job this user can use\n     to submit jobs.\n    :vartype min_priority_per_job: int\n    \"\"\"\n\n    _validation = {\n        'id': {'readonly': True},\n        'name': {'readonly': True},\n        'type': {'readonly': True},\n        'object_id': {'readonly': True},\n        'object_type': {'readonly': True},\n        'max_degree_of_parallelism_per_job': {'readonly': True, 'minimum': 1},\n        'min_priority_per_job': {'readonly': True, 'minimum': 1},\n    }\n\n    _attribute_map = {\n        'id': {'key': 'id', 'type': 'str'},\n        'name': {'key': 'name', 'type': 'str'},\n        'type': {'key': 'type', 'type': 'str'},\n        'object_id': {'key': 'properties.objectId', 'type': 'str'},\n        'object_type': {'key': 'properties.objectType', 'type': 'str'},\n        'max_degree_of_parallelism_per_job': {'key': 'properties.maxDegreeOfParallelismPerJob', 'type': 'int'},\n        'min_priority_per_job': {'key': 'properties.minPriorityPerJob', 'type': 'int'},\n    }\n\n    def __init__(self):\n        super(ComputePolicy, self).__init__()\n        self.object_id = None\n        self.object_type = None\n        self.max_degree_of_parallelism_per_job = None\n        self.min_priority_per_job = None\n","license":"mit","hash":5015207261158012778,"line_mean":38.7611940299,"line_max":111,"alpha_frac":0.6024774775,"autogenerated":false},
{"repo_name":"tobetter\/linaro-image-tools_packaging","path":"linaro_image_tools\/utils.py","copies":"1","size":"13217","content":"# Copyright (C) 2010, 2011 Linaro\n#\n# Author: Guilherme Salgado <guilherme.salgado@linaro.org>\n#\n# This file is part of Linaro Image Tools.\n#\n# Linaro Image Tools is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Linaro Image Tools is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Linaro Image Tools.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport platform\nimport subprocess\nimport re\nimport logging\nimport tempfile\nimport tarfile\nimport sys\n\nfrom linaro_image_tools import cmd_runner\n\nDEFAULT_LOGGER_NAME = 'linaro_image_tools'\n\n# The boot path in the boot tarball.\nBOOT_DIR_IN_TARBALL = \"boot\"\n# The name of the hwpack file found in the boot tarball.\nHWPACK_NAME = \"config\"\n\n\n# try_import was copied from python-testtools 0.9.12 and was originally\n# licensed under a MIT-style license but relicensed under the GPL in Linaro\n# Image Tools.\n# Copyright (c) 2011 Jonathan M. Lange <jml@mumak.net>.\ndef try_import(name, alternative=None, error_callback=None):\n    \"\"\"Attempt to import ``name``.  If it fails, return ``alternative``.\n\n    When supporting multiple versions of Python or optional dependencies, it\n    is useful to be able to try to import a module.\n\n    :param name: The name of the object to import, e.g. ``os.path`` or\n        ``os.path.join``.\n    :param alternative: The value to return if no module can be imported.\n        Defaults to None.\n    :param error_callback: If non-None, a callable that is passed the\n        ImportError when the module cannot be loaded.\n    \"\"\"\n    module_segments = name.split('.')\n    last_error = None\n    while module_segments:\n        module_name = '.'.join(module_segments)\n        try:\n            module = __import__(module_name)\n        except ImportError:\n            last_error = sys.exc_info()[1]\n            module_segments.pop()\n            continue\n        else:\n            break\n    else:\n        if last_error is not None and error_callback is not None:\n            error_callback(last_error)\n        return alternative\n    nonexistent = object()\n    for segment in name.split('.')[1:]:\n        module = getattr(module, segment, nonexistent)\n        if module is nonexistent:\n            if last_error is not None and error_callback is not None:\n                error_callback(last_error)\n            return alternative\n    return module\n\n\nCommandNotFound = try_import('CommandNotFound.CommandNotFound')\n\n\ndef path_in_tarfile_exists(path, tar_file):\n    exists = True\n    try:\n        tarinfo = tarfile.open(tar_file, 'r:gz')\n        tarinfo.getmember(path)\n        tarinfo.close()\n    except KeyError:\n        exists = False\n    finally:\n        return exists\n\n\ndef verify_file_integrity(sig_file_list):\n    \"\"\"Verify a list of signature files.\n\n    The parameter is a list of filenames of gpg signature files which will be\n    verified using gpg. For each of the files it is assumed that there is an\n    sha1 hash file with the same file name minus the '.asc' extension.\n\n    Each of the sha1 files will be checked using sha1sums. All files listed in\n    the sha1 hash file must be found in the same directory as the hash file.\n    \"\"\"\n\n    gpg_sig_ok = True\n    gpg_out = \"\"\n\n    verified_files = []\n    for sig_file in sig_file_list:\n        hash_file = sig_file[0:-len('.asc')]\n        tmp = tempfile.NamedTemporaryFile()\n\n        try:\n            cmd_runner.run(['gpg', '--status-file={0}'.format(tmp.name),\n                            '--verify', sig_file]).wait()\n        except cmd_runner.SubcommandNonZeroReturnValue:\n            gpg_sig_ok = False\n            gpg_out = gpg_out + tmp.read()\n\n        tmp.close()\n\n        if os.path.dirname(hash_file) == '':\n            sha_cwd = None\n        else:\n            sha_cwd = os.path.dirname(hash_file)\n\n        try:\n            sha1sums_out, _ = cmd_runner.Popen(\n                ['sha1sum', '-c', hash_file],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                cwd=sha_cwd\n            ).communicate()\n        except cmd_runner.SubcommandNonZeroReturnValue as inst:\n            sha1sums_out = inst.stdout\n\n        for line in sha1sums_out.splitlines():\n            sha1_check = re.search(r'^(.*):\\s+OK', line)\n            if sha1_check:\n                verified_files.append(sha1_check.group(1))\n\n    return verified_files, gpg_sig_ok, gpg_out\n\n\ndef check_file_integrity_and_log_errors(sig_file_list, binary, hwpacks):\n    \"\"\"\n    Wrapper around verify_file_integrity that prints error messages to stderr\n    if verify_file_integrity finds any problems.\n    \"\"\"\n    verified_files, gpg_sig_pass, _ = verify_file_integrity(sig_file_list)\n\n    # Check the outputs from verify_file_integrity\n    # Abort if anything fails.\n    logger = logging.getLogger(__name__)\n    if len(sig_file_list):\n        if not gpg_sig_pass:\n            logger.error(\"GPG signature verification failed.\")\n            return False, []\n\n        if not os.path.basename(binary) in verified_files:\n            logger.error(\"OS Binary verification failed\")\n            return False, []\n\n        for hwpack in hwpacks:\n            if not os.path.basename(hwpack) in verified_files:\n                logger.error(\"Hwpack {0} verification failed\".format(hwpack))\n                return False, []\n\n        for verified_file in verified_files:\n            logger.info('Hash verification of file {0} OK.'.format(\n                verified_file))\n    return True, verified_files\n\n\ndef install_package_providing(command):\n    \"\"\"Install a package which provides the given command.\n\n    If we can't find any package which provides it, raise\n    UnableToFindPackageProvidingCommand.\n\n    If the user denies installing the package, the program exits.\n    \"\"\"\n\n    if CommandNotFound is None:\n        raise UnableToFindPackageProvidingCommand(\n            \"CommandNotFound python module does not exist.\")\n\n    packages = CommandNotFound().getPackages(command)\n    if len(packages) == 0:\n        raise UnableToFindPackageProvidingCommand(\n            \"Unable to find any package providing %s\" % command)\n\n    # TODO: Ask the user to pick a package when there's more than one that\n    # provides the given command.\n    package, _ = packages[0]\n    output, _ = cmd_runner.run(['apt-get', '-s', 'install', package],\n                               stdout=subprocess.PIPE).communicate()\n    to_install = []\n    for line in output.splitlines():\n        if line.startswith(\"Inst\"):\n            to_install.append(line.split()[1])\n    if not to_install:\n        raise UnableToFindPackageProvidingCommand(\n            \"Unable to find any package to be installed.\")\n\n    try:\n        print (\"In order to use the '%s' command, the following package\/s \"\n               \"have to be installed: %s\" % (command, \" \".join(to_install)))\n        resp = raw_input(\"Install? (Y\/n) \")\n        if resp.lower() != 'y':\n            print \"Package installation is necessary to continue. Exiting.\"\n            sys.exit(1)\n        print (\"Installing required command '%s' from package '%s'...\"\n               % (command, package))\n        cmd_runner.run(['apt-get', '--yes', 'install', package],\n                       as_root=True).wait()\n    except EOFError:\n        raise PackageInstallationRefused(\n            \"Package installation interrupted: input error.\")\n    except KeyboardInterrupt:\n        raise PackageInstallationRefused(\n            \"Package installation interrupted by the user.\")\n\n\ndef has_command(command):\n    \"\"\"Check the given command is available.\"\"\"\n    try:\n        cmd_runner.run(\n            ['which', command], stdout=open('\/dev\/null', 'w')).wait()\n        return True\n    except cmd_runner.SubcommandNonZeroReturnValue:\n        return False\n\n\ndef ensure_command(command):\n    \"\"\"Ensure the given command is available.\n\n    If it's not, look up a package that provides it and install that.\n    \"\"\"\n    if not has_command(command):\n        install_package_providing(command)\n\n\ndef find_command(name, prefer_dir=None):\n    \"\"\"Finds a linaro-image-tools command.\n\n    Prefers specified directory, otherwise searches only the current directory\n    when running from a checkout, or only PATH when running from an installed\n    version.\n    \"\"\"\n    assert name != \"\"\n    assert os.path.dirname(name) == \"\"\n\n    cmd_runner.sanitize_path(os.environ)\n\n    # default to searching in current directory when running from a bzr\n    # checkout\n    dirs = [os.getcwd(), ]\n    if os.path.isabs(__file__):\n        dirs = os.environ[\"PATH\"].split(os.pathsep)\n        # empty dir in PATH means current directory\n        dirs = map(lambda x: x == '' and '.' or x, dirs)\n\n    if prefer_dir is not None:\n        dirs.insert(0, prefer_dir)\n\n    for dir in dirs:\n        path = os.path.join(dir, name)\n        if os.path.exists(path) and os.access(path, os.X_OK):\n            return path\n\n    return None\n\n\ndef is_arm_host():\n    return platform.machine().startswith('arm')\n\n\ndef preferred_tools_dir():\n    prefer_dir = None\n    # running from bzr checkout?\n    if not os.path.isabs(__file__):\n        prefer_dir = os.getcwd()\n    return prefer_dir\n\n\ndef prep_media_path(args):\n    if args.directory is not None:\n        loc = os.path.abspath(args.directory)\n        try:\n            os.makedirs(loc)\n        except OSError:\n            # Directory exists.\n            pass\n\n        path = os.path.join(loc, args.device)\n    else:\n        path = args.device\n\n    return path\n\n\nclass UnableToFindPackageProvidingCommand(Exception):\n    \"\"\"We can't find a package which provides the given command.\"\"\"\n\n\nclass PackageInstallationRefused(Exception):\n    \"\"\"User has chosen not to install a package.\"\"\"\n\n\nclass InvalidHwpackFile(Exception):\n    \"\"\"The hwpack parameter is not a regular file.\"\"\"\n\n\nclass MissingRequiredOption(Exception):\n    \"\"\"A required option from the command line is missing.\"\"\"\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)\n\n\nclass IncompatibleOptions(Exception):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)\n\n\ndef additional_option_checks(args):\n    if args.directory is not None:\n    # If args.device is a path to a device (\/dev\/) then this is an error\n        if \"--mmc\" in sys.argv:\n            raise IncompatibleOptions(\"--directory option incompatible with \"\n                                      \"option --mmc\")\n\n        # If directory is used as well as having a full path (rather than just\n        # a file name or relative path) in args.device, this is an error.\n        if re.search(r\"^\/\", args.device):\n            raise IncompatibleOptions(\"--directory option incompatible with \"\n                                      \"a full path in --image-file\")\n\n    for hwpack in args.hwpacks:\n        if not os.path.isfile(hwpack):\n            raise InvalidHwpackFile(\n                \"--hwpack argument (%s) is not a regular file\" % hwpack)\n\n\ndef additional_android_option_checks(args):\n    \"\"\"Checks that some of the args passed to l-a-m-c are valid.\"\"\"\n    if args.hwpack:\n        if not os.path.isfile(args.hwpack):\n            raise InvalidHwpackFile(\n                \"--hwpack argument (%s) is not a regular file\" % args.hwpack)\n\n\ndef andorid_hwpack_in_boot_tarball(boot_dir):\n    \"\"\"Simple check for existence of a path.\n\n    Needed to make cli command testable in some way.\n    :param boot_dir: The path where the boot tarball has been extracted.\n    :type str\n    :return A tuple with a bool if the path exists, and the path to the config\n            file.\n    \"\"\"\n    conf_file = os.path.join(boot_dir, BOOT_DIR_IN_TARBALL, HWPACK_NAME)\n    return os.path.exists(conf_file), conf_file\n\n\ndef check_required_args(args):\n    \"\"\"Check that the required args are passed.\"\"\"\n    if args.dev is None:\n        raise MissingRequiredOption(\"--dev option is required\")\n    if args.binary is None:\n        raise MissingRequiredOption(\"--binary option is required\")\n\n\ndef get_logger(name=DEFAULT_LOGGER_NAME, debug=False):\n    \"\"\"\n    Retrieves a named logger. Default name is set in the variable\n    DEFAULT_LOG_NAME. Debug is set to False by default.\n\n    :param name: The name of the logger.\n    :param debug: If debug level should be turned on\n    :return: A logger instance.\n    \"\"\"\n    logger = logging.getLogger(name)\n    ch = logging.StreamHandler()\n\n    if debug:\n        ch.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        ch.setFormatter(formatter)\n        logger.setLevel(logging.DEBUG)\n    else:\n        ch.setLevel(logging.INFO)\n        formatter = logging.Formatter(\"%(message)s\")\n        ch.setFormatter(formatter)\n        logger.setLevel(logging.INFO)\n\n    logger.addHandler(ch)\n    return logger\n","license":"gpl-3.0","hash":8347262142669630789,"line_mean":31.3946078431,"line_max":78,"alpha_frac":0.6335023076,"autogenerated":false},
{"repo_name":"cbertinato\/pandas","path":"pandas\/io\/date_converters.py","copies":"1","size":"1865","content":"\"\"\"This module is designed for community supported date conversion functions\"\"\"\nimport numpy as np\n\nfrom pandas._libs.tslibs import parsing\n\n\ndef parse_date_time(date_col, time_col):\n    date_col = _maybe_cast(date_col)\n    time_col = _maybe_cast(time_col)\n    return parsing.try_parse_date_and_time(date_col, time_col)\n\n\ndef parse_date_fields(year_col, month_col, day_col):\n    year_col = _maybe_cast(year_col)\n    month_col = _maybe_cast(month_col)\n    day_col = _maybe_cast(day_col)\n    return parsing.try_parse_year_month_day(year_col, month_col, day_col)\n\n\ndef parse_all_fields(year_col, month_col, day_col, hour_col, minute_col,\n                     second_col):\n    year_col = _maybe_cast(year_col)\n    month_col = _maybe_cast(month_col)\n    day_col = _maybe_cast(day_col)\n    hour_col = _maybe_cast(hour_col)\n    minute_col = _maybe_cast(minute_col)\n    second_col = _maybe_cast(second_col)\n    return parsing.try_parse_datetime_components(year_col, month_col, day_col,\n                                                 hour_col, minute_col,\n                                                 second_col)\n\n\ndef generic_parser(parse_func, *cols):\n    N = _check_columns(cols)\n    results = np.empty(N, dtype=object)\n\n    for i in range(N):\n        args = [c[i] for c in cols]\n        results[i] = parse_func(*args)\n\n    return results\n\n\ndef _maybe_cast(arr):\n    if not arr.dtype.type == np.object_:\n        arr = np.array(arr, dtype=object)\n    return arr\n\n\ndef _check_columns(cols):\n    if not len(cols):\n        raise AssertionError(\"There must be at least 1 column\")\n\n    head, tail = cols[0], cols[1:]\n\n    N = len(head)\n\n    for i, n in enumerate(map(len, tail)):\n        if n != N:\n            raise AssertionError('All columns must have the same length: {0}; '\n                                 'column {1} has length {2}'.format(N, i, n))\n\n    return N\n","license":"bsd-3-clause","hash":8620790840864050110,"line_mean":28.6031746032,"line_max":79,"alpha_frac":0.5962466488,"autogenerated":false},
{"repo_name":"Digilent\/u-boot-digilent","path":"test\/py\/tests\/test_i2c.py","copies":"1","size":"1946","content":"# Copyright (c) 2015 Stephen Warren\n#\n# SPDX-License-Identifier: GPL-2.0\n\nimport pytest\nimport random\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_bus(u_boot_console):\n    expected_response = \"Bus\"\n    response = u_boot_console.run_command(\"i2c bus\")\n    assert(expected_response in response)\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_dev(u_boot_console):\n    expected_response = \"Current bus\"\n    response = u_boot_console.run_command(\"i2c dev\")\n    assert(expected_response in response)\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe(u_boot_console):\n    expected_response = \"Valid chip addresses:\"\n    response = u_boot_console.run_command(\"i2c probe\")\n    assert(expected_response in response)\n\n@pytest.mark.boardidentity(\"!qemu\")\n@pytest.mark.boardspec(\"zynq_zc702\")\n@pytest.mark.boardspec(\"zynq_zc706\")\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe_zc70x(u_boot_console):\n    # Enable i2c mux bridge\n    u_boot_console.run_command(\"i2c mw 74 0 4\")\n    u_boot_console.run_command(\"i2c probe\")\n    val = format(random.randint(0,255), '02x')\n    u_boot_console.run_command(\"i2c mw 54 0 \" + val + \" 5\")\n    response = u_boot_console.run_command(\"i2c md 54 0 5\")\n    expected_response = \"0000: \" + val + \" \" + val + \" \" + val + \" \" + val + \" \" + val + \" \"\n    assert(expected_response in response)\n\n@pytest.mark.boardspec(\"xilinx_zynqmp_zcu102\")\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe_zcu102(u_boot_console):\n    # This is using i2c mux wiring from config file\n    u_boot_console.run_command(\"i2c dev 5\")\n    u_boot_console.run_command(\"i2c probe\")\n    val = format(random.randint(0,255), '02x')\n    u_boot_console.run_command(\"i2c mw 54 0 \" + val + \" 5\")\n    response = u_boot_console.run_command(\"i2c md 54 0 5\")\n    expected_response = \"0000: \" + val + \" \" + val + \" \" + val + \" \" + val + \" \" + val + \" \"\n    print expected_response\n    assert(expected_response in response)\n","license":"gpl-2.0","hash":-4781732101770132973,"line_mean":37.1568627451,"line_max":92,"alpha_frac":0.674717369,"autogenerated":false},
{"repo_name":"JeffHoogland\/qtdesigner-pyside-tutorial","path":"tut1-mainwindow\/ui_mainWindow.py","copies":"2","size":"1602","content":"# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'mainWindow.ui'\n#\n# Created: Thu Mar 12 15:46:01 2015\n#      by: pyside-uic 0.2.15 running on PySide 1.2.1\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PySide import QtCore, QtGui\n\nclass Ui_mainWindow(object):\n    def setupUi(self, mainWindow):\n        mainWindow.setObjectName(\"mainWindow\")\n        mainWindow.resize(376, 207)\n        self.centralwidget = QtGui.QWidget(mainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.verticalLayout = QtGui.QVBoxLayout(self.centralwidget)\n        self.verticalLayout.setObjectName(\"verticalLayout\")\n        self.goText = QtGui.QTextEdit(self.centralwidget)\n        self.goText.setEnabled(True)\n        self.goText.setTextInteractionFlags(QtCore.Qt.TextSelectableByKeyboard|QtCore.Qt.TextSelectableByMouse)\n        self.goText.setObjectName(\"goText\")\n        self.verticalLayout.addWidget(self.goText)\n        self.goButton = QtGui.QPushButton(self.centralwidget)\n        self.goButton.setObjectName(\"goButton\")\n        self.verticalLayout.addWidget(self.goButton)\n        mainWindow.setCentralWidget(self.centralwidget)\n\n        self.retranslateUi(mainWindow)\n        QtCore.QMetaObject.connectSlotsByName(mainWindow)\n\n    def retranslateUi(self, mainWindow):\n        mainWindow.setWindowTitle(QtGui.QApplication.translate(\"mainWindow\", \"Qt Designer MainWindow Example\", None, QtGui.QApplication.UnicodeUTF8))\n        self.goButton.setText(QtGui.QApplication.translate(\"mainWindow\", \"Go Button\", None, QtGui.QApplication.UnicodeUTF8))\n\n","license":"bsd-3-clause","hash":-2359122869957934119,"line_mean":43.5,"line_max":149,"alpha_frac":0.7322097378,"autogenerated":false},
{"repo_name":"bburan\/psiexperiment","path":"psi\/context\/expression.py","copies":"1","size":"3158","content":"import logging\r\nlog = logging.getLogger(__name__)\r\n\r\nfrom atom.api import Atom, Typed\r\nfrom psi.util import get_dependencies\r\n\r\n\r\nclass Expr(object):\r\n\r\n    def __init__(self, expression):\r\n        if not isinstance(expression, str):\r\n            raise ValueError('Expression must be a string')\r\n        if not expression:\r\n            raise ValueError('No value provided for expression')\r\n        self._expression = expression\r\n        self._code = compile(expression, 'dynamic', 'eval')\r\n        self._dependencies = get_dependencies(expression)\r\n\r\n    def evaluate(self, context):\r\n        return eval(self._expression, context)\r\n\r\n\r\nclass ExpressionNamespace(Atom):\r\n\r\n    _locals = Typed(dict, {})\r\n    _expressions = Typed(dict, {})\r\n    _globals = Typed(dict, {})\r\n\r\n    def __init__(self, expressions=None, globals=None):\r\n        if globals is None:\r\n            globals = {}\r\n        if expressions is None:\r\n            expressions = {}\r\n        self._locals = {}\r\n        self._globals = globals\r\n        self._expressions = {k: Expr(str(v)) for k, v in expressions.items()}\r\n\r\n    def update_expressions(self, expressions):\r\n        self._expressions.update({k: Expr(str(v)) for k, v in expressions.items()})\r\n\r\n    def update_symbols(self, symbols):\r\n        self._globals.update(symbols)\r\n\r\n    def reset(self, context_item_names=None):\r\n        '''\r\n        Clears the computed values (as well as any user-provided values) in\r\n        preparation for the next cycle.\r\n        '''\r\n        self._locals = {}\r\n\r\n    def get_value(self, name, context=None):\r\n        if name not in self._locals:\r\n            self._evaluate_value(name, context)\r\n        return self._locals[name]\r\n\r\n    def get_values(self, names=None, context=None):\r\n        if names is None:\r\n            names = self._expressions.keys()\r\n        for name in names:\r\n            if name not in self._locals:\r\n                self._evaluate_value(name, context)\r\n        return dict(self._locals.copy())\r\n\r\n    def set_value(self, name, value):\r\n        _locals = self._locals.copy()\r\n        _locals[name] = value\r\n        self._locals = _locals\r\n\r\n    def set_values(self, values):\r\n        _locals = self._locals.copy()\r\n        _locals.update(values)\r\n        self._locals = _locals\r\n\r\n    def _evaluate_value(self, name, context=None):\r\n        if context is None:\r\n            context = {}\r\n\r\n        if name in context:\r\n            self._locals[name] = context[name]\r\n            return\r\n\r\n        expr = self._expressions[name]\r\n        c = self._globals.copy()\r\n        c.update(self._locals)\r\n        c.update(context)\r\n\r\n        # Build a context dictionary containing the dependencies required for\r\n        # evaluating the expression.\r\n        for d in expr._dependencies:\r\n            if d not in c and d in self._expressions:\r\n                c[d] = self.get_value(d, c)\r\n        \r\n        # Note that in the past I was forcing a copy of self._locals to ensure\r\n        # that the GUI was updated as needed; however, this proved to be a very\r\n        # slow process since it triggered a cascade of GUI updates. \r\n        self._locals[name] = expr.evaluate(c)\r\n","license":"mit","hash":6478125211385565033,"line_mean":30.8958333333,"line_max":83,"alpha_frac":0.5782140595,"autogenerated":false},
{"repo_name":"abrt\/faf","path":"src\/pyfaf\/utils\/contextmanager.py","copies":"1","size":"1078","content":"import sys\nfrom contextlib import contextmanager\n\nfrom io import StringIO\nfrom typing import Generator\nfrom _io import TextIOWrapper\n\n\n@contextmanager\ndef captured_output() -> Generator[TextIOWrapper, None, None]:\n    \"\"\"\n    Capture stdout and stderr output of the executed block\n\n    Example:\n\n    with captured_output() as (out, err):\n        foo()\n    \"\"\"\n\n    new_out, new_err = StringIO(), StringIO()\n    old_out, old_err = sys.stdout, sys.stderr\n    try:\n        sys.stdout, sys.stderr = new_out, new_err\n        yield sys.stdout, sys.stderr\n    finally:\n        sys.stdout, sys.stderr = old_out, old_err\n\n\n@contextmanager\ndef captured_output_combined() -> Generator[TextIOWrapper, None, None]:\n    \"\"\"\n    Capture stdout and stderr combined output of the executed block\n\n    Example:\n\n    with captured_output_combined() as out:\n        foo()\n    \"\"\"\n\n    new_out = StringIO()\n    old_out, old_err = sys.stdout, sys.stderr\n    try:\n        sys.stdout, sys.stderr = new_out, new_out\n        yield sys.stdout\n    finally:\n        sys.stdout, sys.stderr = old_out, old_err\n","license":"gpl-3.0","hash":-6979362351339216327,"line_mean":22.4347826087,"line_max":71,"alpha_frac":0.6484230056,"autogenerated":false},
{"repo_name":"eirmag\/weboob","path":"modules\/okc\/browser.py","copies":"1","size":"8451","content":"# -*- coding: utf-8 -*-\n\n# Copyright(C) 2012 Roger Philibert\n#\n# This file is part of weboob.\n#\n# weboob is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# weboob is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with weboob. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport urllib\n\nfrom weboob.tools.browser import BaseBrowser, BasePage\nfrom weboob.tools.ordereddict import OrderedDict\n\nfrom .pages import LoginPage, ThreadPage, MessagesPage, PostMessagePage, ProfilePage, PhotosPage\n\n__all__ = ['OkCBrowser']\n\n\nclass OkCException(Exception):\n    pass\n\nclass OkCBrowser(BaseBrowser):\n    DOMAIN = 'm.okcupid.com'\n    PROTOCOL = 'https'\n    ENCODING = 'UTF-8'\n    PAGES = OrderedDict((\n            ('https:\/\/%s\/login.*' % DOMAIN, LoginPage),\n            ('http:\/\/%s\/home' % DOMAIN, BasePage),\n            ('http:\/\/%s\/messages' % DOMAIN, ThreadPage),\n            ('http:\/\/%s\/messages\\?compose=1' % DOMAIN, PostMessagePage),\n            ('http:\/\/%s\/messages\\?.*' % DOMAIN, MessagesPage),\n            ('http:\/\/%s\/profile\/.*\/photos' % DOMAIN, PhotosPage),\n            ('http:\/\/%s\/profile\/[^\/]*' % DOMAIN, ProfilePage),\n    ))\n\n\n    logged_in = False\n\n    def home(self):\n        self.location(self.absurl('\/home'))\n\n    def login(self):\n        self.location(self.absurl('\/login'), no_login=True)\n        self.page.login(self.username, self.password)\n        self.logged_in = True\n\n    def is_logged(self):\n        return self.logged_in\n\n    def check_login(func):\n        def inner(self, *args, **kwargs):\n            if not self.logged_in:\n                self.login()\n            return func(self, *args, **kwargs)\n        return inner\n\n    def get_consts(self):\n        return { 'conts' : 'blah' }\n    #    if self.consts is not None:\n    #        return self.consts\n\n    #    self.consts = []\n    #    for i in xrange(2):\n    #        r = self.api_request('me', 'all_values', data={'sex': i})\n    #        self.consts.append(r['result']['values'])\n\n    #    return self.consts\n\n    #@check_login\n    #def score(self):\n    #    #r = self.api_request('member', 'view', data={'id': self.my_id})\n    #    return int(r['result']['member']['popu']['popu'])\n\n    def get_my_name(self):\n        return self.username\n\n    #@check_login\n    #def nb_new_mails(self):\n    #    r = self.api_request('me', '[default]')\n    #    return r['result']['news']['newMails']\n\n    #@check_login\n    #def nb_new_baskets(self):\n    #    r = self.api_request('me', '[default]')\n    #    return r['result']['news']['newBaskets']\n\n    #@check_login\n    #def nb_new_visites(self):\n    #    r = self.api_request('me', '[default]')\n    #    return r['result']['news']['newVisits']\n\n    #@check_login\n    #def nb_available_charms(self):\n    #    r = self.login()\n    #    return r['result']['flashs']\n\n    #@check_login\n    #def nb_godchilds(self):\n    #    r = self.api_request('member', 'view', data={'id': self.my_id})\n    #    return int(r['result']['member']['popu']['invits'])\n\n    #@check_login\n    #def get_baskets(self):\n    #    r = self.api_request('me', 'basket')\n    #    return r['result']['basket']\n\n    #@check_login\n    #def get_flashs(self):\n    #    r = self.api_request('me', 'flashs')\n    #    return r['result']['all']\n\n    #@check_login\n    #def get_visits(self):\n    #    r = self.api_request('me', 'visits')\n    #    return r['result']['news'] + r['result']['olds']\n\n    @check_login\n    def get_threads_list(self, count=30):\n        self.location('http:\/\/m.okcupid.com\/messages')\n        return self.page.get_threads()\n\n    @check_login\n    def get_thread_mails(self, id, count=30):\n        id = int(id)\n        self.location(self.absurl('\/messages?readmsg=true&threadid=%i&folder=1' % id))\n\n        # Find the peer username\n        mails = self.page.get_thread_mails(count)\n        for mail in mails['messages']:\n            if mail['id_from'] != self.get_my_name():\n                mails['member']['pseudo'] = mail['id_from']\n                break\n        return mails\n\n    @check_login\n    def post_mail(self, id, content):\n        self.location(self.absurl('\/messages?compose=1'))\n        self.page.post_mail(id, content)\n\n    @check_login\n    def post_reply(self, thread_id, content):\n        self.location(self.absurl('\/messages?readmsg=true&threadid=%s&folder=1' % thread_id))\n        username, key = self.page.get_post_params()\n        data = urllib.urlencode({\n            'ajax' : 1,\n            'sendmsg' : 1,\n            'r1' : username,\n            'subject' : '',\n            'body' : content,\n            'threadid' : thread_id,\n            'authcode' : key,\n            'reply' : 1,\n            })\n        self.addheaders = [('Referer', self.page.url), ('Content-Type', 'application\/x-www-form-urlencoded')]\n        self.open('http:\/\/m.okcupid.com\/mailbox', data=data)\n\n    #@check_login\n    #@url2id\n    #def delete_thread(self, id):\n    #    r = self.api_request('message', 'delete', data={'id_user': id})\n    #    self.logger.debug('Thread deleted: %r' % r)\n\n    #@check_login\n    #@url2id\n    #def send_charm(self, id):\n    #    try:\n    #        self.api_request('member', 'addBasket', data={'id': id})\n    #    except AuMException:\n    #        return False\n    #    else:\n    #        return True\n\n    #def search_profiles(self, **kwargs):\n    #    if self.search_query is None:\n    #        r = self.api_request('searchs', '[default]')\n    #        self.search_query = r['result']['search']['query']\n\n    #    params = {}\n    #    for key, value in json.loads(self.search_query).iteritems():\n    #        if isinstance(value, dict):\n    #            for k, v in value.iteritems():\n    #                params['%s%s' % (key, k.capitalize())] = v\n    #        else:\n    #            params[key] = value or ''\n    #    r = self.api_request('searchs', 'advanced', '30,0', params)\n    #    ids = [s['id'] for s in r['result']['search']]\n    #    return set(ids)\n\n    @check_login\n    def get_profile(self, id):\n        self.location(self.absurl('\/profile\/%s' % id))\n        profile = self.page.get_profile()\n        return profile\n\n    @check_login\n    def get_photos(self, id):\n        self.location(self.absurl('\/profile\/%s\/photos' % id))\n        return self.page.get_photos()\n\n    #def _get_chat_infos(self):\n    #    try:\n    #        data = json.load(self.openurl('http:\/\/www.adopteunmec.com\/1.1_cht_get.php?anticache=%f' % random.random()))\n    #    except ValueError:\n    #        raise BrowserUnavailable()\n\n    #    if data['error']:\n    #        raise ChatException(u'Error while getting chat infos. json:\\n%s' % data)\n    #    return data\n\n    #def iter_contacts(self):\n    #    def iter_dedupe(contacts):\n    #        yielded_ids = set()\n    #        for contact in contacts:\n    #            if contact['id'] not in yielded_ids:\n    #                yield contact\n    #            yielded_ids.add(contact['id'])\n\n    #    data = self._get_chat_infos()\n    #    return iter_dedupe(data['contacts'])\n\n    #def iter_chat_messages(self, _id=None):\n    #    data = self._get_chat_infos()\n    #    if data['messages'] is not None:\n    #        for message in data['messages']:\n    #            yield ChatMessage(id_from=message['id_from'], id_to=message['id_to'], message=message['message'], date=message['date'])\n\n    #def send_chat_message(self, _id, message):\n    #    url = 'http:\/\/www.adopteunmec.com\/1.1_cht_send.php?anticache=%f' % random.random()\n    #    data = dict(id=_id, message=message)\n    #    headers = {\n    #            'Content-type': 'application\/x-www-form-urlencoded',\n    #            'Accept': 'text\/plain',\n    #            'Referer': 'http:\/\/www.adopteunmec.com\/chat.php',\n    #            'Origin': 'http:\/\/www.adopteunmec.com',\n    #            }\n    #    request = self.request_class(url, urllib.urlencode(data), headers)\n    #    response = self.openurl(request).read()\n    #    try:\n    #        datetime.datetime.strptime(response,  '%Y-%m-%d %H:%M:%S')\n    #        return True\n    #    except ValueError:\n    #        return False\n","license":"agpl-3.0","hash":-6699884950415688889,"line_mean":32.4031620553,"line_max":136,"alpha_frac":0.5630102946,"autogenerated":false},
{"repo_name":"aniversarioperu\/django-manolo","path":"scrapers\/manolo_scraper\/items.py","copies":"2","size":"1103","content":"# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http:\/\/doc.scrapy.org\/en\/latest\/topics\/items.html\n\nimport scrapy\n\n\nclass ManoloItem(scrapy.Item):\n    sha1 = scrapy.Field()\n    full_name = scrapy.Field()  # Full name of visitor\n    entity = scrapy.Field()  # Entity that the visitor represents\n    meeting_place = scrapy.Field()  # Location where the meeting is held\n    office = scrapy.Field()  # Office that visitor visits, also `unidad`\n    host_name = scrapy.Field()  # Name of person that receives visitor\n    reason = scrapy.Field()  # Reason behind the meeting\n    institution = scrapy.Field()  # institution visited: OSCE, Vivienda?\n    location = scrapy.Field()  # Location of institution\n    id_number = scrapy.Field()\n    id_document = scrapy.Field()  # DNI, Brevete?\n    date = scrapy.Field()  # Should be object or string in format YYYY-mm-dd\n    host_title = scrapy.Field()  # Official title of host person, \"cargo\"\n    time_start = scrapy.Field()\n    time_end = scrapy.Field()\n    created = scrapy.Field()\n    modified = scrapy.Field()\n","license":"bsd-3-clause","hash":999062501576890224,"line_mean":38.3928571429,"line_max":76,"alpha_frac":0.6808703536,"autogenerated":false},
{"repo_name":"redhoyasa\/halallist-scraper","path":"halallist\/spiders\/halal_mui_spider.py","copies":"1","size":"2769","content":"import scrapy\nfrom HTMLParser import HTMLParser\nfrom datetime import datetime\n\n\nclass HalalMUIItem(scrapy.Item):\n    name = scrapy.Field()\n    category = scrapy.Field()\n    certificate_number = scrapy.Field()\n    producer = scrapy.Field()\n    expiration_date = scrapy.Field()\n\n\nclass HalalMUISpider(scrapy.Spider):\n    name = 'halalmui'\n\n    def start_requests(self):\n        urls = ['http:\/\/www.halalmui.org\/mui14\/index.php\/main\/produk_halal_masuk\/1']\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        category_hyperlinks = response.xpath(\n            '\/\/a[contains(@class, \"linkhalal\")]').extract()\n\n        for hyperlink in category_hyperlinks:\n            category_text = hyperlink.split(\"%22\")[8][3:-4]\n            category_url = hyperlink.split(\"%22\")[7]\n\n            # crawl product of each category\n            category_url = category_url + '\/offset\/0'\n            yield scrapy.Request(\n                url=category_url,\n                callback=self.crawl_product,\n                meta={'category': category_text})\n\n    def crawl_product(self, response):\n        # initiate HTML escape characters parser\n        h = HTMLParser()\n\n        # crawl product of each category\n        if self.is_available(response):\n            url = response.url\n\n            old_offset = int(response.url.split('\/')[-1])\n            new_offset = old_offset + 10\n            new_url = url.replace(\n                'offset\/{}'.format(old_offset),\n                'offset\/{}'.format(new_offset)\n            )\n\n            product_rows = response.xpath('\/\/tr\/\/td')\n            for product_row in product_rows:\n                product_details = product_row.xpath('span\/text()').extract()\n                if product_details:\n                    item = HalalMUIItem()\n                    item['name'] = h.unescape(product_row.xpath('span\/h4\/text()').extract_first()).strip()\n                    item['certificate_number'] = product_details[0].split(' : ')[-1].strip()\n                    item['category'] = response.meta['category']\n                    item['producer'] = h.unescape(\n                        product_details[1].split(' : ')[-1]).strip()\n                    item['expiration_date'] = datetime.strptime(\n                        product_details[2].split(' : ')[-1],\n                        '%d %B %Y').date()\n\n                    yield item\n\n            yield scrapy.Request(\n                url=new_url,\n                callback=self.crawl_product,\n                meta={'category': response.meta['category']})\n\n    def is_available(self, response):\n        if not response.xpath('\/\/i\/text()').extract_first() == 'no result found':\n            return True\n        else:\n            return False\n","license":"mit","hash":-7505916903210997472,"line_mean":34.961038961,"line_max":106,"alpha_frac":0.5388226797,"autogenerated":false},
{"repo_name":"amwelch\/mininet-sandbox","path":"bgp-sample\/network.py","copies":"1","size":"5193","content":"#!\/usr\/bin\/env python\n\nfrom mininet.topo import Topo\nfrom mininet.net import Mininet\nfrom mininet.log import lg, info, setLogLevel\nfrom mininet.util import dumpNodeConnections, quietRun, moveIntf\nfrom mininet.cli import CLI\nfrom mininet.node import Switch, OVSKernelSwitch\n\nfrom subprocess import Popen, PIPE, check_output\nfrom time import sleep, time\nfrom multiprocessing import Process\nfrom argparse import ArgumentParser\n\nimport os\nimport sys\n\n#Taken\/Derived from https:\/\/bitbucket.org\/jvimal\/bgp\/overview\n#https:\/\/github.com\/mininet\/mininet\/wiki\/BGP-Path-Hijacking-Attack-Demo\n\nsetLogLevel('info')\n\nclass Router(Switch):\n    \"\"\"Defines a new router that is inside a network namespace so that the\n    individual routing entries don't collide.\n\n    \"\"\"\n    ID = 0\n    def __init__(self, name, **kwargs):\n        kwargs['inNamespace'] = True\n        Switch.__init__(self, name, **kwargs)\n        Router.ID += 1\n        self.switch_id = Router.ID\n\n    @staticmethod\n    def setup():\n        return\n\n    def start(self, controllers):\n        pass\n\n    def stop(self):\n        self.deleteIntfs()\n\n    def log(self, s, col=\"magenta\"):\n        print T.colored(s, col)\n\nclass SimpleTopo(Topo):\n    \"\"\"\n    TOPOLOGY:\n        AS2\n       \/   \\ \n    AS1     AS4\n       \\   \/\n        AS3\n    \"\"\"\n\n    def __init__(self):\n        # Add default members to class.\n        super(SimpleTopo, self ).__init__()\n        num_hosts_per_as = 1\n        num_ases = 4\n        num_hosts = num_hosts_per_as * num_ases\n        # The topology has one router per AS\n        routers = []\n        for i in range(1, num_ases+1):\n            router = self.addSwitch('R%d' % (i))\n            routers.append(router)\n        hosts = []\n        for i in range(1, num_ases+1):\n            router = 'R%d' % (i)\n            for j in range(num_hosts_per_as):\n                hostname = 'h%d_%d' % (i, j)\n                host = self.addNode(hostname)\n                hosts.append(host)\n                self.addLink(router, host)\n  \n        #AS2 has a 1 MBPS link while AS3 has 10 MBPS\n        self.addLink('R1', 'R2')\n        self.addLink('R2', 'R4')\n        self.addLink('R1', 'R3')\n        self.addLink('R3', 'R4')\n\n        return\n\ndef get_host_id(hostname):\n    return int(hostname.split('_')[1])\n\ndef get_host_asn(hostname):\n    asn = int(hostname.split('_')[0].replace('h', ''))\n    return asn\n\ndef get_router_asn(hostname):\n    return int(hostname.replace('R', ''))\n\ndef get_router_ip(hostname):\n    asn = get_router_asn(hostname)\n    ip = '{}.0.1.254'.format(10+asn)\n    return ip\n\ndef getIP(hostname):\n    asn = get_host_asn(hostname)\n    num = get_host_id(hostname)\n    ip = '%s.0.%s.1' % (10+asn, num)\n    return ip\n\ndef getGateway(hostname):\n    asn = get_host_asn(hostname)\n    gw = '%s.0.1.254' % (10+asn)\n    return gw\n\nbird_template = '''\nlog syslog all;\n\nrouter id {local_ip};\n\nprotocol kernel {{\n   import all;\n   export all;\n}}\n{neighbors}\n'''\nneighbor_template = '''\nprotocol bgp as{remote_as} {{\n    import all;\n    export all;\n    local as {local_as};\n    source address {local_ip};\n    neighbor {neighbor_ip} as {remote_as};\n}}\n'''\n\ndef make_directories(d):\n    try:\n        os.makedirs(os.path.dirname(d))\n    except OSError:\n        pass\n    return\n\ndef get_bird_conf(hostname):\n    return '\/usr\/local\/etc\/bird.{}.conf'.format(hostname)\n\ndef write_bgp_conf(hostname, neighbors):\n    local_asn = get_router_asn(hostname)\n    local_ip = get_router_ip(hostname)\n\n    neighbor_buf = ''\n    for neighbor in neighbors:\n        asn = get_router_asn(neighbor)\n        ip = get_router_ip(neighbor)\n        neighbor_buf += neighbor_template.format(remote_as=asn, \n            local_as=local_asn, local_ip=local_ip, neighbor_ip=ip)\n\n    buf = bird_template.format(local_ip=ip, neighbors=neighbor_buf)\n    ofn = get_bird_conf(hostname)\n    make_directories(ofn)\n    with open(ofn, 'w') as fp:\n        fp.write(buf)\n\ndef main():\n    os.system(\"rm -f \/tmp\/R*.log \/tmp\/R*.pid logs\/*\")\n    os.system(\"mn -c >\/dev\/null 2>&1\")\n    os.system(\"killall -9 bird > \/dev\/null 2>&1\")\n\n    net = Mininet(topo=SimpleTopo(), switch=Router)\n    net.start()\n    for router in net.switches:\n        router.cmd(\"sysctl -w net.ipv4.ip_forward=1\")\n        router.waitOutput()\n\n    sleep_time = 1\n    print \"Waiting {} seconds for sysctl changes to take effect...\".format(sleep_time)\n    sleep(sleep_time)\n\n    #Write the bird config files\n    write_bgp_conf('R1', ['R2', 'R3'])\n    write_bgp_conf('R2', ['R1', 'R4'])\n    write_bgp_conf('R3', ['R1', 'R4'])\n    write_bgp_conf('R4', ['R2', 'R3'])\n\n    os.system(\"killall -9 bird\")\n    for router in net.switches:\n\n        router.cmd(\"ifconfig {}-eth1 {}\".format(router.name, get_router_ip(router.name)))\n\n        conf = get_bird_conf(router.name)\n        cmd = \"bird -c {} 2>&1\".format(conf)\n        print \"Starting bird on %s\" % router.name\n        print cmd\n        router.cmd(cmd, shell=True)\n        router.waitOutput()\n\n    for host in net.hosts:\n        host.cmd(\"ifconfig %s-eth0 %s\" % (host.name, getIP(host.name)))\n        host.cmd(\"route add default gw %s\" % (getGateway(host.name)))\n\n    CLI(net)\n    net.stop()\n    os.system(\"killall -9 bird\")\n\n\nif __name__ == \"__main__\":\n    main()\n","license":"mit","hash":2904846336992453141,"line_mean":24.5812807882,"line_max":89,"alpha_frac":0.5948392066,"autogenerated":false},
{"repo_name":"Gebesa-Dev\/Addons-gebesa","path":"invoice_apply_advance_invoice\/models\/account_invoice.py","copies":"1","size":"6423","content":"# -*- coding: utf-8 -*-\n# \u00a9 <YEAR(S)> <AUTHOR(S)>\n# License AGPL-3.0 or later (http:\/\/www.gnu.org\/licenses\/agpl.html).\n\nfrom openerp import api, _, fields, models\nfrom openerp.addons import decimal_precision as dp\nfrom openerp.exceptions import UserError\n\n\nclass AccountInvoice(models.Model):\n    _inherit = 'account.invoice'\n\n    advance_id = fields.Many2one(\n        'account.invoice',\n        string=_('Advance Invoice'),\n    )\n\n    amount_advance = fields.Float(\n        _('Amount Advance'),\n        digits_compute=dp.get_precision('Account'),\n        compute='_compute_amount_adv',\n        store=True,\n    )\n\n    advance_ids = fields.One2many(\n        'account.invoice.rel.advance',\n        'invoice_id',\n        string=(_('Advance')),\n    )\n\n    @api.onchange('advance_id')\n    def _onchange_advance_id(self):\n        advance = self.advance_id\n        if advance:\n            if advance.amount_residual_advance >= self.amount_total:\n                self.amount_advance = self.amount_total\n            else:\n                self.amount_advance = advance.amount_residual_advance\n        return\n\n    @api.multi\n    def update_advance(self):\n        for invoice in self:\n            for advance in invoice.advance_ids:\n                if advance.advance_id:\n                    advance._onchange_advance_id()\n\n    @api.multi\n    def action_cancel(self):\n        res = super(AccountInvoice, self).action_cancel()\n        suma = 0.0\n        for inv in self:\n            for advance in inv.advance_ids:\n                if advance.advance_id:\n                    suma = advance.advance_id.amount_residual_advance + advance.amount_advance\n                    advance.advance_id.amount_residual_advance = suma\n                    advance.advance_id.advance_applied = False\n                    inv.advance_ids = ''\n            return res\n\n    #@api.depends('advance_id')\n    #def _compute_amount_adv(self):\n        #if self.advance_id:\n            #self.amount_advance = self.advance_id.amount_total\n\n    @api.onchange('partner_id', 'company_id')\n    def _onchange_partner_id(self):\n        res = super(AccountInvoice, self)._onchange_partner_id()\n        for partner in self:\n            partner.advance_ids = ''\n\n    @api.multi\n    def action_move_create(self):\n        res = super(AccountInvoice, self).action_move_create()\n        for inv in self:\n            if inv.prepayment_move_ids:\n                advance_invoice_ids = self.prepayment_move_ids.mapped(\n                    'line_ids').mapped('invoice_id')\n                l10n_mx_edi_origin = '07|'\n                for advance_invoice in advance_invoice_ids:\n                    advance_invoice.advance_applied = True\n                    l10n_mx_edi_origin += advance_invoice.cfdi_uuid + ','\n                inv.l10n_mx_edi_origin = l10n_mx_edi_origin[:-1]\n            elif inv.advance_id:\n                inv.advance_id.advance_applied = True\n                inv.l10n_mx_edi_origin = '07|' + inv.advance_id.cfdi_uuid\n            elif inv.advance_ids:\n                total_advance = 0.0\n                resta = 0.0\n                inv.l10n_mx_edi_origin = '07|'\n                for advance in inv.advance_ids:\n                    if advance.advance_id:\n                        if round(advance.amount_advance, 2) <= round(advance.advance_id.amount_residual_advance, 2):\n                            resta = advance.advance_id.amount_residual_advance - advance.amount_advance\n                            advance.advance_id.sudo().write({\n                                'amount_residual_advance': resta\n                            })\n                            if advance.advance_id.amount_residual_advance <= 0.0:\n                                advance.advance_id.sudo().write({\n                                    'advance_applied': True\n                                })\n                        else:\n                            raise UserError('El monto de anticipo es mayor al saldo de la factura %s' % advance.advance_id.number)\n                        total_advance += advance.amount_advance\n                        inv.l10n_mx_edi_origin += str(advance.advance_id.cfdi_uuid) + ','\n                    else:\n                        raise UserError('El monto necesita un anticipo')\n\n                if total_advance > self.amount_total:\n                    raise UserError('La sumatoria de las facturas de anticipo es mayor que el monto total de esta facturas')        \n                inv.l10n_mx_edi_origin = inv.l10n_mx_edi_origin[:-1]\n\n            # if inv.advance_id and not inv.advance_id.sale_id:\n            #     adv_id = inv.advance_id\n            #     prod_adv = False\n            #     tax_prod = []\n            #     for line in adv_id.invoice_line_ids:\n            #         deposit = self.pool['ir.values'].get_default(\n            #             self._cr, self._uid, 'sale.config.settings',\n            #             'deposit_product_id_setting') or False\n            #         if line.product_id.id == deposit:\n            #             product = self.env['product.product'].search(\n            #                 [('id', '=', deposit)])\n            #             prod_adv = product\n            #             tax_prod = [(6, 0, [x.id for x in\n            #                          line.product_id.taxes_id])]\n\n            #     if not prod_adv:\n            #         raise UserError(_('The Advance Invoice to which it refers,'\n            #                           '\\n does not have an Article type'\n            #                           'in Advance'))\n\n            #     inv_line_values2 = {\n            #         'name': _('Aplication of advance'),\n            #         'origin': inv.advance_id.number,\n            #         'account_id': prod_adv.property_account_income_id.id,\n            #         'price_unit': inv.amount_advance * -1,\n            #         'quantity': 1.0,\n            #         'discount': False,\n            #         'uom_id': prod_adv.uom_id.id or False,\n            #         'product_id': prod_adv.id,\n            #         'invoice_line_tax_id': tax_prod,\n            #         'account_analytic_id': inv.account_analytic_id.id,\n            #         'invoice_id': inv.id,\n            #     }\n            #     inv_line_obj = self.env['account.invoice.line']\n            #     inv_line_id = inv_line_obj.create(inv_line_values2)\n\n            #     inv.advance_id.advance_applied = True\n\n        return res\n","license":"agpl-3.0","hash":6125377372553144393,"line_mean":41.5298013245,"line_max":132,"alpha_frac":0.5018685768,"autogenerated":false},
{"repo_name":"Melissa-AI\/Melissa-Core","path":"melissa\/actions\/define_subject.py","copies":"2","size":"1071","content":"import re\nimport wikipedia\n\n# Melissa\nfrom melissa.tts import tts\n\nWORDS = {'define_subject': {'groups': ['define']}\n         }\n\n\ndef define_subject(speech_text):\n    words_of_message = speech_text.split()\n    words_of_message.remove('define')\n    cleaned_message = ' '.join(words_of_message).rstrip()\n    if len(cleaned_message) == 0:\n        msg = 'define requires subject words'\n        print msg\n        tts(msg)\n        return\n\n    try:\n        wiki_data = wikipedia.summary(cleaned_message, sentences=5)\n\n        regEx = re.compile(r'([^\\(]*)\\([^\\)]*\\) *(.*)')\n        m = regEx.match(wiki_data)\n        while m:\n            wiki_data = m.group(1) + m.group(2)\n            m = regEx.match(wiki_data)\n\n        wiki_data = wiki_data.replace(\"'\", \"\")\n        tts(wiki_data)\n    except wikipedia.exceptions.DisambiguationError as e:\n        tts('Can you please be more specific? You may choose something' +\n            'from the following.')\n        print(\"Can you please be more specific? You may choose something\" +\n              \"from the following; {0}\".format(e))\n","license":"mit","hash":1569015858634939701,"line_mean":28.75,"line_max":75,"alpha_frac":0.5845004669,"autogenerated":false},
{"repo_name":"larsoner\/mne-python","path":"mne\/viz\/topomap.py","copies":"2","size":"105477","content":"\"\"\"Functions to plot M\/EEG data e.g. topographies.\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Denis Engemann <denis.engemann@gmail.com>\n#          Martin Luessi <mluessi@nmr.mgh.harvard.edu>\n#          Eric Larson <larson.eric.d@gmail.com>\n#          Robert Luke <mail@robertluke.net>\n#\n# License: Simplified BSD\n\nimport copy\nimport itertools\nfrom functools import partial\nfrom numbers import Integral\nimport warnings\n\nimport numpy as np\n\nfrom ..baseline import rescale\nfrom ..channels.channels import _get_ch_type\nfrom ..channels.layout import (\n    _find_topomap_coords, find_layout, _pair_grad_sensors, _merge_ch_data)\nfrom ..defaults import _EXTRAPOLATE_DEFAULT, _BORDER_DEFAULT\nfrom ..io.pick import (pick_types, _picks_by_type, pick_info, pick_channels,\n                       _pick_data_channels, _picks_to_idx, _get_channel_types,\n                       _MEG_CH_TYPES_SPLIT)\nfrom ..utils import (_clean_names, _time_mask, verbose, logger, fill_doc,\n                     _validate_type, _check_sphere, _check_option, _is_numeric)\nfrom .utils import (tight_layout, _setup_vmin_vmax, _prepare_trellis,\n                    _check_delayed_ssp, _draw_proj_checkbox, figure_nobar,\n                    plt_show, _process_times, DraggableColorbar,\n                    _validate_if_list_of_axes, _setup_cmap, _check_time_unit)\nfrom ..time_frequency import psd_multitaper\nfrom ..defaults import _handle_default\nfrom ..transforms import apply_trans, invert_transform\nfrom ..io.meas_info import Info, _simplify_info\nfrom ..io.proj import Projection\n\n\n_fnirs_types = ('hbo', 'hbr', 'fnirs_cw_amplitude', 'fnirs_od')\n\n\ndef _adjust_meg_sphere(sphere, info, ch_type):\n    sphere = _check_sphere(sphere, info)\n    assert ch_type is not None\n    if ch_type in ('mag', 'grad', 'planar1', 'planar2'):\n        # move sphere X\/Y (head coords) to device X\/Y space\n        if info['dev_head_t'] is not None:\n            head_dev_t = invert_transform(info['dev_head_t'])\n            sphere[:3] = apply_trans(head_dev_t, sphere[:3])\n            # Set the sphere Z=0 because all this really affects is flattening.\n            # We could make the head size change as a function of depth in\n            # the helmet like:\n            #\n            #     sphere[2] \/= -5\n            #\n            # but let's just assume some orthographic rather than parallel\n            # projection for explicitness \/ simplicity.\n            sphere[2] = 0.\n        clip_origin = (0., 0.)\n    else:\n        clip_origin = sphere[:2].copy()\n    return sphere, clip_origin\n\n\ndef _prepare_topomap_plot(inst, ch_type, sphere=None):\n    \"\"\"Prepare topo plot.\"\"\"\n    info = copy.deepcopy(inst if isinstance(inst, Info) else inst.info)\n    sphere, clip_origin = _adjust_meg_sphere(sphere, info, ch_type)\n\n    clean_ch_names = _clean_names(info['ch_names'])\n    for ii, this_ch in enumerate(info['chs']):\n        this_ch['ch_name'] = clean_ch_names[ii]\n    info['bads'] = _clean_names(info['bads'])\n    for comp in info['comps']:\n        comp['data']['col_names'] = _clean_names(comp['data']['col_names'])\n\n    info._update_redundant()\n    info._check_consistency()\n\n    # special case for merging grad channels\n    layout = find_layout(info)\n    if (ch_type == 'grad' and layout is not None and\n            (layout.kind.startswith('Vectorview') or\n             layout.kind.startswith('Neuromag_122'))):\n        picks, _ = _pair_grad_sensors(info, layout)\n        pos = _find_topomap_coords(info, picks[::2], sphere=sphere)\n        merge_channels = True\n    elif ch_type in _fnirs_types:\n        # fNIRS data commonly has overlapping channels, so deal with separately\n        picks, pos, merge_channels, overlapping_channels = \\\n            _average_fnirs_overlaps(info, ch_type, sphere)\n    else:\n        merge_channels = False\n        if ch_type == 'eeg':\n            picks = pick_types(info, meg=False, eeg=True, ref_meg=False,\n                               exclude='bads')\n        elif ch_type == 'csd':\n            picks = pick_types(info, meg=False, csd=True, ref_meg=False,\n                               exclude='bads')\n        else:\n            picks = pick_types(info, meg=ch_type, ref_meg=False,\n                               exclude='bads')\n\n        if len(picks) == 0:\n            raise ValueError(\"No channels of type %r\" % ch_type)\n\n        pos = _find_topomap_coords(info, picks, sphere=sphere)\n\n    ch_names = [info['ch_names'][k] for k in picks]\n    if ch_type in _fnirs_types:\n        # Remove the chroma label type for cleaner labeling.\n        ch_names = [k[:-4] for k in ch_names]\n\n    if merge_channels:\n        if ch_type == 'grad':\n            # change names so that vectorview combined grads appear as MEG014x\n            # instead of MEG0142 or MEG0143 which are the 2 planar grads.\n            ch_names = [ch_names[k][:-1] + 'x' for k in\n                        range(0, len(ch_names), 2)]\n        else:\n            assert ch_type in _fnirs_types\n            # Modify the nirs channel names to indicate they are to be merged\n            # New names will have the form  S1_D1xS2_D2\n            # More than two channels can overlap and be merged\n            for set in overlapping_channels:\n                idx = ch_names.index(set[0][:-4])\n                new_name = 'x'.join(s[:-4] for s in set)\n                ch_names[idx] = new_name\n\n    pos = np.array(pos)[:, :2]  # 2D plot, otherwise interpolation bugs\n    return picks, pos, merge_channels, ch_names, ch_type, sphere, clip_origin\n\n\ndef _average_fnirs_overlaps(info, ch_type, sphere):\n\n    from scipy.spatial.distance import pdist, squareform\n\n    picks = pick_types(info, meg=False, ref_meg=False,\n                       fnirs=ch_type, exclude='bads')\n    chs = [info['chs'][i] for i in picks]\n    locs3d = np.array([ch['loc'][:3] for ch in chs])\n    dist = pdist(locs3d)\n\n    # Store the sets of channels to be merged\n    overlapping_channels = list()\n    # Channels to be excluded from picks, as will be removed after merging\n    channels_to_exclude = list()\n\n    if len(locs3d) > 1 and np.min(dist) < 1e-10:\n\n        overlapping_mask = np.triu(squareform(dist < 1e-10))\n        for chan_idx in range(overlapping_mask.shape[0]):\n            already_overlapped = list(itertools.chain.from_iterable(\n                overlapping_channels))\n            if overlapping_mask[chan_idx].any() and \\\n                    (chs[chan_idx]['ch_name'] not in already_overlapped):\n                # Determine the set of channels to be combined. Ensure the\n                # first listed channel is the one to be replaced with merge\n                overlapping_set = [chs[i]['ch_name'] for i in\n                                   np.where(overlapping_mask[chan_idx])[0]]\n                overlapping_set = np.insert(overlapping_set, 0,\n                                            (chs[chan_idx]['ch_name']))\n                overlapping_channels.append(overlapping_set)\n                channels_to_exclude.append(overlapping_set[1:])\n\n        exclude = list(itertools.chain.from_iterable(channels_to_exclude))\n        [exclude.append(bad) for bad in info['bads']]\n        picks = pick_types(info, meg=False, ref_meg=False, fnirs=ch_type,\n                           exclude=exclude)\n        pos = _find_topomap_coords(info, picks, sphere=sphere)\n        picks = pick_types(info, meg=False, ref_meg=False, fnirs=ch_type)\n        # Overload the merge_channels variable as this is returned to calling\n        # function and indicates that merging of data is required\n        merge_channels = overlapping_channels\n\n    else:\n        picks = pick_types(info, meg=False, ref_meg=False, fnirs=ch_type,\n                           exclude='bads')\n        merge_channels = False\n        pos = _find_topomap_coords(info, picks, sphere=sphere)\n\n    return picks, pos, merge_channels, overlapping_channels\n\n\ndef _plot_update_evoked_topomap(params, bools):\n    \"\"\"Update topomaps.\"\"\"\n    projs = [proj for ii, proj in enumerate(params['projs'])\n             if ii in np.where(bools)[0]]\n\n    params['proj_bools'] = bools\n    new_evoked = params['evoked'].copy()\n    new_evoked.info['projs'] = []\n    new_evoked.add_proj(projs)\n    new_evoked.apply_proj()\n\n    data = new_evoked.data[:, params['time_idx']] * params['scale']\n    if params['merge_channels']:\n        data, _ = _merge_ch_data(data, 'grad', [])\n\n    interp = params['interp']\n    new_contours = list()\n    for cont, ax, im, d in zip(params['contours_'], params['axes'],\n                               params['images'], data.T):\n        Zi = interp.set_values(d)()\n        im.set_data(Zi)\n        # must be removed and re-added\n        if len(cont.collections) > 0:\n            tp = cont.collections[0]\n            visible = tp.get_visible()\n            patch_ = tp.get_clip_path()\n            color = tp.get_color()\n            lw = tp.get_linewidth()\n        for tp in cont.collections:\n            tp.remove()\n        cont = ax.contour(interp.Xi, interp.Yi, Zi, params['contours'],\n                          colors=color, linewidths=lw)\n        for tp in cont.collections:\n            tp.set_visible(visible)\n            tp.set_clip_path(patch_)\n        new_contours.append(cont)\n    params['contours_'] = new_contours\n\n    params['fig'].canvas.draw()\n\n\ndef _add_colorbar(ax, im, cmap, side=\"right\", pad=.05, title=None,\n                  format=None, size=\"5%\"):\n    \"\"\"Add a colorbar to an axis.\"\"\"\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(side, size=size, pad=pad)\n    cbar = plt.colorbar(im, cax=cax, format=format)\n    if cmap is not None and cmap[1]:\n        ax.CB = DraggableColorbar(cbar, im)\n    if title is not None:\n        cax.set_title(title, y=1.05, fontsize=10)\n    return cbar, cax\n\n\ndef _eliminate_zeros(proj):\n    \"\"\"Remove grad or mag data if only contains 0s (gh 5641).\"\"\"\n    GRAD_ENDING = ('2', '3')\n    MAG_ENDING = '1'\n\n    proj = copy.deepcopy(proj)\n    proj['data']['data'] = np.atleast_2d(proj['data']['data'])\n\n    for ending in (GRAD_ENDING, MAG_ENDING):\n        names = proj['data']['col_names']\n        idx = [i for i, name in enumerate(names) if name.endswith(ending)]\n\n        # if all 0, remove the 0s an their labels\n        if not proj['data']['data'][0][idx].any():\n            new_col_names = np.delete(np.array(names), idx).tolist()\n            new_data = np.delete(np.array(proj['data']['data'][0]), idx)\n            proj['data']['col_names'] = new_col_names\n            proj['data']['data'] = np.array([new_data])\n\n    proj['data']['ncol'] = len(proj['data']['col_names'])\n    return proj\n\n\n@fill_doc\ndef plot_projs_topomap(projs, info, cmap=None, sensors=True,\n                       colorbar=False, res=64, size=1, show=True,\n                       outlines='head', contours=6, image_interp='bilinear',\n                       axes=None, vlim=(None, None),\n                       sphere=None, extrapolate=_EXTRAPOLATE_DEFAULT,\n                       border=_BORDER_DEFAULT):\n    \"\"\"Plot topographic maps of SSP projections.\n\n    Parameters\n    ----------\n    projs : list of Projection\n        The projections.\n    info : instance of Info\n        The info associated with the channels in the projectors.\n\n        .. versionchanged:: 0.20\n            The positional argument ``layout`` was deprecated and replaced\n            by ``info``.\n    %(proj_topomap_kwargs)s\n    %(topomap_sphere_auto)s\n    %(topomap_extrapolate)s\n\n        .. versionadded:: 0.20\n    %(topomap_border)s\n\n    Returns\n    -------\n    fig : instance of matplotlib.figure.Figure\n        Figure with a topomap subplot for each projector.\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    \"\"\"\n    import matplotlib.pyplot as plt\n    sphere = _check_sphere(sphere, info)\n\n    # be forgiving if `projs` isn't a list\n    if isinstance(projs, Projection):\n        projs = [projs]\n\n    _validate_type(info, 'info', 'info')\n\n    types, datas, poss, spheres, outliness, ch_typess = [], [], [], [], [], []\n    for proj in projs:\n        # get ch_names, ch_types, data\n        proj = _eliminate_zeros(proj)  # gh 5641\n        ch_names = _clean_names(proj['data']['col_names'],\n                                remove_whitespace=True)\n        if vlim == 'joint':\n            ch_idxs = np.where(np.in1d(info['ch_names'],\n                                       proj['data']['col_names']))[0]\n            these_ch_types = _get_channel_types(info, ch_idxs, unique=True)\n            # each projector should have only one channel type\n            assert len(these_ch_types) == 1\n            types.append(list(these_ch_types)[0])\n        data = proj['data']['data'].ravel()\n        info_names = _clean_names(info['ch_names'], remove_whitespace=True)\n        picks = pick_channels(info_names, ch_names)\n        if len(picks) == 0:\n            raise ValueError(\n                f'No channel names in info match projector {proj}')\n        use_info = pick_info(info, picks)\n        data_picks, pos, merge_channels, names, ch_type, this_sphere, \\\n            clip_origin = _prepare_topomap_plot(\n                use_info, _get_ch_type(use_info, None), sphere=sphere)\n        these_outlines = _make_head_outlines(\n            sphere, pos, outlines, clip_origin)\n        data = data[data_picks]\n        if merge_channels:\n            data, _ = _merge_ch_data(data, 'grad', [])\n            data = data.ravel()\n\n        # populate containers\n        datas.append(data)\n        poss.append(pos)\n        spheres.append(this_sphere)\n        outliness.append(these_outlines)\n        ch_typess.append(ch_type)\n        del data, pos, this_sphere, these_outlines, ch_type\n    del sphere\n\n    # setup axes\n    n_projs = len(projs)\n    if axes is None:\n        fig, axes, ncols, nrows = _prepare_trellis(\n            n_projs, ncols='auto', nrows='auto', sharex=True, sharey=True)\n    elif isinstance(axes, plt.Axes):\n        axes = [axes]\n    _validate_if_list_of_axes(axes, n_projs)\n\n    # handle vmin\/vmax\n    vlims = [None for _ in range(len(datas))]\n    if vlim == 'joint':\n        for _ch_type in set(types):\n            idx = np.where(np.in1d(types, _ch_type))[0]\n            these_data = np.concatenate(np.array(datas, dtype=object)[idx])\n            norm = all(these_data >= 0)\n            _vl = _setup_vmin_vmax(these_data, vmin=None, vmax=None, norm=norm)\n            for _idx in idx:\n                vlims[_idx] = _vl\n        # make sure we got a vlim for all projs\n        assert all([vl is not None for vl in vlims])\n    else:\n        vlims = [vlim for _ in range(len(datas))]\n\n    # plot\n    for proj, ax, _data, _pos, _vlim, _sphere, _outlines, _ch_type in zip(\n            projs, axes, datas, poss, vlims, spheres, outliness, ch_typess):\n        # title\n        title = proj['desc']\n        title = '\\n'.join(title[ii:ii + 22] for ii in range(0, len(title), 22))\n        ax.set_title(title, fontsize=10)\n        # plot\n        vmin, vmax = _vlim\n        im = plot_topomap(_data, _pos[:, :2], vmin=vmin, vmax=vmax, cmap=cmap,\n                          sensors=sensors, res=res, axes=ax,\n                          outlines=_outlines, contours=contours,\n                          image_interp=image_interp, show=False,\n                          extrapolate=extrapolate, sphere=_sphere,\n                          border=border, ch_type=_ch_type)[0]\n\n        if colorbar:\n            _add_colorbar(ax, im, cmap)\n\n    fig = ax.get_figure()\n    with warnings.catch_warnings(record=True):\n        warnings.simplefilter('ignore')\n        tight_layout(fig=fig)\n    plt_show(show)\n    return fig\n\n\ndef _make_head_outlines(sphere, pos, outlines, clip_origin):\n    \"\"\"Check or create outlines for topoplot.\"\"\"\n    assert isinstance(sphere, np.ndarray)\n    x, y, _, radius = sphere\n    del sphere\n\n    if outlines in ('head', 'skirt', None):\n        ll = np.linspace(0, 2 * np.pi, 101)\n        head_x = np.cos(ll) * radius + x\n        head_y = np.sin(ll) * radius + y\n        dx = np.exp(np.arccos(np.deg2rad(12)) * 1j)\n        dx, dy = dx.real, dx.imag\n        nose_x = np.array([-dx, 0, dx]) * radius + x\n        nose_y = np.array([dy, 1.15, dy]) * radius + y\n        ear_x = np.array([.497, .510, .518, .5299, .5419, .54, .547,\n                          .532, .510, .489]) * (radius * 2)\n        ear_y = np.array([.0555, .0775, .0783, .0746, .0555, -.0055, -.0932,\n                          -.1313, -.1384, -.1199]) * (radius * 2) + y\n\n        if outlines is not None:\n            # Define the outline of the head, ears and nose\n            outlines_dict = dict(head=(head_x, head_y), nose=(nose_x, nose_y),\n                                 ear_left=(ear_x + x, ear_y),\n                                 ear_right=(-ear_x + x, ear_y))\n        else:\n            outlines_dict = dict()\n\n        # Make the figure encompass slightly more than all points\n        mask_scale = 1.25 if outlines == 'skirt' else 1.\n        # We probably want to ensure it always contains our most\n        # extremely positioned channels, so we do:\n        mask_scale = max(\n            mask_scale, np.linalg.norm(pos, axis=1).max() * 1.01 \/ radius)\n        outlines_dict['mask_pos'] = (mask_scale * head_x, mask_scale * head_y)\n        clip_radius = radius * mask_scale\n        outlines_dict['clip_radius'] = (clip_radius,) * 2\n        outlines_dict['clip_origin'] = clip_origin\n        outlines = outlines_dict\n\n    elif isinstance(outlines, dict):\n        if 'mask_pos' not in outlines:\n            raise ValueError('You must specify the coordinates of the image '\n                             'mask.')\n    else:\n        raise ValueError('Invalid value for `outlines`.')\n\n    return outlines\n\n\ndef _draw_outlines(ax, outlines):\n    \"\"\"Draw the outlines for a topomap.\"\"\"\n    from matplotlib import rcParams\n    outlines_ = {k: v for k, v in outlines.items()\n                 if k not in ['patch']}\n    for key, (x_coord, y_coord) in outlines_.items():\n        if 'mask' in key or key in ('clip_radius', 'clip_origin'):\n            continue\n        ax.plot(x_coord, y_coord, color=rcParams['axes.edgecolor'],\n                linewidth=1, clip_on=False)\n    return outlines_\n\n\ndef _get_extra_points(pos, extrapolate, origin, radii):\n    \"\"\"Get coordinates of additinal interpolation points.\"\"\"\n    from scipy.spatial.qhull import Delaunay\n    radii = np.array(radii, float)\n    assert radii.shape == (2,)\n    x, y = origin\n    # auto should be gone by now\n    _check_option('extrapolate', extrapolate, ('head', 'box', 'local'))\n\n    # the old method of placement - large box\n    mask_pos = None\n    if extrapolate == 'box':\n        extremes = np.array([pos.min(axis=0), pos.max(axis=0)])\n        diffs = extremes[1] - extremes[0]\n        extremes[0] -= diffs\n        extremes[1] += diffs\n        eidx = np.array(list(itertools.product(\n            *([[0] * (pos.shape[1] - 1) + [1]] * pos.shape[1]))))\n        pidx = np.tile(np.arange(pos.shape[1])[np.newaxis], (len(eidx), 1))\n        outer_pts = extremes[eidx, pidx]\n        return outer_pts, mask_pos, Delaunay(np.concatenate((pos, outer_pts)))\n\n    # check if positions are colinear:\n    diffs = np.diff(pos, axis=0)\n    with np.errstate(divide='ignore'):\n        slopes = diffs[:, 1] \/ diffs[:, 0]\n    colinear = ((slopes == slopes[0]).all() or np.isinf(slopes).all())\n\n    # compute median inter-electrode distance\n    if colinear or pos.shape[0] < 4:\n        dim = 1 if diffs[:, 1].sum() > diffs[:, 0].sum() else 0\n        sorting = np.argsort(pos[:, dim])\n        pos_sorted = pos[sorting, :]\n        diffs = np.diff(pos_sorted, axis=0)\n        distances = np.linalg.norm(diffs, axis=1)\n        distance = np.median(distances)\n    else:\n        tri = Delaunay(pos, incremental=True)\n        idx1, idx2, idx3 = tri.simplices.T\n        distances = np.concatenate(\n            [np.linalg.norm(pos[i1, :] - pos[i2, :], axis=1)\n             for i1, i2 in zip([idx1, idx2], [idx2, idx3])])\n        distance = np.median(distances)\n\n    if extrapolate == 'local':\n        if colinear or pos.shape[0] < 4:\n            # special case for colinear points and when there is too\n            # little points for Delaunay (needs at least 3)\n            edge_points = sorting[[0, -1]]\n            line_len = np.diff(pos[edge_points, :], axis=0)\n            unit_vec = line_len \/ np.linalg.norm(line_len) * distance\n            unit_vec_par = unit_vec[:, ::-1] * [[-1, 1]]\n\n            edge_pos = (pos[edge_points, :] +\n                        np.concatenate([-unit_vec, unit_vec], axis=0))\n            new_pos = np.concatenate([pos + unit_vec_par,\n                                      pos - unit_vec_par, edge_pos], axis=0)\n\n            if pos.shape[0] == 3:\n                # there may be some new_pos points that are too close\n                # to the original points\n                new_pos_diff = pos[..., np.newaxis] - new_pos.T[np.newaxis, :]\n                new_pos_diff = np.linalg.norm(new_pos_diff, axis=1)\n                good_extra = (new_pos_diff > 0.5 * distance).all(axis=0)\n                new_pos = new_pos[good_extra]\n\n            tri = Delaunay(np.concatenate([pos, new_pos], axis=0))\n            return new_pos, new_pos, tri\n\n        # get the convex hull of data points from triangulation\n        hull_pos = pos[tri.convex_hull]\n\n        # extend the convex hull limits outwards a bit\n        channels_center = pos.mean(axis=0)\n        radial_dir = hull_pos - channels_center\n        unit_radial_dir = radial_dir \/ np.linalg.norm(radial_dir, axis=-1,\n                                                      keepdims=True)\n        hull_extended = hull_pos + unit_radial_dir * distance\n        mask_pos = hull_pos + unit_radial_dir * distance * 0.5\n        hull_diff = np.diff(hull_pos, axis=1)[:, 0]\n        hull_distances = np.linalg.norm(hull_diff, axis=-1)\n        del channels_center\n\n        # Construct a mask\n        mask_pos = np.unique(mask_pos.reshape(-1, 2), axis=0)\n        mask_center = np.mean(mask_pos, axis=0)\n        mask_pos -= mask_center\n        mask_pos = mask_pos[\n            np.argsort(np.arctan2(mask_pos[:, 1], mask_pos[:, 0]))]\n        mask_pos += mask_center\n\n        # add points along hull edges so that the distance between points\n        # is around that of average distance between channels\n        add_points = list()\n        eps = np.finfo('float').eps\n        n_times_dist = np.round(0.25 * hull_distances \/ distance).astype('int')\n        for n in range(2, n_times_dist.max() + 1):\n            mask = n_times_dist == n\n            mult = np.arange(1 \/ n, 1 - eps, 1 \/ n)[:, np.newaxis, np.newaxis]\n            steps = hull_diff[mask][np.newaxis, ...] * mult\n            add_points.append((hull_extended[mask, 0][np.newaxis, ...] +\n                               steps).reshape((-1, 2)))\n\n        # remove duplicates from hull_extended\n        hull_extended = np.unique(hull_extended.reshape((-1, 2)), axis=0)\n        new_pos = np.concatenate([hull_extended] + add_points)\n    else:\n        assert extrapolate == 'head'\n        # return points on the head circle\n        angle = np.arcsin(distance \/ np.mean(radii))\n        n_pnts = max(12, int(np.round(2 * np.pi \/ angle)))\n        points_l = np.linspace(0, 2 * np.pi, n_pnts, endpoint=False)\n        use_radii = radii * 1.1 + distance\n        points_x = np.cos(points_l) * use_radii[0] + x\n        points_y = np.sin(points_l) * use_radii[1] + y\n        new_pos = np.stack([points_x, points_y], axis=1)\n        if colinear or pos.shape[0] == 3:\n            tri = Delaunay(np.concatenate([pos, new_pos], axis=0))\n            return new_pos, mask_pos, tri\n    tri.add_points(new_pos)\n    return new_pos, mask_pos, tri\n\n\nclass _GridData(object):\n    \"\"\"Unstructured (x,y) data interpolator.\n\n    This class allows optimized interpolation by computing parameters\n    for a fixed set of true points, and allowing the values at those points\n    to be set independently.\n    \"\"\"\n\n    def __init__(self, pos, extrapolate, origin, radii, border):\n        # in principle this works in N dimensions, not just 2\n        assert pos.ndim == 2 and pos.shape[1] == 2, pos.shape\n        _validate_type(border, ('numeric', str), 'border')\n\n        # check that border, if string, is correct\n        if isinstance(border, str):\n            _check_option('border', border, ('mean',), extra='when a string')\n\n        # Adding points outside the extremes helps the interpolators\n        outer_pts, mask_pts, tri = _get_extra_points(\n            pos, extrapolate, origin, radii)\n        self.n_extra = outer_pts.shape[0]\n        self.mask_pts = mask_pts\n        self.border = border\n        self.tri = tri\n\n    def set_values(self, v):\n        \"\"\"Set the values at interpolation points.\"\"\"\n        # Rbf with thin-plate is what we used to use, but it's slower and\n        # looks about the same:\n        #\n        #     zi = Rbf(x, y, v, function='multiquadric', smooth=0)(xi, yi)\n        #\n        # Eventually we could also do set_values with this class if we want,\n        # see scipy\/interpolate\/rbf.py, especially the self.nodes one-liner.\n        from scipy.interpolate import CloughTocher2DInterpolator\n\n        if isinstance(self.border, str):\n            # we've already checked that border = 'mean'\n            n_points = v.shape[0]\n            v_extra = np.zeros(self.n_extra)\n            indices, indptr = self.tri.vertex_neighbor_vertices\n            rng = range(n_points, n_points + self.n_extra)\n            used = np.zeros(len(rng), bool)\n            for idx, extra_idx in enumerate(rng):\n                ngb = indptr[indices[extra_idx]:indices[extra_idx + 1]]\n                ngb = ngb[ngb < n_points]\n                if len(ngb) > 0:\n                    used[idx] = True\n                    v_extra[idx] = v[ngb].mean()\n            if not used.all() and used.any():\n                # Eventually we might want to use the value of the nearest\n                # point or something, but this case should hopefully be\n                # rare so for now just use the average value of all extras\n                v_extra[~used] = np.mean(v_extra[used])\n        else:\n            v_extra = np.full(self.n_extra, self.border, dtype=float)\n\n        v = np.concatenate((v, v_extra))\n        self.interpolator = CloughTocher2DInterpolator(self.tri, v)\n        return self\n\n    def set_locations(self, Xi, Yi):\n        \"\"\"Set locations for easier (delayed) calling.\"\"\"\n        self.Xi = Xi\n        self.Yi = Yi\n        return self\n\n    def __call__(self, *args):\n        \"\"\"Evaluate the interpolator.\"\"\"\n        if len(args) == 0:\n            args = [self.Xi, self.Yi]\n        return self.interpolator(*args)\n\n\ndef _topomap_plot_sensors(pos_x, pos_y, sensors, ax):\n    \"\"\"Plot sensors.\"\"\"\n    if sensors is True:\n        ax.scatter(pos_x, pos_y, s=0.25, marker='o',\n                   edgecolor=['k'] * len(pos_x), facecolor='none')\n    else:\n        ax.plot(pos_x, pos_y, sensors)\n\n\ndef _get_pos_outlines(info, picks, sphere, to_sphere=True):\n    ch_type = _get_ch_type(pick_info(_simplify_info(info), picks), None)\n    orig_sphere = sphere\n    sphere, clip_origin = _adjust_meg_sphere(sphere, info, ch_type)\n    logger.debug('Generating pos outlines with sphere '\n                 f'{sphere} from {orig_sphere} for {ch_type}')\n    pos = _find_topomap_coords(\n        info, picks, ignore_overlap=True, to_sphere=to_sphere,\n        sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, 'head', clip_origin)\n    return pos, outlines\n\n\n@fill_doc\ndef plot_topomap(data, pos, vmin=None, vmax=None, cmap=None, sensors=True,\n                 res=64, axes=None, names=None, show_names=False, mask=None,\n                 mask_params=None, outlines='head',\n                 contours=6, image_interp='bilinear', show=True,\n                 onselect=None, extrapolate=_EXTRAPOLATE_DEFAULT,\n                 sphere=None, border=_BORDER_DEFAULT,\n                 ch_type='eeg'):\n    \"\"\"Plot a topographic map as image.\n\n    Parameters\n    ----------\n    data : array, shape (n_chan,)\n        The data values to plot.\n    pos : array, shape (n_chan, 2) | instance of Info\n        Location information for the data points(\/channels).\n        If an array, for each data point, the x and y coordinates.\n        If an Info object, it must contain only one data type and\n        exactly ``len(data)`` data channels, and the x\/y coordinates will\n        be inferred from this Info object.\n    vmin : float | callable | None\n        The value specifying the lower bound of the color range.\n        If None, and vmax is None, -vmax is used. Else np.min(data).\n        If callable, the output equals vmin(data). Defaults to None.\n    vmax : float | callable | None\n        The value specifying the upper bound of the color range.\n        If None, the maximum absolute value is used. If callable, the output\n        equals vmax(data). Defaults to None.\n    cmap : matplotlib colormap | None\n        Colormap to use. If None, 'Reds' is used for all positive data,\n        otherwise defaults to 'RdBu_r'.\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib plot\n        format string (e.g., 'r+' for red plusses). If True (default), circles\n        will be used.\n    res : int\n        The resolution of the topomap image (n pixels along each side).\n    axes : instance of Axes | None\n        The axes to plot to. If None, the current axes will be used.\n    names : list | None\n        List of channel names. If None, channel names are not plotted.\n    %(topomap_show_names)s\n        If ``True``, a list of names must be provided (see ``names`` keyword).\n    mask : ndarray of bool, shape (n_channels, n_times) | None\n        The channels to be marked as significant at a given time point.\n        Indices set to ``True`` will be considered. Defaults to None.\n    mask_params : dict | None\n        Additional plotting parameters for plotting significant sensors.\n        Default (None) equals::\n\n           dict(marker='o', markerfacecolor='w', markeredgecolor='k',\n                linewidth=0, markersize=4)\n    %(topomap_outlines)s\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        If an array, the values represent the levels for the contours. The\n        values are in \u00b5V for EEG, fT for magnetometers and fT\/m for\n        gradiometers. Defaults to 6.\n    image_interp : str\n        The image interpolation to be used. All matplotlib options are\n        accepted.\n    show : bool\n        Show figure if True.\n    onselect : callable | None\n        Handle for a function that is called when the user selects a set of\n        channels by rectangle selection (matplotlib ``RectangleSelector``). If\n        None interactive selection is disabled. Defaults to None.\n    %(topomap_extrapolate)s\n\n        .. versionadded:: 0.18\n    %(topomap_sphere)s\n    %(topomap_border)s\n    %(topomap_ch_type)s\n\n    Returns\n    -------\n    im : matplotlib.image.AxesImage\n        The interpolated data.\n    cn : matplotlib.contour.ContourSet\n        The fieldlines.\n    \"\"\"\n    sphere = _check_sphere(sphere)\n    return _plot_topomap(data, pos, vmin, vmax, cmap, sensors, res, axes,\n                         names, show_names, mask, mask_params, outlines,\n                         contours, image_interp, show,\n                         onselect, extrapolate, sphere=sphere, border=border,\n                         ch_type=ch_type)[:2]\n\n\ndef _setup_interp(pos, res, extrapolate, sphere, outlines, border):\n    logger.debug(f'Interpolation mode {extrapolate} to {border}')\n    xlim = np.inf, -np.inf,\n    ylim = np.inf, -np.inf,\n    mask_ = np.c_[outlines['mask_pos']]\n    clip_radius = outlines['clip_radius']\n    clip_origin = outlines.get('clip_origin', (0., 0.))\n    xmin, xmax = (np.min(np.r_[xlim[0],\n                               mask_[:, 0],\n                               clip_origin[0] - clip_radius[0]]),\n                  np.max(np.r_[xlim[1],\n                               mask_[:, 0],\n                               clip_origin[0] + clip_radius[0]]))\n    ymin, ymax = (np.min(np.r_[ylim[0],\n                               mask_[:, 1],\n                               clip_origin[1] - clip_radius[1]]),\n                  np.max(np.r_[ylim[1],\n                               mask_[:, 1],\n                               clip_origin[1] + clip_radius[1]]))\n    xi = np.linspace(xmin, xmax, res)\n    yi = np.linspace(ymin, ymax, res)\n    Xi, Yi = np.meshgrid(xi, yi)\n    interp = _GridData(pos, extrapolate, clip_origin, clip_radius, border)\n    extent = (xmin, xmax, ymin, ymax)\n    return extent, Xi, Yi, interp\n\n\ndef _get_patch(outlines, extrapolate, interp, ax):\n    from matplotlib import patches\n    clip_radius = outlines['clip_radius']\n    clip_origin = outlines.get('clip_origin', (0., 0.))\n    _use_default_outlines = any(k.startswith('head') for k in outlines)\n    patch_ = None\n    if 'patch' in outlines:\n        patch_ = outlines['patch']\n        patch_ = patch_() if callable(patch_) else patch_\n        patch_.set_clip_on(False)\n        ax.add_patch(patch_)\n        ax.set_transform(ax.transAxes)\n        ax.set_clip_path(patch_)\n    if _use_default_outlines:\n        if extrapolate == 'local':\n            patch_ = patches.Polygon(\n                interp.mask_pts, clip_on=True, transform=ax.transData)\n        else:\n            patch_ = patches.Ellipse(\n                clip_origin, 2 * clip_radius[0], 2 * clip_radius[1],\n                clip_on=True, transform=ax.transData)\n    return patch_\n\n\ndef _plot_topomap(data, pos, vmin=None, vmax=None, cmap=None, sensors=True,\n                  res=64, axes=None, names=None, show_names=False, mask=None,\n                  mask_params=None, outlines='head',\n                  contours=6, image_interp='bilinear', show=True,\n                  onselect=None, extrapolate=_EXTRAPOLATE_DEFAULT, sphere=None,\n                  border=_BORDER_DEFAULT, ch_type='eeg'):\n    import matplotlib.pyplot as plt\n    from matplotlib.widgets import RectangleSelector\n    data = np.asarray(data)\n    logger.debug(f'Plotting topomap for {ch_type} data shape {data.shape}')\n\n    if isinstance(pos, Info):  # infer pos from Info object\n        picks = _pick_data_channels(pos, exclude=())  # pick only data channels\n        pos = pick_info(pos, picks)\n\n        # check if there is only 1 channel type, and n_chans matches the data\n        ch_type = _get_channel_types(pos, unique=True)\n        info_help = (\"Pick Info with e.g. mne.pick_info and \"\n                     \"mne.io.pick.channel_indices_by_type.\")\n        if len(ch_type) > 1:\n            raise ValueError(\"Multiple channel types in Info structure. \" +\n                             info_help)\n        elif len(pos[\"chs\"]) != data.shape[0]:\n            raise ValueError(\"Number of channels in the Info object (%s) and \"\n                             \"the data array (%s) do not match. \"\n                             % (len(pos['chs']), data.shape[0]) + info_help)\n        else:\n            ch_type = ch_type.pop()\n\n        if any(type_ in ch_type for type_ in ('planar', 'grad')):\n            # deal with grad pairs\n            picks = _pair_grad_sensors(pos, topomap_coords=False)\n            pos = _find_topomap_coords(pos, picks=picks[::2], sphere=sphere)\n            data, _ = _merge_ch_data(data, ch_type, [])\n            data = data.reshape(-1)\n        else:\n            picks = list(range(data.shape[0]))\n            pos = _find_topomap_coords(pos, picks=picks, sphere=sphere)\n\n    extrapolate = _check_extrapolate(extrapolate, ch_type)\n    if data.ndim > 1:\n        raise ValueError(\"Data needs to be array of shape (n_sensors,); got \"\n                         \"shape %s.\" % str(data.shape))\n\n    # Give a helpful error message for common mistakes regarding the position\n    # matrix.\n    pos_help = (\"Electrode positions should be specified as a 2D array with \"\n                \"shape (n_channels, 2). Each row in this matrix contains the \"\n                \"(x, y) position of an electrode.\")\n    if pos.ndim != 2:\n        error = (\"{ndim}D array supplied as electrode positions, where a 2D \"\n                 \"array was expected\").format(ndim=pos.ndim)\n        raise ValueError(error + \" \" + pos_help)\n    elif pos.shape[1] == 3:\n        error = (\"The supplied electrode positions matrix contains 3 columns. \"\n                 \"Are you trying to specify XYZ coordinates? Perhaps the \"\n                 \"mne.channels.create_eeg_layout function is useful for you.\")\n        raise ValueError(error + \" \" + pos_help)\n    # No error is raised in case of pos.shape[1] == 4. In this case, it is\n    # assumed the position matrix contains both (x, y) and (width, height)\n    # values, such as Layout.pos.\n    elif pos.shape[1] == 1 or pos.shape[1] > 4:\n        raise ValueError(pos_help)\n    pos = pos[:, :2]\n\n    if len(data) != len(pos):\n        raise ValueError(\"Data and pos need to be of same length. Got data of \"\n                         \"length %s, pos of length %s\" % (len(data), len(pos)))\n\n    norm = min(data) >= 0\n    vmin, vmax = _setup_vmin_vmax(data, vmin, vmax, norm)\n    if cmap is None:\n        cmap = 'Reds' if norm else 'RdBu_r'\n\n    outlines = _make_head_outlines(sphere, pos, outlines, (0., 0.))\n    assert isinstance(outlines, dict)\n\n    ax = axes if axes else plt.gca()\n    _prepare_topomap(pos, ax)\n\n    mask_params = _handle_default('mask_params', mask_params)\n\n    # find mask limits\n    extent, Xi, Yi, interp = _setup_interp(\n        pos, res, extrapolate, sphere, outlines, border)\n    interp.set_values(data)\n    Zi = interp.set_locations(Xi, Yi)()\n\n    # plot outline\n    patch_ = _get_patch(outlines, extrapolate, interp, ax)\n\n    # plot interpolated map\n    im = ax.imshow(Zi, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower',\n                   aspect='equal', extent=extent,\n                   interpolation=image_interp)\n\n    # gh-1432 had a workaround for no contours here, but we'll remove it\n    # because mpl has probably fixed it\n    linewidth = mask_params['markeredgewidth']\n    cont = True\n    if isinstance(contours, (np.ndarray, list)):\n        pass\n    elif contours == 0 or ((Zi == Zi[0, 0]) | np.isnan(Zi)).all():\n        cont = None  # can't make contours for constant-valued functions\n    if cont:\n        with warnings.catch_warnings(record=True):\n            warnings.simplefilter('ignore')\n            cont = ax.contour(Xi, Yi, Zi, contours, colors='k',\n                              linewidths=linewidth \/ 2.)\n\n    if patch_ is not None:\n        im.set_clip_path(patch_)\n        if cont is not None:\n            for col in cont.collections:\n                col.set_clip_path(patch_)\n\n    pos_x, pos_y = pos.T\n    if sensors is not False and mask is None:\n        _topomap_plot_sensors(pos_x, pos_y, sensors=sensors, ax=ax)\n    elif sensors and mask is not None:\n        idx = np.where(mask)[0]\n        ax.plot(pos_x[idx], pos_y[idx], **mask_params)\n        idx = np.where(~mask)[0]\n        _topomap_plot_sensors(pos_x[idx], pos_y[idx], sensors=sensors, ax=ax)\n    elif not sensors and mask is not None:\n        idx = np.where(mask)[0]\n        ax.plot(pos_x[idx], pos_y[idx], **mask_params)\n\n    if isinstance(outlines, dict):\n        _draw_outlines(ax, outlines)\n\n    if show_names:\n        if names is None:\n            raise ValueError(\"To show names, a list of names must be provided\"\n                             \" (see `names` keyword).\")\n        if show_names is True:\n            def _show_names(x):\n                return x\n        else:\n            _show_names = show_names\n        show_idx = np.arange(len(names)) if mask is None else np.where(mask)[0]\n        for ii, (p, ch_id) in enumerate(zip(pos, names)):\n            if ii not in show_idx:\n                continue\n            ch_id = _show_names(ch_id)\n            ax.text(p[0], p[1], ch_id, horizontalalignment='center',\n                    verticalalignment='center', size='x-small')\n\n    plt.subplots_adjust(top=.95)\n\n    if onselect is not None:\n        lim = ax.dataLim\n        x0, y0, width, height = lim.x0, lim.y0, lim.width, lim.height\n        ax.RS = RectangleSelector(ax, onselect=onselect)\n        ax.set(xlim=[x0, x0 + width], ylim=[y0, y0 + height])\n    plt_show(show)\n    return im, cont, interp\n\n\ndef _plot_ica_topomap(ica, idx=0, ch_type=None, res=64,\n                      vmin=None, vmax=None, cmap='RdBu_r', colorbar=False,\n                      title=None, show=True, outlines='head', contours=6,\n                      image_interp='bilinear', axes=None,\n                      sensors=True, allow_ref_meg=False,\n                      extrapolate=_EXTRAPOLATE_DEFAULT,\n                      sphere=None, border=_BORDER_DEFAULT):\n    \"\"\"Plot single ica map to axes.\"\"\"\n    from matplotlib.axes import Axes\n\n    if ica.info is None:\n        raise RuntimeError('The ICA\\'s measurement info is missing. Please '\n                           'fit the ICA or add the corresponding info object.')\n    sphere = _check_sphere(sphere, ica.info)\n    if not isinstance(axes, Axes):\n        raise ValueError('axis has to be an instance of matplotlib Axes, '\n                         'got %s instead.' % type(axes))\n    ch_type = _get_ch_type(ica, ch_type, allow_ref_meg=ica.allow_ref_meg)\n    if ch_type == \"ref_meg\":\n        logger.info(\"Cannot produce topographies for MEG reference channels.\")\n        return\n\n    data = ica.get_components()[:, idx]\n    data_picks, pos, merge_channels, names, _, sphere, clip_origin = \\\n        _prepare_topomap_plot(ica, ch_type, sphere=sphere)\n    data = data[data_picks]\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n\n    if merge_channels:\n        data, names = _merge_ch_data(data, ch_type, names)\n\n    axes.set_title(ica._ica_names[idx], fontsize=12)\n    vmin_, vmax_ = _setup_vmin_vmax(data, vmin, vmax)\n    im = plot_topomap(\n        data.ravel(), pos, vmin=vmin_, vmax=vmax_, res=res, axes=axes,\n        cmap=cmap, outlines=outlines, contours=contours, sensors=sensors,\n        image_interp=image_interp, show=show, extrapolate=extrapolate,\n        sphere=sphere, border=border, ch_type=ch_type)[0]\n    if colorbar:\n        cbar, cax = _add_colorbar(axes, im, cmap, pad=.05, title=\"AU\",\n                                  format='%3.2f')\n        cbar.ax.tick_params(labelsize=12)\n        cbar.set_ticks((vmin_, vmax_))\n    _hide_frame(axes)\n\n\n@verbose\ndef plot_ica_components(ica, picks=None, ch_type=None, res=64,\n                        vmin=None, vmax=None, cmap='RdBu_r',\n                        sensors=True, colorbar=False, title=None,\n                        show=True, outlines='head', contours=6,\n                        image_interp='bilinear',\n                        inst=None, plot_std=True, topomap_args=None,\n                        image_args=None, psd_args=None, reject='auto',\n                        sphere=None, *, verbose=None):\n    \"\"\"Project mixing matrix on interpolated sensor topography.\n\n    Parameters\n    ----------\n    ica : instance of mne.preprocessing.ICA\n        The ICA solution.\n    %(picks_all)s\n        If None all are plotted in batches of 20.\n    ch_type : 'mag' | 'grad' | 'planar1' | 'planar2' | 'eeg' | None\n        The channel type to plot. For 'grad', the gradiometers are\n        collected in pairs and the RMS for each pair is plotted.\n        If None, then channels are chosen in the order given above.\n    res : int\n        The resolution of the topomap image (n pixels along each side).\n    vmin : float | callable | None\n        The value specifying the lower bound of the color range.\n        If None, and vmax is None, -vmax is used. Else np.min(data).\n        If callable, the output equals vmin(data). Defaults to None.\n    vmax : float | callable | None\n        The value specifying the upper bound of the color range.\n        If None, the maximum absolute value is used. If callable, the output\n        equals vmax(data). Defaults to None.\n    cmap : matplotlib colormap | (colormap, bool) | 'interactive' | None\n        Colormap to use. If tuple, the first value indicates the colormap to\n        use and the second value is a boolean defining interactivity. In\n        interactive mode the colors are adjustable by clicking and dragging the\n        colorbar with left and right mouse button. Left mouse button moves the\n        scale up and down and right mouse button adjusts the range. Hitting\n        space bar resets the range. Up and down arrows can be used to change\n        the colormap. If None, 'Reds' is used for all positive data,\n        otherwise defaults to 'RdBu_r'. If 'interactive', translates to\n        (None, True). Defaults to 'RdBu_r'.\n\n        .. warning::  Interactive mode works smoothly only for a small amount\n                      of topomaps.\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib\n        plot format string (e.g., 'r+' for red plusses). If True (default),\n        circles  will be used.\n    colorbar : bool\n        Plot a colorbar.\n    title : str | None\n        Title to use.\n    show : bool\n        Show figure if True.\n    %(topomap_outlines)s\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        When an integer, matplotlib ticker locator is used to find suitable\n        values for the contour thresholds (may sometimes be inaccurate, use\n        array for accuracy). If an array, the values represent the levels for\n        the contours. Defaults to 6.\n    image_interp : str\n        The image interpolation to be used. All matplotlib options are\n        accepted.\n    inst : Raw | Epochs | None\n        To be able to see component properties after clicking on component\n        topomap you need to pass relevant data - instances of Raw or Epochs\n        (for example the data that ICA was trained on). This takes effect\n        only when running matplotlib in interactive mode.\n    plot_std : bool | float\n        Whether to plot standard deviation in ERP\/ERF and spectrum plots.\n        Defaults to True, which plots one standard deviation above\/below.\n        If set to float allows to control how many standard deviations are\n        plotted. For example 2.5 will plot 2.5 standard deviation above\/below.\n    topomap_args : dict | None\n        Dictionary of arguments to ``plot_topomap``. If None, doesn't pass any\n        additional arguments. Defaults to None.\n    image_args : dict | None\n        Dictionary of arguments to ``plot_epochs_image``. If None, doesn't pass\n        any additional arguments. Defaults to None.\n    psd_args : dict | None\n        Dictionary of arguments to ``psd_multitaper``. If None, doesn't pass\n        any additional arguments. Defaults to None.\n    reject : 'auto' | dict | None\n        Allows to specify rejection parameters used to drop epochs\n        (or segments if continuous signal is passed as inst).\n        If None, no rejection is applied. The default is 'auto',\n        which applies the rejection parameters used when fitting\n        the ICA object.\n    %(topomap_sphere_auto)s\n    %(verbose)s\n\n    Returns\n    -------\n    fig : instance of matplotlib.figure.Figure or list\n        The figure object(s).\n\n    Notes\n    -----\n    When run in interactive mode, ``plot_ica_components`` allows to reject\n    components by clicking on their title label. The state of each component\n    is indicated by its label color (gray: rejected; black: retained). It is\n    also possible to open component properties by clicking on the component\n    topomap (this option is only available when the ``inst`` argument is\n    supplied).\n    \"\"\"\n    from ..io import BaseRaw\n    from ..epochs import BaseEpochs\n\n    if ica.info is None:\n        raise RuntimeError('The ICA\\'s measurement info is missing. Please '\n                           'fit the ICA or add the corresponding info object.')\n\n    topomap_args = dict() if topomap_args is None else topomap_args\n    topomap_args = copy.copy(topomap_args)\n    if 'sphere' not in topomap_args:\n        topomap_args['sphere'] = sphere\n    if picks is None:  # plot components by sets of 20\n        ch_type = _get_ch_type(ica, ch_type)\n        n_components = ica.mixing_matrix_.shape[1]\n        p = 20\n        figs = []\n        for k in range(0, n_components, p):\n            picks = range(k, min(k + p, n_components))\n            fig = plot_ica_components(\n                ica, picks=picks, ch_type=ch_type, res=res, vmax=vmax,\n                cmap=cmap, sensors=sensors, colorbar=colorbar, title=title,\n                show=show, outlines=outlines, contours=contours,\n                image_interp=image_interp, inst=inst, plot_std=plot_std,\n                topomap_args=topomap_args, image_args=image_args,\n                psd_args=psd_args, reject=reject, sphere=sphere)\n            figs.append(fig)\n        return figs\n    else:\n        picks = _picks_to_idx(ica.info, picks)\n    ch_type = _get_ch_type(ica, ch_type)\n\n    cmap = _setup_cmap(cmap, n_axes=len(picks))\n    data = np.dot(ica.mixing_matrix_[:, picks].T,\n                  ica.pca_components_[:ica.n_components_])\n\n    data_picks, pos, merge_channels, names, ch_type, sphere, clip_origin = \\\n        _prepare_topomap_plot(ica, ch_type, sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n\n    data = np.atleast_2d(data)\n    data = data[:, data_picks]\n\n    # prepare data for iteration\n    fig, axes, _, _ = _prepare_trellis(len(data), ncols=5)\n    if title is None:\n        title = 'ICA components'\n    fig.suptitle(title)\n\n    titles = list()\n    for ii, data_, ax in zip(picks, data, axes):\n        kwargs = dict(color='gray') if ii in ica.exclude else dict()\n        titles.append(ax.set_title(ica._ica_names[ii], fontsize=12, **kwargs))\n        if merge_channels:\n            data_, names_ = _merge_ch_data(data_, ch_type, names.copy())\n        vmin_, vmax_ = _setup_vmin_vmax(data_, vmin, vmax)\n        im = plot_topomap(\n            data_.flatten(), pos, vmin=vmin_, vmax=vmax_, res=res, axes=ax,\n            cmap=cmap[0], outlines=outlines, contours=contours,\n            image_interp=image_interp, show=False, sensors=sensors,\n            ch_type=ch_type, **topomap_args)[0]\n        im.axes.set_label(ica._ica_names[ii])\n        if colorbar:\n            cbar, cax = _add_colorbar(ax, im, cmap, title=\"AU\",\n                                      side=\"right\", pad=.05, format='%3.2f')\n            cbar.ax.tick_params(labelsize=12)\n            cbar.set_ticks((vmin_, vmax_))\n        _hide_frame(ax)\n    del pos\n    tight_layout(fig=fig)\n    fig.subplots_adjust(top=0.88, bottom=0.)\n    fig.canvas.draw()\n\n    # add title selection interactivity\n    def onclick_title(event, ica=ica, titles=titles):\n        # check if any title was pressed\n        title_pressed = None\n        for title in titles:\n            if title.contains(event)[0]:\n                title_pressed = title\n                break\n        # title was pressed -> identify the IC\n        if title_pressed is not None:\n            label = title_pressed.get_text()\n            ic = int(label[-3:])\n            # add or remove IC from exclude depending on current state\n            if ic in ica.exclude:\n                ica.exclude.remove(ic)\n                title_pressed.set_color('k')\n            else:\n                ica.exclude.append(ic)\n                title_pressed.set_color('gray')\n            fig.canvas.draw()\n    fig.canvas.mpl_connect('button_press_event', onclick_title)\n\n    # add plot_properties interactivity only if inst was passed\n    if isinstance(inst, (BaseRaw, BaseEpochs)):\n        def onclick_topo(event, ica=ica, inst=inst):\n            # check which component to plot\n            if event.inaxes is not None:\n                label = event.inaxes.get_label()\n                if label.startswith('ICA'):\n                    ic = int(label[-3:])\n                    ica.plot_properties(inst, picks=ic, show=True,\n                                        plot_std=plot_std,\n                                        topomap_args=topomap_args,\n                                        image_args=image_args,\n                                        psd_args=psd_args, reject=reject)\n        fig.canvas.mpl_connect('button_press_event', onclick_topo)\n\n    plt_show(show)\n    return fig\n\n\n@fill_doc\ndef plot_tfr_topomap(tfr, tmin=None, tmax=None, fmin=None, fmax=None,\n                     ch_type=None, baseline=None, mode='mean',\n                     vmin=None, vmax=None, cmap=None, sensors=True,\n                     colorbar=True, unit=None, res=64, size=2,\n                     cbar_fmt='%1.1e', show_names=False, title=None,\n                     axes=None, show=True, outlines='head',\n                     contours=6, sphere=None):\n    \"\"\"Plot topographic maps of specific time-frequency intervals of TFR data.\n\n    Parameters\n    ----------\n    tfr : AverageTFR\n        The AverageTFR object.\n    tmin : None | float\n        The first time instant to display. If None the first time point\n        available is used.\n    tmax : None | float\n        The last time instant to display. If None the last time point available\n        is used.\n    fmin : None | float\n        The first frequency to display. If None the first frequency available\n        is used.\n    fmax : None | float\n        The last frequency to display. If None the last frequency available is\n        used.\n    ch_type : 'mag' | 'grad' | 'planar1' | 'planar2' | 'eeg' | None\n        The channel type to plot. For 'grad', the gradiometers are collected in\n        pairs and the mean for each pair is plotted. If None, then channels are\n        chosen in the order given above.\n    baseline : tuple or list of length 2\n        The time interval to apply rescaling \/ baseline correction. If None do\n        not apply it. If baseline is (a, b) the interval is between \"a (s)\" and\n        \"b (s)\". If a is None the beginning of the data is used and if b is\n        None then b is set to the end of the interval. If baseline is equal to\n        (None, None) the whole time interval is used.\n    mode : 'mean' | 'ratio' | 'logratio' | 'percent' | 'zscore' | 'zlogratio' | None\n        Perform baseline correction by\n\n          - subtracting the mean baseline power ('mean')\n          - dividing by the mean baseline power ('ratio')\n          - dividing by the mean baseline power and taking the log ('logratio')\n          - subtracting the mean baseline power followed by dividing by the\n            mean baseline power ('percent')\n          - subtracting the mean baseline power and dividing by the standard\n            deviation of the baseline power ('zscore')\n          - dividing by the mean baseline power, taking the log, and dividing\n            by the standard deviation of the baseline power ('zlogratio')\n\n        If None no baseline correction is applied.\n    vmin : float | callable | None\n        The value specifying the lower bound of the color range.\n        If None, and vmax is None, -vmax is used. Else np.min(data) or in case\n        data contains only positive values 0. If callable, the output equals\n        vmin(data). Defaults to None.\n    vmax : float | callable | None\n        The value specifying the upper bound of the color range. If None, the\n        maximum value is used. If callable, the output equals vmax(data).\n        Defaults to None.\n    cmap : matplotlib colormap | (colormap, bool) | 'interactive' | None\n        Colormap to use. If tuple, the first value indicates the colormap to\n        use and the second value is a boolean defining interactivity. In\n        interactive mode the colors are adjustable by clicking and dragging the\n        colorbar with left and right mouse button. Left mouse button moves the\n        scale up and down and right mouse button adjusts the range. Hitting\n        space bar resets the range. Up and down arrows can be used to change\n        the colormap. If None (default), 'Reds' is used for all positive data,\n        otherwise defaults to 'RdBu_r'. If 'interactive', translates to\n        (None, True).\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib plot\n        format string (e.g., 'r+'). If True (default), circles will be used.\n    colorbar : bool\n        Plot a colorbar.\n    unit : str | None\n        The unit of the channel type used for colorbar labels.\n    res : int\n        The resolution of the topomap image (n pixels along each side).\n    size : float\n        Side length per topomap in inches (only applies when plotting multiple\n        topomaps at a time).\n    cbar_fmt : str\n        String format for colorbar values.\n    %(topomap_show_names)s\n    title : str | None\n        Plot title. If None (default), no title is displayed.\n    axes : instance of Axes | None\n        The axes to plot to. If None the axes is defined automatically.\n    show : bool\n        Show figure if True.\n    %(topomap_outlines)s\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        When an integer, matplotlib ticker locator is used to find suitable\n        values for the contour thresholds (may sometimes be inaccurate, use\n        array for accuracy). If an array, the values represent the levels for\n        the contours. If colorbar=True, the ticks in colorbar correspond to the\n        contour levels. Defaults to 6.\n    %(topomap_sphere_auto)s\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure containing the topography.\n    \"\"\"  # noqa: E501\n    import matplotlib.pyplot as plt\n    ch_type = _get_ch_type(tfr, ch_type)\n\n    picks, pos, merge_channels, names, _, sphere, clip_origin = \\\n        _prepare_topomap_plot(tfr, ch_type, sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n\n    if not show_names:\n        names = None\n\n    data = tfr.data[picks, :, :]\n\n    # merging grads before rescaling makes ERDs visible\n    if merge_channels:\n        data, names = _merge_ch_data(data, ch_type, names, method='mean')\n\n    data = rescale(data, tfr.times, baseline, mode, copy=True)\n\n    # crop time\n    itmin, itmax = None, None\n    idx = np.where(_time_mask(tfr.times, tmin, tmax))[0]\n    if tmin is not None:\n        itmin = idx[0]\n    if tmax is not None:\n        itmax = idx[-1] + 1\n\n    # crop freqs\n    ifmin, ifmax = None, None\n    idx = np.where(_time_mask(tfr.freqs, fmin, fmax))[0]\n    if fmin is not None:\n        ifmin = idx[0]\n    if fmax is not None:\n        ifmax = idx[-1] + 1\n\n    data = data[:, ifmin:ifmax, itmin:itmax]\n    data = np.mean(np.mean(data, axis=2), axis=1)[:, np.newaxis]\n\n    norm = False if np.min(data) < 0 else True\n    vmin, vmax = _setup_vmin_vmax(data, vmin, vmax, norm)\n    cmap = _setup_cmap(cmap, norm=norm)\n\n    axes = plt.subplots(figsize=(size, size))[1] if axes is None else axes\n    fig = axes.figure\n\n    _hide_frame(axes)\n\n    locator = None\n    if not isinstance(contours, (list, np.ndarray)):\n        locator, contours = _set_contour_locator(vmin, vmax, contours)\n\n    if title is not None:\n        axes.set_title(title)\n    fig_wrapper = list()\n    selection_callback = partial(_onselect, tfr=tfr, pos=pos, ch_type=ch_type,\n                                 itmin=itmin, itmax=itmax, ifmin=ifmin,\n                                 ifmax=ifmax, cmap=cmap[0], fig=fig_wrapper)\n\n    if not isinstance(contours, (list, np.ndarray)):\n        _, contours = _set_contour_locator(vmin, vmax, contours)\n\n    im, _ = plot_topomap(data[:, 0], pos, vmin=vmin, vmax=vmax,\n                         axes=axes, cmap=cmap[0], image_interp='bilinear',\n                         contours=contours, names=names, show_names=show_names,\n                         show=False, onselect=selection_callback,\n                         sensors=sensors, res=res, ch_type=ch_type,\n                         outlines=outlines, sphere=sphere)\n\n    if colorbar:\n        from matplotlib import ticker\n        unit = _handle_default('units', unit)['misc']\n        cbar, cax = _add_colorbar(axes, im, cmap, title=unit, format=cbar_fmt)\n        if locator is None:\n            locator = ticker.MaxNLocator(nbins=5)\n        cbar.locator = locator\n        cbar.update_ticks()\n        cbar.ax.tick_params(labelsize=12)\n\n    plt_show(show)\n    return fig\n\n\n@fill_doc\ndef plot_evoked_topomap(evoked, times=\"auto\", ch_type=None,\n                        vmin=None, vmax=None, cmap=None, sensors=True,\n                        colorbar=True, scalings=None,\n                        units=None, res=64, size=1, cbar_fmt='%3.1f',\n                        time_unit='s', time_format=None, proj=False,\n                        show=True, show_names=False, title=None, mask=None,\n                        mask_params=None, outlines='head', contours=6,\n                        image_interp='bilinear', average=None,\n                        axes=None, extrapolate=_EXTRAPOLATE_DEFAULT,\n                        sphere=None, border=_BORDER_DEFAULT,\n                        nrows=1, ncols='auto'):\n    \"\"\"Plot topographic maps of specific time points of evoked data.\n\n    Parameters\n    ----------\n    evoked : Evoked\n        The Evoked object.\n    times : float | array of float | \"auto\" | \"peaks\" | \"interactive\"\n        The time point(s) to plot. If \"auto\", the number of ``axes`` determines\n        the amount of time point(s). If ``axes`` is also None, at most 10\n        topographies will be shown with a regular time spacing between the\n        first and last time instant. If \"peaks\", finds time points\n        automatically by checking for local maxima in global field power. If\n        \"interactive\", the time can be set interactively at run-time by using a\n        slider.\n    %(topomap_ch_type)s\n    %(topomap_vmin_vmax)s\n    %(topomap_cmap)s\n    %(topomap_sensors)s\n    %(topomap_colorbar)s\n    %(topomap_scalings)s\n    %(topomap_units)s\n    %(topomap_res)s\n    %(topomap_size)s\n    %(topomap_cbar_fmt)s\n    time_unit : str\n        The units for the time axis, can be \"ms\" or \"s\" (default).\n\n        .. versionadded:: 0.16\n    time_format : str | None\n        String format for topomap values. Defaults (None) to \"%%01d ms\" if\n        ``time_unit='ms'``, \"%%0.3f s\" if ``time_unit='s'``, and\n        \"%%g\" otherwise. Can be an empty string to omit the time label.\n    %(plot_proj)s\n    %(show)s\n    %(topomap_show_names)s\n    %(title_None)s\n    %(topomap_mask)s\n    %(topomap_mask_params)s\n    %(topomap_outlines)s\n    %(topomap_contours)s\n    %(topomap_image_interp)s\n    %(topomap_average)s\n    %(topomap_axes)s\n    %(topomap_extrapolate)s\n\n        .. versionadded:: 0.18\n    %(topomap_sphere_auto)s\n    %(topomap_border)s\n    nrows : int | 'auto'\n        The number of rows of topographies to plot. Defaults to 1. If 'auto',\n        obtains the number of rows depending on the amount of times to plot\n        and the number of cols. Not valid when times == 'interactive'.\n\n        .. versionadded:: 0.20\n    ncols : int | 'auto'\n        The number of columns of topographies to plot. If 'auto' (default),\n        obtains the number of columns depending on the amount of times to plot\n        and the number of rows. Not valid when times == 'interactive'.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    fig : instance of matplotlib.figure.Figure\n       The figure.\n\n    Notes\n    -----\n    When existing ``axes`` are provided and ``colorbar=True``, note that the\n    colorbar scale will only accurately reflect topomaps that are generated in\n    the same call as the colorbar. Note also that the colorbar will not be\n    resized automatically when ``axes`` are provided; use matplotlib's\n    :meth:`axes.set_position() <matplotlib.axes.Axes.set_position>` method or\n    :doc:`gridspec <matplotlib:tutorials\/intermediate\/gridspec>` interface to\n    adjust the colorbar size yourself.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.gridspec import GridSpec\n    from matplotlib.widgets import Slider\n    from ..evoked import Evoked\n\n    _validate_type(evoked, Evoked, 'evoked')\n    _validate_type(colorbar, bool, 'colorbar')\n    evoked = evoked.copy()  # make a copy, since we'll be picking\n    ch_type = _get_ch_type(evoked, ch_type)\n    # time units \/ formatting\n    time_unit, _ = _check_time_unit(time_unit, evoked.times)\n    scaling_time = 1. if time_unit == 's' else 1e3\n    _validate_type(time_format, (None, str), 'time_format')\n    if time_format is None:\n        time_format = '%0.3f s' if time_unit == 's' else '%01d ms'\n    del time_unit\n    # mask_params defaults\n    mask_params = _handle_default('mask_params', mask_params)\n    mask_params['markersize'] *= size \/ 2.\n    mask_params['markeredgewidth'] *= size \/ 2.\n    # setup various parameters, and prepare outlines\n    picks, pos, merge_channels, names, ch_type, sphere, clip_origin = \\\n        _prepare_topomap_plot(evoked, ch_type, sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n    # check interactive\n    axes_given = axes is not None\n    interactive = isinstance(times, str) and times == 'interactive'\n    if interactive and axes_given:\n        raise ValueError(\"User-provided axes not allowed when \"\n                         \"times='interactive'.\")\n    # units, scalings\n    key = 'grad' if ch_type.startswith('planar') else ch_type\n    scaling = _handle_default('scalings', scalings)[key]\n    unit = _handle_default('units', units)[key]\n    # ch_names (required for NIRS)\n    ch_names = names\n    if not show_names:\n        names = None\n    # apply projections before picking. NOTE: the `if proj is True`\n    # anti-pattern is needed here to exclude proj='interactive'\n    _check_option('proj', proj, (True, False, 'interactive', 'reconstruct'))\n    if proj is True and not evoked.proj:\n        evoked.apply_proj()\n    elif proj == 'reconstruct':\n        evoked._reconstruct_proj()\n    data = evoked.data\n\n    # remove compensation matrices (safe: only plotting & already made copy)\n    evoked.info['comps'] = []\n    evoked = evoked._pick_drop_channels(picks)\n    # determine which times to plot\n    if isinstance(axes, plt.Axes):\n        axes = [axes]\n    n_peaks = len(axes) - int(colorbar) if axes_given else None\n    times = _process_times(evoked, times, n_peaks)\n    n_times = len(times)\n    space = 1 \/ (2. * evoked.info['sfreq'])\n    if (max(times) > max(evoked.times) + space or\n            min(times) < min(evoked.times) - space):\n        raise ValueError(f'Times should be between {evoked.times[0]:0.3} and '\n                         f'{evoked.times[-1]:0.3}.')\n    # create axes\n    want_axes = n_times + int(colorbar)\n    if interactive:\n        height_ratios = [5, 1]\n        nrows = 2\n        ncols = want_axes\n        width = size * ncols\n        height = size + max(0, 0.1 * (4 - size)) + bool(title) * 0.5\n        fig = figure_nobar(figsize=(width * 1.5, height * 1.5))\n        g_kwargs = {'left': 0.2, 'right': 0.8, 'bottom': 0.05, 'top': 0.9}\n        gs = GridSpec(nrows, ncols, height_ratios=height_ratios, **g_kwargs)\n        axes = []\n        for ax_idx in range(n_times):\n            axes.append(plt.subplot(gs[0, ax_idx]))\n    elif axes is None:\n        fig, axes, ncols, nrows = _prepare_trellis(\n            n_times, ncols=ncols, nrows=nrows, title=title,\n            colorbar=colorbar, size=size)\n    else:\n        nrows, ncols = None, None  # Deactivate ncols when axes were passed\n        fig = axes[0].get_figure()\n        # check: enough space for colorbar?\n        if len(axes) != want_axes:\n            cbar_err = ' plus one for the colorbar' if colorbar else ''\n            raise RuntimeError(f'You must provide {want_axes} axes (one for '\n                               f'each time{cbar_err}), got {len(axes)}.')\n    # figure margins\n    side_margin = plt.rcParams['figure.subplot.wspace'] \/ (2 * want_axes)\n    top_margin = max((0.05 if title is None else 0.25), .2 \/ size)\n    fig.subplots_adjust(left=side_margin, right=1 - side_margin, bottom=0,\n                        top=1 - top_margin)\n    # find first index that's >= (to rounding error) to each time point\n    time_idx = [np.where(_time_mask(evoked.times, tmin=t, tmax=None,\n                                    sfreq=evoked.info['sfreq']))[0][0]\n                for t in times]\n    # do averaging if requested\n    avg_err = '\"average\" must be `None` or a positive number of seconds'\n    if average is None:\n        data = data[np.ix_(picks, time_idx)]\n    elif not _is_numeric(average):\n        raise TypeError(f'{avg_err}; got type {type(average)}.')\n    elif average <= 0:\n        raise ValueError(f'{avg_err}; got {average}.')\n    else:\n        data_ = np.zeros((len(picks), len(time_idx)))\n        ave_time = average \/ 2.\n        iter_times = evoked.times[time_idx]\n        for ii, (idx, tmin_, tmax_) in enumerate(zip(time_idx,\n                                                     iter_times - ave_time,\n                                                     iter_times + ave_time)):\n            my_range = (tmin_ < evoked.times) & (evoked.times < tmax_)\n            data_[:, ii] = data[picks][:, my_range].mean(-1)\n        data = data_\n    # apply scalings and merge channels\n    data *= scaling\n    if merge_channels:\n        data, ch_names = _merge_ch_data(data, ch_type, ch_names)\n        if ch_type in _fnirs_types:\n            merge_channels = False\n    # apply mask if requested\n    if mask is not None:\n        if ch_type == 'grad':\n            mask_ = (mask[np.ix_(picks[::2], time_idx)] |\n                     mask[np.ix_(picks[1::2], time_idx)])\n        else:  # mag, eeg, planar1, planar2\n            mask_ = mask[np.ix_(picks, time_idx)]\n    # set up colormap\n    vlims = [_setup_vmin_vmax(data[:, i], vmin, vmax, norm=merge_channels)\n             for i in range(n_times)]\n    vmin = np.min(vlims)\n    vmax = np.max(vlims)\n    cmap = _setup_cmap(cmap, n_axes=n_times, norm=vmin >= 0)\n    # set up contours\n    if not isinstance(contours, (list, np.ndarray)):\n        _, contours = _set_contour_locator(vmin, vmax, contours)\n    # prepare for main loop over times\n    kwargs = dict(vmin=vmin, vmax=vmax, sensors=sensors, res=res, names=names,\n                  show_names=show_names, cmap=cmap[0], mask_params=mask_params,\n                  outlines=outlines, contours=contours,\n                  image_interp=image_interp, show=False,\n                  extrapolate=extrapolate, sphere=sphere, border=border,\n                  ch_type=ch_type)\n    images, contours_ = [], []\n    # loop over times\n    for idx, time in enumerate(times):\n        adjust_for_cbar = colorbar and ncols is not None and idx >= ncols - 1\n        ax_idx = idx + 1 if adjust_for_cbar else idx\n        tp, cn, interp = _plot_topomap(\n            data[:, idx], pos, axes=axes[ax_idx],\n            mask=mask_[:, idx] if mask is not None else None, **kwargs)\n\n        images.append(tp)\n        if cn is not None:\n            contours_.append(cn)\n        if time_format != '':\n            axes[ax_idx].set_title(time_format % (time * scaling_time))\n\n    if interactive:\n        axes.append(plt.subplot(gs[1, :-1]))\n        slider = Slider(axes[-1], 'Time', evoked.times[0], evoked.times[-1],\n                        times[0], valfmt='%1.2fs')\n        slider.vline.remove()  # remove initial point indicator\n        func = _merge_ch_data if merge_channels else lambda x: x\n        changed_callback = partial(_slider_changed, ax=axes[0],\n                                   data=evoked.data, times=evoked.times,\n                                   pos=pos, scaling=scaling, func=func,\n                                   time_format=time_format,\n                                   scaling_time=scaling_time, kwargs=kwargs)\n        slider.on_changed(changed_callback)\n        ts = np.tile(evoked.times, len(evoked.data)).reshape(evoked.data.shape)\n        axes[-1].plot(ts, evoked.data, color='k')\n        axes[-1].slider = slider\n    if title is not None:\n        plt.suptitle(title, verticalalignment='top', size='x-large')\n\n    if colorbar:\n        if interactive:\n            cax = plt.subplot(gs[0, -1])\n            _resize_cbar(cax, ncols, size)\n        elif nrows is None or ncols is None:\n            # axes were given by the user, so don't resize the colorbar\n            cax = axes[-1]\n        else:  # use the entire last column\n            cax = axes[ncols - 1]\n            _resize_cbar(cax, ncols, size)\n\n        if unit is not None:\n            cax.set_title(unit)\n        cbar = fig.colorbar(images[-1], ax=cax, cax=cax, format=cbar_fmt)\n        if cn is not None:\n            cbar.set_ticks(contours)\n        cbar.ax.tick_params(labelsize=7)\n        if cmap[1]:\n            for im in images:\n                im.axes.CB = DraggableColorbar(cbar, im)\n\n    if proj == 'interactive':\n        _check_delayed_ssp(evoked)\n        params = dict(\n            evoked=evoked, fig=fig, projs=evoked.info['projs'], picks=picks,\n            images=images, contours_=contours_, pos=pos, time_idx=time_idx,\n            res=res, plot_update_proj_callback=_plot_update_evoked_topomap,\n            merge_channels=merge_channels, scale=scaling, axes=axes,\n            contours=contours, interp=interp, extrapolate=extrapolate)\n        _draw_proj_checkbox(None, params)\n\n    plt_show(show, block=False)\n    if axes_given:\n        fig.canvas.draw()\n    return fig\n\n\ndef _resize_cbar(cax, n_fig_axes, size=1):\n    \"\"\"Resize colorbar.\"\"\"\n    cpos = cax.get_position()\n    if size <= 1:\n        cpos.x0 = 1 - (0.7 + 0.1 \/ size) \/ n_fig_axes\n    cpos.x1 = cpos.x0 + 0.1 \/ n_fig_axes\n    cpos.y0 = 0.2\n    cpos.y1 = 0.7\n    cax.set_position(cpos)\n\n\ndef _slider_changed(val, ax, data, times, pos, scaling, func, time_format,\n                    scaling_time, kwargs):\n    \"\"\"Handle selection in interactive topomap.\"\"\"\n    idx = np.argmin(np.abs(times - val))\n    data = func(data[:, idx]).ravel() * scaling\n    ax.clear()\n    im, _ = plot_topomap(data, pos, axes=ax, **kwargs)\n    if hasattr(ax, 'CB'):\n        ax.CB.mappable = im\n        _resize_cbar(ax.CB.cbar.ax, 2)\n    if time_format is not None:\n        ax.set_title(time_format % (val * scaling_time))\n\n\ndef _plot_topomap_multi_cbar(data, pos, ax, title=None, unit=None, vmin=None,\n                             vmax=None, cmap=None, outlines='head',\n                             colorbar=False, cbar_fmt='%3.3f',\n                             sphere=None, ch_type='eeg'):\n    \"\"\"Plot topomap multi cbar.\"\"\"\n    _hide_frame(ax)\n    vmin = np.min(data) if vmin is None else vmin\n    vmax = np.max(data) if vmax is None else vmax\n    # this definition of \"norm\" allows non-diverging colormap for cases where\n    # min & vmax are both negative (e.g., when they are power in dB)\n    signs = np.sign([vmin, vmax])\n    norm = len(set(signs)) == 1 or np.any(signs == 0)\n\n    cmap = _setup_cmap(cmap, norm=norm)\n    if title is not None:\n        ax.set_title(title, fontsize=10)\n    im, _ = plot_topomap(data, pos, vmin=vmin, vmax=vmax, axes=ax,\n                         cmap=cmap[0], image_interp='bilinear', contours=0,\n                         outlines=outlines, show=False, sphere=sphere,\n                         ch_type=ch_type)\n\n    if colorbar:\n        cbar, cax = _add_colorbar(ax, im, cmap, pad=0.25, title=None,\n                                  size=\"10%\", format=cbar_fmt)\n        cbar.set_ticks((vmin, vmax))\n        if unit is not None:\n            cbar.ax.set_ylabel(unit, fontsize=8)\n        cbar.ax.tick_params(labelsize=8)\n\n\n@verbose\ndef plot_epochs_psd_topomap(epochs, bands=None,\n                            tmin=None, tmax=None, proj=False,\n                            bandwidth=None, adaptive=False, low_bias=True,\n                            normalization='length', ch_type=None,\n                            cmap=None, agg_fun=None, dB=False, n_jobs=1,\n                            normalize=False, cbar_fmt='auto',\n                            outlines='head', axes=None, show=True,\n                            sphere=None, vlim=(None, None), verbose=None):\n    \"\"\"Plot the topomap of the power spectral density across epochs.\n\n    Parameters\n    ----------\n    epochs : instance of Epochs\n        The epochs object.\n    %(psd_topo_bands)s\n    tmin : float | None\n        Start time to consider.\n    tmax : float | None\n        End time to consider.\n    proj : bool\n        Apply projection.\n    bandwidth : float\n        The bandwidth of the multi taper windowing function in Hz. The default\n        value is a window half-bandwidth of 4 Hz.\n    adaptive : bool\n        Use adaptive weights to combine the tapered spectra into PSD\n        (slow, use n_jobs >> 1 to speed up computation).\n    low_bias : bool\n        Only use tapers with more than 90%% spectral concentration within\n        bandwidth.\n    normalization : str\n        Either \"full\" or \"length\" (default). If \"full\", the PSD will\n        be normalized by the sampling rate as well as the length of\n        the signal (as in nitime).\n    ch_type : 'mag' | 'grad' | 'planar1' | 'planar2' | 'eeg' | None\n        The channel type to plot. For 'grad', the gradiometers are collected in\n        pairs and the mean for each pair is plotted. If None, then first\n        available channel type from order given above is used. Defaults to\n        None.\n    %(psd_topo_cmap)s\n    %(psd_topo_agg_fun)s\n    %(psd_topo_dB)s\n    %(n_jobs)s\n    %(psd_topo_normalize)s\n    %(psd_topo_cbar_fmt)s\n    %(topomap_outlines)s\n    %(psd_topo_axes)s\n    show : bool\n        Show figure if True.\n    %(topomap_sphere_auto)s\n    %(psd_topo_vlim_joint)s\n    %(verbose)s\n\n    Returns\n    -------\n    fig : instance of Figure\n        Figure distributing one image per channel across sensor topography.\n    \"\"\"\n    ch_type = _get_ch_type(epochs, ch_type)\n    units = _handle_default('units', None)\n    unit = units[ch_type]\n\n    picks, pos, merge_channels, names, ch_type, sphere, clip_origin = \\\n        _prepare_topomap_plot(epochs, ch_type, sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n\n    psds, freqs = psd_multitaper(epochs, tmin=tmin, tmax=tmax,\n                                 bandwidth=bandwidth, adaptive=adaptive,\n                                 low_bias=low_bias,\n                                 normalization=normalization, picks=picks,\n                                 proj=proj, n_jobs=n_jobs)\n    psds = np.mean(psds, axis=0)\n\n    if merge_channels:\n        psds, names = _merge_ch_data(psds, ch_type, names, method='mean')\n\n    return plot_psds_topomap(\n        psds=psds, freqs=freqs, pos=pos, agg_fun=agg_fun,\n        bands=bands, cmap=cmap, dB=dB, normalize=normalize,\n        cbar_fmt=cbar_fmt, outlines=outlines, axes=axes, show=show,\n        sphere=sphere, vlim=vlim, unit=unit, ch_type=ch_type)\n\n\n@fill_doc\ndef plot_psds_topomap(\n        psds, freqs, pos, agg_fun=None, bands=None,\n        cmap=None, dB=True, normalize=False, cbar_fmt='%0.3f', outlines='head',\n        axes=None, show=True, sphere=None, vlim=(None, None), unit=None,\n        ch_type='eeg'):\n    \"\"\"Plot spatial maps of PSDs.\n\n    Parameters\n    ----------\n    psds : np.ndarray of float, shape (n_channels, n_freqs)\n        Power spectral densities\n    freqs : np.ndarray of float, shape (n_freqs)\n        Frequencies used to compute psds.\n    pos : numpy.ndarray of float, shape (n_sensors, 2)\n        The positions of the sensors.\n    %(psd_topo_agg_fun)s\n    %(psd_topo_bands)s\n    %(psd_topo_cmap)s\n    %(psd_topo_dB)s\n    %(psd_topo_normalize)s\n    %(psd_topo_cbar_fmt)s\n    %(topomap_outlines)s\n    %(psd_topo_axes)s\n    show : bool\n        Show figure if True.\n    %(topomap_sphere)s\n    %(psd_topo_vlim_joint)s\n    unit : str | None\n        Measurement unit to be displayed with the colorbar. If ``None``, no\n        unit is displayed (only \"power\" or \"dB\" as appropriate).\n    %(topomap_ch_type)s\n\n    Returns\n    -------\n    fig : instance of matplotlib.figure.Figure\n        Figure with a topomap subplot for each band.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    sphere = _check_sphere(sphere)\n\n    if cbar_fmt == 'auto':\n        cbar_fmt = '%0.1f' if dB else '%0.3f'\n\n    if bands is None:\n        bands = [(0, 4, 'Delta (0-4 Hz)'), (4, 8, 'Theta (4-8 Hz)'),\n                 (8, 12, 'Alpha (8-12 Hz)'), (12, 30, 'Beta (12-30 Hz)'),\n                 (30, 45, 'Gamma (30-45 Hz)')]\n    else:  # upconvert single freqs to band upper\/lower edges as needed\n        bin_spacing = np.diff(freqs)[0]\n        bin_edges = np.array([0, bin_spacing]) - bin_spacing \/ 2\n        bands = [tuple(bin_edges + freqs[np.argmin(np.abs(freqs - band[0]))]) +\n                 (band[1],) if len(band) == 2 else band for band in bands]\n\n    if agg_fun is None:\n        agg_fun = np.sum if normalize else np.mean\n\n    if normalize:\n        psds \/= psds.sum(axis=-1, keepdims=True)\n        assert np.allclose(psds.sum(axis=-1), 1.)\n\n    n_axes = len(bands)\n    if axes is not None:\n        _validate_if_list_of_axes(axes, n_axes)\n        fig = axes[0].figure\n    else:\n        fig, axes = plt.subplots(1, n_axes, figsize=(2 * n_axes, 1.5))\n        if n_axes == 1:\n            axes = [axes]\n\n    # handle vmin\/vmax\n    if vlim == 'joint':\n        _freq_masks = [(fmin < freqs) & (freqs < fmax)\n                       for (fmin, fmax, _) in bands]\n        _datas = [agg_fun(psds[:, _freq_mask], axis=1)\n                  for _freq_mask in _freq_masks]\n        _datas = [10 * np.log10(_d) if (dB and not normalize) else _d\n                  for _d in _datas]\n        vmin = np.array(_datas).min()\n        vmax = np.array(_datas).max()\n    else:\n        vmin, vmax = vlim\n\n    if unit is None:\n        unit = 'dB' if dB and not normalize else 'power'\n    else:\n        if '\/' in unit:\n            unit = '(%s)' % unit\n        unit += '\u00b2\/Hz'\n        if dB and not normalize:\n            unit += ' (dB)'\n\n    for ax, (fmin, fmax, title) in zip(axes, bands):\n        freq_mask = (fmin < freqs) & (freqs < fmax)\n        if freq_mask.sum() == 0:\n            raise RuntimeError('No frequencies in band \"%s\" (%s, %s)'\n                               % (title, fmin, fmax))\n        data = agg_fun(psds[:, freq_mask], axis=1)\n        if dB and not normalize:\n            data = 10 * np.log10(data)\n\n        _plot_topomap_multi_cbar(data, pos, ax, title=title, vmin=vmin,\n                                 vmax=vmax, cmap=cmap, outlines=outlines,\n                                 colorbar=True, unit=unit, cbar_fmt=cbar_fmt,\n                                 sphere=sphere, ch_type=ch_type)\n    tight_layout(fig=fig)\n    fig.canvas.draw()\n    plt_show(show)\n    return fig\n\n\n@fill_doc\ndef plot_layout(layout, picks=None, show=True):\n    \"\"\"Plot the sensor positions.\n\n    Parameters\n    ----------\n    layout : None | Layout\n        Layout instance specifying sensor positions.\n    %(picks_nostr)s\n    show : bool\n        Show figure if True. Defaults to True.\n\n    Returns\n    -------\n    fig : instance of Figure\n        Figure containing the sensor topography.\n\n    Notes\n    -----\n    .. versionadded:: 0.12.0\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(max(plt.rcParams['figure.figsize']),) * 2)\n    ax = fig.add_subplot(111)\n    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None,\n                        hspace=None)\n    ax.set(xticks=[], yticks=[], aspect='equal')\n    pos = np.array([(p[0] + p[2] \/ 2., p[1] + p[3] \/ 2.) for p in layout.pos])\n    outlines = dict(border=([0, 1, 1, 0, 0], [0, 0, 1, 1, 0]))\n    _draw_outlines(ax, outlines)\n    picks = _picks_to_idx(len(layout.names), picks)\n    pos = pos[picks]\n    names = np.array(layout.names)[picks]\n    for ii, (this_pos, ch_id) in enumerate(zip(pos, names)):\n        ax.annotate(ch_id, xy=this_pos[:2], horizontalalignment='center',\n                    verticalalignment='center', size='x-small')\n    ax.axis('off')\n    tight_layout(fig=fig, pad=0, w_pad=0, h_pad=0)\n    plt_show(show)\n    return fig\n\n\ndef _onselect(eclick, erelease, tfr, pos, ch_type, itmin, itmax, ifmin, ifmax,\n              cmap, fig, layout=None):\n    \"\"\"Handle drawing average tfr over channels called from topomap.\"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.collections import PathCollection\n    ax = eclick.inaxes\n    xmin = min(eclick.xdata, erelease.xdata)\n    xmax = max(eclick.xdata, erelease.xdata)\n    ymin = min(eclick.ydata, erelease.ydata)\n    ymax = max(eclick.ydata, erelease.ydata)\n    indices = ((pos[:, 0] < xmax) & (pos[:, 0] > xmin) &\n               (pos[:, 1] < ymax) & (pos[:, 1] > ymin))\n    colors = ['r' if ii else 'k' for ii in indices]\n    indices = np.where(indices)[0]\n    for collection in ax.collections:\n        if isinstance(collection, PathCollection):  # this is our \"scatter\"\n            collection.set_color(colors)\n    ax.figure.canvas.draw()\n    if len(indices) == 0:\n        return\n    data = tfr.data\n    if ch_type == 'mag':\n        picks = pick_types(tfr.info, meg=ch_type, ref_meg=False)\n        data = np.mean(data[indices, ifmin:ifmax, itmin:itmax], axis=0)\n        chs = [tfr.ch_names[picks[x]] for x in indices]\n    elif ch_type == 'grad':\n        grads = _pair_grad_sensors(tfr.info, layout=layout,\n                                   topomap_coords=False)\n        idxs = list()\n        for idx in indices:\n            idxs.append(grads[idx * 2])\n            idxs.append(grads[idx * 2 + 1])  # pair of grads\n        data = np.mean(data[idxs, ifmin:ifmax, itmin:itmax], axis=0)\n        chs = [tfr.ch_names[x] for x in idxs]\n    elif ch_type == 'eeg':\n        picks = pick_types(tfr.info, meg=False, eeg=True, ref_meg=False)\n        data = np.mean(data[indices, ifmin:ifmax, itmin:itmax], axis=0)\n        chs = [tfr.ch_names[picks[x]] for x in indices]\n    logger.info('Averaging TFR over channels ' + str(chs))\n    if len(fig) == 0:\n        fig.append(figure_nobar())\n    if not plt.fignum_exists(fig[0].number):\n        fig[0] = figure_nobar()\n    ax = fig[0].add_subplot(111)\n    itmax = len(tfr.times) - 1 if itmax is None else min(itmax,\n                                                         len(tfr.times) - 1)\n    ifmax = len(tfr.freqs) - 1 if ifmax is None else min(ifmax,\n                                                         len(tfr.freqs) - 1)\n    if itmin is None:\n        itmin = 0\n    if ifmin is None:\n        ifmin = 0\n    extent = (tfr.times[itmin] * 1e3, tfr.times[itmax] * 1e3, tfr.freqs[ifmin],\n              tfr.freqs[ifmax])\n\n    title = 'Average over %d %s channels.' % (len(chs), ch_type)\n    ax.set_title(title)\n    ax.set_xlabel('Time (ms)')\n    ax.set_ylabel('Frequency (Hz)')\n    img = ax.imshow(data, extent=extent, aspect=\"auto\", origin=\"lower\",\n                    cmap=cmap)\n    if len(fig[0].get_axes()) < 2:\n        fig[0].get_axes()[1].cbar = fig[0].colorbar(mappable=img)\n    else:\n        fig[0].get_axes()[1].cbar.on_mappable_changed(mappable=img)\n    fig[0].canvas.draw()\n    plt.figure(fig[0].number)\n    plt_show(True)\n\n\ndef _prepare_topomap(pos, ax, check_nonzero=True):\n    \"\"\"Prepare the topomap axis and check positions.\n\n    Hides axis frame and check that position information is present.\n    \"\"\"\n    _hide_frame(ax)\n    if check_nonzero and not pos.any():\n        raise RuntimeError('No position information found, cannot compute '\n                           'geometries for topomap.')\n\n\ndef _hide_frame(ax):\n    \"\"\"Hide axis frame for topomaps.\"\"\"\n    ax.get_yticks()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n    ax.set_frame_on(False)\n\n\ndef _check_extrapolate(extrapolate, ch_type):\n    _check_option('extrapolate', extrapolate, ('box', 'local', 'head', 'auto'))\n    if extrapolate == 'auto':\n        extrapolate = 'local' if ch_type in _MEG_CH_TYPES_SPLIT else 'head'\n    return extrapolate\n\n\n@verbose\ndef _init_anim(ax, ax_line, ax_cbar, params, merge_channels, sphere, ch_type,\n               extrapolate, verbose):\n    \"\"\"Initialize animated topomap.\"\"\"\n    logger.info('Initializing animation...')\n    data = params['data']\n    items = list()\n    if params['butterfly']:\n        all_times = params['all_times']\n        for idx in range(len(data)):\n            ax_line.plot(all_times, data[idx], color='k', lw=1)\n        vmin, vmax = _setup_vmin_vmax(data, None, None)\n        ax_line.set(yticks=np.around(np.linspace(vmin, vmax, 5), -1),\n                    xlim=all_times[[0, -1]])\n        params['line'] = ax_line.axvline(all_times[0], color='r')\n        items.append(params['line'])\n    if merge_channels:\n        from mne.channels.layout import _merge_ch_data\n        data, _ = _merge_ch_data(data, 'grad', [])\n    norm = True if np.min(data) > 0 else False\n    cmap = 'Reds' if norm else 'RdBu_r'\n\n    vmin, vmax = _setup_vmin_vmax(data, None, None, norm)\n\n    outlines = _make_head_outlines(sphere, params['pos'], 'head',\n                                   params['clip_origin'])\n\n    _hide_frame(ax)\n    extent, Xi, Yi, interp = _setup_interp(\n        params['pos'], 64, extrapolate, sphere, outlines, 0)\n\n    patch_ = _get_patch(outlines, extrapolate, interp, ax)\n\n    params['Zis'] = list()\n    for frame in params['frames']:\n        params['Zis'].append(interp.set_values(data[:, frame])(Xi, Yi))\n    Zi = params['Zis'][0]\n    zi_min = np.nanmin(params['Zis'])\n    zi_max = np.nanmax(params['Zis'])\n    cont_lims = np.linspace(zi_min, zi_max, 7, endpoint=False)[1:]\n    params.update({'vmin': vmin, 'vmax': vmax, 'Xi': Xi, 'Yi': Yi, 'Zi': Zi,\n                   'extent': extent, 'cmap': cmap, 'cont_lims': cont_lims})\n    # plot map and contour\n    im = ax.imshow(Zi, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower',\n                   aspect='equal', extent=extent,\n                   interpolation='bilinear')\n    ax.autoscale(enable=True, tight=True)\n    ax.figure.colorbar(im, cax=ax_cbar)\n    cont = ax.contour(Xi, Yi, Zi, levels=cont_lims, colors='k', linewidths=1)\n\n    im.set_clip_path(patch_)\n    text = ax.text(0.55, 0.95, '', transform=ax.transAxes, va='center',\n                   ha='right')\n    params['text'] = text\n    items.append(im)\n    items.append(text)\n    for col in cont.collections:\n        col.set_clip_path(patch_)\n\n    outlines_ = _draw_outlines(ax, outlines)\n\n    params.update({'patch': patch_, 'outlines': outlines_})\n    ax.figure.tight_layout()\n    return tuple(items) + tuple(cont.collections)\n\n\ndef _animate(frame, ax, ax_line, params):\n    \"\"\"Update animated topomap.\"\"\"\n    if params['pause']:\n        frame = params['frame']\n    time_idx = params['frames'][frame]\n\n    if params['time_unit'] == 'ms':\n        title = '%6.0f ms' % (params['times'][frame] * 1e3,)\n    else:\n        title = '%6.3f s' % (params['times'][frame],)\n    if params['blit']:\n        text = params['text']\n    else:\n        ax.cla()  # Clear old contours.\n        text = ax.text(0.45, 1.15, '', transform=ax.transAxes)\n        for k, (x, y) in params['outlines'].items():\n            if 'mask' in k:\n                continue\n            ax.plot(x, y, color='k', linewidth=1, clip_on=False)\n\n    _hide_frame(ax)\n    text.set_text(title)\n\n    vmin = params['vmin']\n    vmax = params['vmax']\n    Xi = params['Xi']\n    Yi = params['Yi']\n    Zi = params['Zis'][frame]\n    extent = params['extent']\n    cmap = params['cmap']\n    patch = params['patch']\n\n    im = ax.imshow(Zi, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower',\n                   aspect='equal', extent=extent, interpolation='bilinear')\n    cont_lims = params['cont_lims']\n    with warnings.catch_warnings(record=True):\n        warnings.simplefilter('ignore')\n        cont = ax.contour(\n            Xi, Yi, Zi, levels=cont_lims, colors='k', linewidths=1)\n\n    im.set_clip_path(patch)\n    for col in cont.collections:\n        col.set_clip_path(patch)\n\n    items = [im, text]\n    if params['butterfly']:\n        all_times = params['all_times']\n        line = params['line']\n        line.remove()\n        ylim = ax_line.get_ylim()\n        params['line'] = ax_line.axvline(all_times[time_idx], color='r')\n        ax_line.set_ylim(ylim)\n        items.append(params['line'])\n    params['frame'] = frame\n    return tuple(items) + tuple(cont.collections)\n\n\ndef _pause_anim(event, params):\n    \"\"\"Pause or continue the animation on mouse click.\"\"\"\n    params['pause'] = not params['pause']\n\n\ndef _key_press(event, params):\n    \"\"\"Handle key presses for the animation.\"\"\"\n    if event.key == 'left':\n        params['pause'] = True\n        params['frame'] = max(params['frame'] - 1, 0)\n    elif event.key == 'right':\n        params['pause'] = True\n        params['frame'] = min(params['frame'] + 1, len(params['frames']) - 1)\n\n\ndef _topomap_animation(evoked, ch_type, times, frame_rate, butterfly, blit,\n                       show, time_unit, sphere, extrapolate, *, verbose=None):\n    \"\"\"Make animation of evoked data as topomap timeseries.\n\n    See mne.evoked.Evoked.animate_topomap.\n    \"\"\"\n    from matplotlib import pyplot as plt, animation\n    if ch_type is None:\n        ch_type = _picks_by_type(evoked.info)[0][0]\n    if ch_type not in ('mag', 'grad', 'eeg',\n                       'hbo', 'hbr', 'fnirs_od', 'fnirs_cw_amplitude'):\n        raise ValueError(\"Channel type not supported. Supported channel \"\n                         \"types include 'mag', 'grad', 'eeg'. 'hbo', 'hbr', \"\n                         \"'fnirs_cw_amplitude', and 'fnirs_od'.\")\n    time_unit, _ = _check_time_unit(time_unit, evoked.times)\n    if times is None:\n        times = np.linspace(evoked.times[0], evoked.times[-1], 10)\n    times = np.array(times)\n\n    if times.ndim != 1:\n        raise ValueError('times must be 1D, got %d dimensions' % times.ndim)\n    if max(times) > evoked.times[-1] or min(times) < evoked.times[0]:\n        raise ValueError('All times must be inside the evoked time series.')\n    frames = [np.abs(evoked.times - time).argmin() for time in times]\n\n    picks, pos, merge_channels, _, ch_type, sphere, clip_origin = \\\n        _prepare_topomap_plot(evoked, ch_type, sphere=sphere)\n    data = evoked.data[picks, :]\n    data *= _handle_default('scalings')[ch_type]\n\n    fig = plt.figure(figsize=(6, 5))\n    shape = (8, 12)\n    colspan = shape[1] - 1\n    rowspan = shape[0] - bool(butterfly)\n    ax = plt.subplot2grid(shape, (0, 0), rowspan=rowspan, colspan=colspan)\n    if butterfly:\n        ax_line = plt.subplot2grid(shape, (rowspan, 0), colspan=colspan)\n    else:\n        ax_line = None\n    if isinstance(frames, Integral):\n        frames = np.linspace(0, len(evoked.times) - 1, frames).astype(int)\n    ax_cbar = plt.subplot2grid(shape, (0, colspan), rowspan=rowspan)\n    ax_cbar.set_title(_handle_default('units')[ch_type], fontsize=10)\n    extrapolate = _check_extrapolate(extrapolate, ch_type)\n\n    params = dict(data=data, pos=pos, all_times=evoked.times, frame=0,\n                  frames=frames, butterfly=butterfly, blit=blit,\n                  pause=False, times=times, time_unit=time_unit,\n                  clip_origin=clip_origin)\n    init_func = partial(_init_anim, ax=ax, ax_cbar=ax_cbar, ax_line=ax_line,\n                        params=params, merge_channels=merge_channels,\n                        sphere=sphere, ch_type=ch_type,\n                        extrapolate=extrapolate, verbose=verbose)\n    animate_func = partial(_animate, ax=ax, ax_line=ax_line, params=params)\n    pause_func = partial(_pause_anim, params=params)\n    fig.canvas.mpl_connect('button_press_event', pause_func)\n    key_press_func = partial(_key_press, params=params)\n    fig.canvas.mpl_connect('key_press_event', key_press_func)\n    if frame_rate is None:\n        frame_rate = evoked.info['sfreq'] \/ 10.\n    interval = 1000 \/ frame_rate  # interval is in ms\n    anim = animation.FuncAnimation(fig, animate_func, init_func=init_func,\n                                   frames=len(frames), interval=interval,\n                                   blit=blit)\n    fig.mne_animation = anim  # to make sure anim is not garbage collected\n    plt_show(show, block=False)\n    if 'line' in params:\n        # Finally remove the vertical line so it does not appear in saved fig.\n        params['line'].remove()\n\n    return fig, anim\n\n\ndef _set_contour_locator(vmin, vmax, contours):\n    \"\"\"Set correct contour levels.\"\"\"\n    locator = None\n    if isinstance(contours, Integral) and contours > 0:\n        from matplotlib import ticker\n        # nbins = ticks - 1, since 2 of the ticks are vmin and vmax, the\n        # correct number of bins is equal to contours + 1.\n        locator = ticker.MaxNLocator(nbins=contours + 1)\n        contours = locator.tick_values(vmin, vmax)\n    return locator, contours\n\n\ndef _plot_corrmap(data, subjs, indices, ch_type, ica, label, show, outlines,\n                  cmap, contours, template=False, sphere=None):\n    \"\"\"Customize ica.plot_components for corrmap.\"\"\"\n    if not template:\n        title = 'Detected components'\n        if label is not None:\n            title += ' of type ' + label\n    else:\n        title = \"Supplied template\"\n\n    picks = list(range(len(data)))\n\n    p = 20\n    if len(picks) > p:  # plot components by sets of 20\n        n_components = len(picks)\n        figs = [_plot_corrmap(data[k:k + p], subjs[k:k + p],\n                              indices[k:k + p], ch_type, ica, label, show,\n                              outlines=outlines, cmap=cmap, contours=contours)\n                for k in range(0, n_components, p)]\n        return figs\n    elif np.isscalar(picks):\n        picks = [picks]\n\n    data_picks, pos, merge_channels, names, _, sphere, clip_origin = \\\n        _prepare_topomap_plot(ica, ch_type, sphere=sphere)\n    outlines = _make_head_outlines(sphere, pos, outlines, clip_origin)\n\n    data = np.atleast_2d(data)\n    data = data[:, data_picks]\n\n    # prepare data for iteration\n    fig, axes, _, _ = _prepare_trellis(len(picks), ncols=5)\n    fig.suptitle(title)\n\n    for ii, data_, ax, subject, idx in zip(picks, data, axes, subjs, indices):\n        if template:\n            ttl = 'Subj. {}, {}'.format(subject, ica._ica_names[idx])\n            ax.set_title(ttl, fontsize=12)\n        else:\n            ax.set_title('Subj. {}'.format(subject))\n        if merge_channels:\n            data_, _ = _merge_ch_data(data_, ch_type, [])\n        vmin_, vmax_ = _setup_vmin_vmax(data_, None, None)\n        plot_topomap(data_.flatten(), pos, vmin=vmin_, vmax=vmax_,\n                     res=64, axes=ax, cmap=cmap, outlines=outlines,\n                     contours=contours, show=False, image_interp='bilinear')[0]\n        _hide_frame(ax)\n    tight_layout(fig=fig)\n    fig.subplots_adjust(top=0.8)\n    fig.canvas.draw()\n    plt_show(show)\n    return fig\n\n\ndef _trigradient(x, y, z):\n    \"\"\"Take gradients of z on a mesh.\"\"\"\n    from matplotlib.tri import CubicTriInterpolator, Triangulation\n    with warnings.catch_warnings():  # catch matplotlib warnings\n        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n        tri = Triangulation(x, y)\n        tci = CubicTriInterpolator(tri, z)\n        dx, dy = tci.gradient(tri.x, tri.y)\n    return dx, dy\n\n\n@fill_doc\ndef plot_arrowmap(data, info_from, info_to=None, scale=3e-10, vmin=None,\n                  vmax=None, cmap=None, sensors=True, res=64, axes=None,\n                  names=None, show_names=False, mask=None, mask_params=None,\n                  outlines='head', contours=6, image_interp='bilinear',\n                  show=True, onselect=None, extrapolate=_EXTRAPOLATE_DEFAULT,\n                  sphere=None):\n    \"\"\"Plot arrow map.\n\n    Compute arrowmaps, based upon the Hosaka-Cohen transformation\n    :footcite:`CohenHosaka1976`, these arrows represents an estimation of the\n    current flow underneath the MEG sensors. They are a poor man's MNE.\n\n    Since planar gradiometers takes gradients along latitude and longitude,\n    they need to be projected to the flattened manifold span by magnetometer\n    or radial gradiometers before taking the gradients in the 2D Cartesian\n    coordinate system for visualization on the 2D topoplot. You can use the\n    ``info_from`` and ``info_to`` parameters to interpolate from\n    gradiometer data to magnetometer data.\n\n    Parameters\n    ----------\n    data : array, shape (n_channels,)\n        The data values to plot.\n    info_from : instance of Info\n        The measurement info from data to interpolate from.\n    info_to : instance of Info | None\n        The measurement info to interpolate to. If None, it is assumed\n        to be the same as info_from.\n    scale : float, default 3e-10\n        To scale the arrows.\n    vmin : float | callable | None\n        The value specifying the lower bound of the color range.\n        If None, and vmax is None, -vmax is used. Else np.min(data).\n        If callable, the output equals vmin(data). Defaults to None.\n    vmax : float | callable | None\n        The value specifying the upper bound of the color range.\n        If None, the maximum absolute value is used. If callable, the output\n        equals vmax(data). Defaults to None.\n    cmap : matplotlib colormap | None\n        Colormap to use. If None, 'Reds' is used for all positive data,\n        otherwise defaults to 'RdBu_r'.\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib plot\n        format string (e.g., 'r+' for red plusses). If True (default), circles\n        will be used.\n    res : int\n        The resolution of the topomap image (n pixels along each side).\n    axes : instance of Axes | None\n        The axes to plot to. If None, a new figure will be created.\n    names : list | None\n        List of channel names. If None, channel names are not plotted.\n    %(topomap_show_names)s\n        If ``True``, a list of names must be provided (see ``names`` keyword).\n    mask : ndarray of bool, shape (n_channels, n_times) | None\n        The channels to be marked as significant at a given time point.\n        Indices set to ``True`` will be considered. Defaults to None.\n    mask_params : dict | None\n        Additional plotting parameters for plotting significant sensors.\n        Default (None) equals::\n\n            dict(marker='o', markerfacecolor='w', markeredgecolor='k',\n                 linewidth=0, markersize=4)\n    %(topomap_outlines)s\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        If an array, the values represent the levels for the contours. The\n        values are in \u00b5V for EEG, fT for magnetometers and fT\/m for\n        gradiometers. Defaults to 6.\n    image_interp : str\n        The image interpolation to be used. All matplotlib options are\n        accepted.\n    show : bool\n        Show figure if True.\n    onselect : callable | None\n        Handle for a function that is called when the user selects a set of\n        channels by rectangle selection (matplotlib ``RectangleSelector``). If\n        None interactive selection is disabled. Defaults to None.\n    %(topomap_extrapolate)s\n\n        .. versionadded:: 0.18\n    %(topomap_sphere_auto)s\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The Figure of the plot.\n\n    Notes\n    -----\n    .. versionadded:: 0.17\n\n    References\n    ----------\n    .. footbibliography::\n    \"\"\"\n    from matplotlib import pyplot as plt\n    from ..forward import _map_meg_or_eeg_channels\n\n    sphere = _check_sphere(sphere, info_from)\n    ch_type = _picks_by_type(info_from)\n\n    if len(ch_type) > 1:\n        raise ValueError('Multiple channel types are not supported.'\n                         'All channels must either be of type \\'grad\\' '\n                         'or \\'mag\\'.')\n    else:\n        ch_type = ch_type[0][0]\n\n    if ch_type not in ('mag', 'grad'):\n        raise ValueError(\"Channel type '%s' not supported. Supported channel \"\n                         \"types are 'mag' and 'grad'.\" % ch_type)\n\n    if info_to is None and ch_type == 'mag':\n        info_to = info_from\n    else:\n        ch_type = _picks_by_type(info_to)\n        if len(ch_type) > 1:\n            raise ValueError(\"Multiple channel types are not supported.\")\n        else:\n            ch_type = ch_type[0][0]\n\n        if ch_type != 'mag':\n            raise ValueError(\"only 'mag' channel type is supported. \"\n                             \"Got %s\" % ch_type)\n\n    if info_to is not info_from:\n        info_to = pick_info(info_to, pick_types(info_to, meg=True))\n        info_from = pick_info(info_from, pick_types(info_from, meg=True))\n        # XXX should probably support the \"origin\" argument\n        mapping = _map_meg_or_eeg_channels(\n            info_from, info_to, origin=(0., 0., 0.04), mode='accurate')\n        data = np.dot(mapping, data)\n\n    _, pos, _, _, _, sphere, clip_origin = \\\n        _prepare_topomap_plot(info_to, 'mag', sphere=sphere)\n    outlines = _make_head_outlines(\n        sphere, pos, outlines, clip_origin)\n    if axes is None:\n        fig, axes = plt.subplots()\n    else:\n        fig = axes.figure\n    plot_topomap(data, pos, axes=axes, vmin=vmin, vmax=vmax, cmap=cmap,\n                 sensors=sensors, res=res, names=names, show_names=show_names,\n                 mask=mask, mask_params=mask_params, outlines=outlines,\n                 contours=contours, image_interp=image_interp, show=False,\n                 onselect=onselect, extrapolate=extrapolate, sphere=sphere,\n                 ch_type=ch_type)\n    x, y = tuple(pos.T)\n    dx, dy = _trigradient(x, y, data)\n    dxx = dy.data\n    dyy = -dx.data\n    axes.quiver(x, y, dxx, dyy, scale=scale, color='k', lw=1, clip_on=False)\n    axes.figure.canvas.draw_idle()\n    with warnings.catch_warnings(record=True):\n        warnings.simplefilter('ignore')\n        tight_layout(fig=fig)\n    plt_show(show)\n\n    return fig\n","license":"bsd-3-clause","hash":8408466581736333339,"line_mean":40.1044427124,"line_max":84,"alpha_frac":0.586485769,"autogenerated":false},
{"repo_name":"yusufm\/mobly","path":"mobly\/controllers\/sniffer_lib\/local\/local_base.py","copies":"1","size":"5832","content":"#!\/usr\/bin\/env python3.4\n#\n# Copyright 2016 Google Inc.\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nClass for Local sniffers - i.e. running on the local machine.\n\nThis class provides configuration for local interfaces but leaves\nthe actual capture (sniff) to sub-classes.\n\"\"\"\n\nimport os\nimport shutil\nimport signal\nimport subprocess\nimport tempfile\nfrom mobly import logger\nfrom mobly import utils\nfrom mobly.controllers import sniffer\n\nclass SnifferLocalBase(sniffer.Sniffer):\n    \"\"\"This class defines the common behaviors of WLAN sniffers running on\n    WLAN interfaces of the local machine.\n\n    Specific mechanisms to capture packets over the local WLAN interfaces are\n    implemented by sub-classes of this class - i.e. it is not a final class.\n    \"\"\"\n\n    def __init__(self, interface, logger, base_configs=None):\n        \"\"\"See base class documentation\n        \"\"\"\n        self._base_configs = None\n        self._capture_file_path = \"\"\n        self._interface = \"\"\n        self._logger = logger\n        self._process = None\n        self._temp_capture_file_path = \"\"\n\n        if interface == \"\":\n            raise sniffer.InvalidDataError(\"Empty interface provided\")\n        self._interface = interface\n        self._base_configs = base_configs\n\n        try:\n            utils.exe_cmd(\"ifconfig\", self._interface, \"down\")\n            utils.exe_cmd(\"iwconfig\", self._interface, \"mode\", \"monitor\")\n            utils.exe_cmd(\"ifconfig\", self._interface, \"up\")\n        except Exception as err:\n            raise sniffer.ExecutionError(err)\n\n    def get_interface(self):\n        \"\"\"See base class documentation\n        \"\"\"\n        return self._interface\n\n    def get_type(self):\n        \"\"\"See base class documentation\n        \"\"\"\n        return \"local\"\n\n    def get_capture_file(self):\n        return self._capture_file_path\n\n    def _pre_capture_config(self, override_configs=None):\n        \"\"\"Utility function which configures the wireless interface per the\n        specified configurations. Operation is performed before every capture\n        start using baseline configurations (specified when sniffer initialized)\n        and override configurations specified here.\n        \"\"\"\n        final_configs = {}\n        if self._base_configs:\n            final_configs.update(self._base_configs)\n        if override_configs:\n            final_configs.update(override_configs)\n\n        if sniffer.Sniffer.CONFIG_KEY_CHANNEL in final_configs:\n            try:\n                utils.exe_cmd(\"iwconfig\", self._interface, \"channel\",\n                        str(final_configs[sniffer.Sniffer.CONFIG_KEY_CHANNEL]))\n            except Exception as err:\n                raise sniffer.ExecutionError(err)\n\n    def _get_command_line(self, additional_args=None, duration=None,\n                          packet_count=None):\n        \"\"\"Utility function to be implemented by every child class - which\n        are the concrete sniffer classes. Each sniffer-specific class should\n        derive the command line to execute its sniffer based on the specified\n        arguments.\n        \"\"\"\n        raise NotImplementedError(\"Base class should not be called directly!\")\n\n    def _post_process(self):\n        \"\"\"Utility function which is executed after a capture is done. It\n        moves the capture file to the requested location.\n        \"\"\"\n        self._process = None\n        shutil.move(self._temp_capture_file_path, self._capture_file_path)\n\n    def start_capture(self, override_configs=None,\n                      additional_args=None, duration=None,\n                      packet_count=None):\n        \"\"\"See base class documentation\n        \"\"\"\n        if self._process is not None:\n            raise sniffer.InvalidOperationError(\n                    \"Trying to start a sniff while another is still running!\")\n        capture_dir = os.path.join(self._logger.log_path,\n                                   \"Sniffer-{}\".format(self._interface))\n        os.makedirs(capture_dir, exist_ok=True)\n        self._capture_file_path = os.path.join(capture_dir,\n                      \"capture_{}.pcap\".format(logger.get_log_file_timestamp()))\n\n        self._pre_capture_config(override_configs)\n        _, self._temp_capture_file_path = tempfile.mkstemp(suffix=\".pcap\")\n\n        cmd = self._get_command_line(additional_args=additional_args,\n                                duration=duration, packet_count=packet_count)\n\n        self._process = utils.start_standing_subprocess(cmd)\n        return sniffer.ActiveCaptureContext(self, duration)\n\n    def stop_capture(self):\n        \"\"\"See base class documentation\n        \"\"\"\n        if self._process is None:\n            raise sniffer.InvalidOperationError(\n                                      \"Trying to stop a non-started process\")\n        utils.stop_standing_subprocess(self._process, kill_signal=signal.SIGINT)\n        self._post_process()\n\n    def wait_for_capture(self, timeout=None):\n        \"\"\"See base class documentation\n        \"\"\"\n        if self._process is None:\n            raise sniffer.InvalidOperationError(\n                                  \"Trying to wait on a non-started process\")\n        try:\n            utils.wait_for_standing_subprocess(self._process, timeout)\n            self._post_process()\n        except subprocess.TimeoutExpired:\n            self.stop_capture()\n","license":"apache-2.0","hash":-6454002212715778441,"line_mean":37.1176470588,"line_max":80,"alpha_frac":0.633744856,"autogenerated":false},
{"repo_name":"brenton\/cobbler","path":"cobbler\/api.py","copies":"1","size":"17907","content":"\"\"\"\npython API module for Cobbler\nsee source for cobbler.py, or pydoc, for example usage.\nCLI apps and daemons should import api.py, and no other cobbler code.\n\nCopyright 2006, Red Hat, Inc\nMichael DeHaan <mdehaan@redhat.com>\n\nThis software may be freely redistributed under the terms of the GNU\ngeneral public license.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n\"\"\"\n\nimport config\nimport utils\nimport action_sync\nimport action_check\nimport action_import\nimport action_reposync\nimport action_status\nimport action_validate\nimport action_buildiso\nimport action_replicate\nimport action_acl\nfrom cexceptions import *\nimport sub_process\nimport module_loader\nimport kickgen\nimport yumgen\n\nimport logging\nimport os\nimport fcntl\nfrom utils import _\n\nERROR = 100\nINFO  = 10\nDEBUG = 5\n\n# notes on locking:\n# BootAPI is a singleton object\n# the XMLRPC variants allow 1 simultaneous request\n# therefore we flock on \/etc\/cobbler\/settings for now\n# on a request by request basis.\n\nclass BootAPI:\n\n\n    __shared_state = {}\n    __has_loaded = False\n\n    def __init__(self):\n        \"\"\"\n        Constructor\n        \"\"\"\n\n        self.__dict__ = BootAPI.__shared_state\n        self.perms_ok = False\n        if not BootAPI.__has_loaded:\n\n            # NOTE: we do not log all API actions, because\n            # a simple CLI invocation may call adds and such\n            # to load the config, which would just fill up\n            # the logs, so we'll do that logging at CLI\n            # level (and remote.py web service level) instead.\n\n            try:\n                self.logger = self.__setup_logger(\"api\")\n            except CX:\n                # return to CLI\/other but perms are not valid\n                # perms_ok is False\n                return\n\n            self.logger_remote = self.__setup_logger(\"remote\")\n\n            BootAPI.__has_loaded   = True\n            module_loader.load_modules()\n            self._config         = config.Config(self)\n            self.deserialize()\n\n            self.authn = self.get_module_from_file(\n                \"authentication\",\n                \"module\",\n                \"authn_configfile\"\n            )\n            self.authz  = self.get_module_from_file(\n                \"authorization\",\n                \"module\",\n                \"authz_allowall\"\n            )\n            self.kickgen = kickgen.KickGen(self._config)\n            self.yumgen  = yumgen.YumGen(self._config)\n            self.logger.debug(\"API handle initialized\")\n            self.perms_ok = True\n \n    def __setup_logger(self,name):\n        return utils.setup_logger(name)\n\n    def log(self,msg,args=None,debug=False):\n        if debug:\n            logger = self.logger.debug\n        else:\n            logger = self.logger.info \n        if args is None:\n            logger(\"%s\" % msg)\n        else:\n            logger(\"%s; %s\" % (msg, str(args)))\n\n    def version(self):\n        \"\"\"\n        What version is cobbler?\n        Currently checks the RPM DB, which is not perfect.\n        Will return \"?\" if not installed.\n        \"\"\"\n        self.log(\"version\")\n        cmd = sub_process.Popen(\"\/bin\/rpm -q cobbler\", stdout=sub_process.PIPE, shell=True)\n        result = cmd.communicate()[0].replace(\"cobbler-\",\"\")\n        if result.find(\"not installed\") != -1:\n            return \"?\"\n        tokens = result[:result.rfind(\"-\")].split(\".\")\n        return int(tokens[0]) + 0.1 * int(tokens[1]) + 0.001 * int(tokens[2])\n\n    def clear(self):\n        \"\"\"\n        Forget about current list of profiles, distros, and systems\n        \"\"\"\n        return self._config.clear()\n\n    def __cmp(self,a,b):\n        return cmp(a.name,b.name)\n\n    def systems(self):\n        \"\"\"\n        Return the current list of systems\n        \"\"\"\n        return self._config.systems()\n\n    def profiles(self):\n        \"\"\"\n        Return the current list of profiles\n        \"\"\"\n        return self._config.profiles()\n\n    def distros(self):\n        \"\"\"\n        Return the current list of distributions\n        \"\"\"\n        return self._config.distros()\n\n    def repos(self):\n        \"\"\"\n        Return the current list of repos\n        \"\"\"\n        return self._config.repos()\n\n    def images(self):\n        \"\"\"\n        Return the current list of images\n        \"\"\"\n        return self._config.images()\n\n    def settings(self):\n        \"\"\"\n        Return the application configuration\n        \"\"\"\n        return self._config.settings()\n\n    def copy_distro(self, ref, newname):\n        self.log(\"copy_distro\",[ref.name, newname])\n        return self._config.distros().copy(ref,newname)\n\n    def copy_profile(self, ref, newname):\n        self.log(\"copy_profile\",[ref.name, newname])\n        return self._config.profiles().copy(ref,newname)\n\n    def copy_system(self, ref, newname):\n        self.log(\"copy_system\",[ref.name, newname])\n        return self._config.systems().copy(ref,newname)\n\n    def copy_repo(self, ref, newname):\n        self.log(\"copy_repo\",[ref.name, newname])\n        return self._config.repos().copy(ref,newname)\n    \n    def copy_image(self, ref, newname):\n        self.log(\"copy_image\",[ref.name, newname])\n        return self._config.images().copy(ref,newname)\n\n    def remove_distro(self, ref, recursive=False):\n        self.log(\"remove_distro\",[ref.name])\n        return self._config.distros().remove(ref.name, recursive=recursive)\n\n    def remove_profile(self,ref, recursive=False):\n        self.log(\"remove_profile\",[ref.name])\n        return self._config.profiles().remove(ref.name, recursive=recursive)\n\n    def remove_system(self,ref):\n        self.log(\"remove_system\",[ref.name])\n        return self._config.systems().remove(ref.name)\n\n    def remove_repo(self, ref):\n        self.log(\"remove_repo\",[ref.name])\n        return self._config.repos().remove(ref.name)\n    \n    def remove_image(self, ref):\n        self.log(\"remove_image\",[ref.name])\n        return self._config.images().remove(ref.name)\n\n    def rename_distro(self, ref, newname):\n        self.log(\"rename_distro\",[ref.name,newname])\n        return self._config.distros().rename(ref,newname)\n\n    def rename_profile(self, ref, newname):\n        self.log(\"rename_profiles\",[ref.name,newname])\n        return self._config.profiles().rename(ref,newname)\n\n    def rename_system(self, ref, newname):\n        self.log(\"rename_system\",[ref.name,newname])\n        return self._config.systems().rename(ref,newname)\n\n    def rename_repo(self, ref, newname):\n        self.log(\"rename_repo\",[ref.name,newname])\n        return self._config.repos().rename(ref,newname)\n    \n    def rename_image(self, ref, newname):\n        self.log(\"rename_image\",[ref.name,newname])\n        return self._config.image().rename(ref,newname)\n\n    def new_distro(self,is_subobject=False):\n        self.log(\"new_distro\",[is_subobject])\n        return self._config.new_distro(is_subobject=is_subobject)\n\n    def new_profile(self,is_subobject=False):\n        self.log(\"new_profile\",[is_subobject])\n        return self._config.new_profile(is_subobject=is_subobject)\n    \n    def new_system(self,is_subobject=False):\n        self.log(\"new_system\",[is_subobject])\n        return self._config.new_system(is_subobject=is_subobject)\n\n    def new_repo(self,is_subobject=False):\n        self.log(\"new_repo\",[is_subobject])\n        return self._config.new_repo(is_subobject=is_subobject)\n    \n    def new_image(self,is_subobject=False):\n        self.log(\"new_image\",[is_subobject])\n        return self._config.new_image(is_subobject=is_subobject)\n\n    def add_distro(self, ref, check_for_duplicate_names=False):\n        self.log(\"add_distro\",[ref.name])\n        return self._config.distros().add(ref,save=True,check_for_duplicate_names=check_for_duplicate_names)\n\n    def add_profile(self, ref, check_for_duplicate_names=False):\n        self.log(\"add_profile\",[ref.name])\n        return self._config.profiles().add(ref,save=True,check_for_duplicate_names=check_for_duplicate_names)\n\n    def add_system(self, ref, check_for_duplicate_names=False, check_for_duplicate_netinfo=False):\n        self.log(\"add_system\",[ref.name])\n        return self._config.systems().add(ref,save=True,check_for_duplicate_names=check_for_duplicate_names,check_for_duplicate_netinfo=check_for_duplicate_netinfo)\n\n    def add_repo(self, ref, check_for_duplicate_names=False):\n        self.log(\"add_repo\",[ref.name])\n        return self._config.repos().add(ref,save=True,check_for_duplicate_names=check_for_duplicate_names)\n    \n    def add_image(self, ref, check_for_duplicate_names=False):\n        self.log(\"add_image\",[ref.name])\n        return self._config.images().add(ref,save=True,check_for_duplicate_names=check_for_duplicate_names)\n\n    def find_distro(self, name=None, return_list=False, no_errors=False, **kargs):\n        return self._config.distros().find(name=name, return_list=return_list, no_errors=no_errors, **kargs)\n\n    def find_profile(self, name=None, return_list=False, no_errors=False, **kargs):\n        return self._config.profiles().find(name=name, return_list=return_list, no_errors=no_errors, **kargs)\n\n    def find_system(self, name=None, return_list=False, no_errors=False, **kargs):\n        return self._config.systems().find(name=name, return_list=return_list, no_errors=no_errors, **kargs)\n\n    def find_repo(self, name=None, return_list=False, no_errors=False, **kargs):\n        return self._config.repos().find(name=name, return_list=return_list, no_errors=no_errors, **kargs)\n\n    def find_image(self, name=None, return_list=False, no_errors=False, **kargs):\n        return self._config.images().find(name=name, return_list=return_list, no_errors=no_errors, **kargs)\n\n    def dump_vars(self, obj, format=False):\n        return obj.dump_vars(format)\n\n    def auto_add_repos(self):\n        \"\"\"\n        Import any repos this server knows about and mirror them.\n        Credit: Seth Vidal.\n        \"\"\"\n        self.log(\"auto_add_repos\")\n        try:\n            import yum\n        except:\n            raise CX(_(\"yum is not installed\"))\n\n        version = yum.__version__\n        (a,b,c) = version.split(\".\")\n        version = a* 1000 + b*100 + c\n        if version < 324:\n            raise CX(_(\"need yum > 3.2.4 to proceed\"))\n\n        base = yum.YumBase()\n        base.doRepoSetup()\n        repos = base.repos.listEnabled()\n        if len(repos) == 0:\n            raise CX(_(\"no repos enabled\/available -- giving up.\"))\n\n        for repo in repos:\n            url = repo.urls[0]\n            cobbler_repo = self.new_repo()\n            auto_name = repo.name.replace(\" \",\"\")\n            # FIXME: probably doesn't work for yum-rhn-plugin ATM\n            cobbler_repo.set_mirror(url)\n            cobbler_repo.set_name(auto_name)\n            print \"auto adding: %s (%s)\" % (auto_name, url)\n            self._config.repos().add(cobbler_repo,save=True)\n\n        # run cobbler reposync to apply changes\n        return True \n\n    def get_repo_config_for_profile(self,obj):\n        return self.yumgen.get_yum_config(obj,True)\n    \n    def get_repo_config_for_system(self,obj):\n        return self.yumgen.get_yum_config(obj,False)\n\n    def generate_kickstart(self,profile,system):\n        self.log(\"generate_kickstart\")\n        if system:\n            return self.kickgen.generate_kickstart_for_system(system)\n        else:\n            return self.kickgen.generate_kickstart_for_profile(profile) \n\n    def check(self):\n        \"\"\"\n        See if all preqs for network booting are valid.  This returns\n        a list of strings containing instructions on things to correct.\n        An empty list means there is nothing to correct, but that still\n        doesn't mean there are configuration errors.  This is mainly useful\n        for human admins, who may, for instance, forget to properly set up\n        their TFTP servers for PXE, etc.\n        \"\"\"\n        self.log(\"check\")\n        check = action_check.BootCheck(self._config)\n        return check.run()\n\n    def validateks(self):\n        \"\"\"\n        Use ksvalidator (from pykickstart, if available) to determine\n        whether the cobbler kickstarts are going to be (likely) well\n        accepted by Anaconda.  Presence of an error does not indicate\n        the kickstart is bad, only that the possibility exists.  ksvalidator\n        is not available on all platforms and can not detect \"future\"\n        kickstart format correctness.\n        \"\"\"\n        self.log(\"validateks\")\n        validator = action_validate.Validate(self._config)\n        return validator.run()\n\n    def sync(self):\n        \"\"\"\n        Take the values currently written to the configuration files in\n        \/etc, and \/var, and build out the information tree found in\n        \/tftpboot.  Any operations done in the API that have not been\n        saved with serialize() will NOT be synchronized with this command.\n        \"\"\"\n        self.log(\"sync\")\n        sync = self.get_sync()\n        return sync.run()\n\n    def get_sync(self):\n        self.dhcp = self.get_module_from_file(\n           \"dhcp\",\n           \"module\",\n           \"manage_isc\"\n        ).get_manager(self._config)\n        self.dns = self.get_module_from_file(\n           \"dns\",\n           \"module\",\n           \"manage_bind\"\n        ).get_manager(self._config)\n        return action_sync.BootSync(self._config,dhcp=self.dhcp,dns=self.dns)\n\n    def reposync(self, name=None):\n        \"\"\"\n        Take the contents of \/var\/lib\/cobbler\/repos and update them --\n        or create the initial copy if no contents exist yet.\n        \"\"\"\n        self.log(\"reposync\",[name])\n        reposync = action_reposync.RepoSync(self._config)\n        return reposync.run(name)\n\n    def status(self,mode=None):\n        self.log(\"status\")\n        statusifier = action_status.BootStatusReport(self._config,mode)\n        return statusifier.run()\n\n    def import_tree(self,mirror_url,mirror_name,network_root=None,kickstart_file=None,rsync_flags=None,arch=None):\n        \"\"\"\n        Automatically import a directory tree full of distribution files.\n        mirror_url can be a string that represents a path, a user@host \n        syntax for SSH, or an rsync:\/\/ address.  If mirror_url is a \n        filesystem path and mirroring is not desired, set network_root \n        to something like \"nfs:\/\/path\/to\/mirror_url\/root\" \n        \"\"\"\n        self.log(\"import_tree\",[mirror_url, mirror_name, network_root, kickstart_file, rsync_flags])\n        importer = action_import.Importer(\n            self, self._config, mirror_url, mirror_name, network_root, kickstart_file, rsync_flags, arch\n        )\n        return importer.run()\n\n    def acl_config(self,adduser=None,addgroup=None,removeuser=None,removegroup=None):\n        \"\"\"\n        Configures users\/groups to run the cobbler CLI as non-root.\n        Pass in only one option at a time.  Powers \"cobbler aclconfig\"\n        \"\"\"\n        acl = action_acl.AclConfig(self._config)\n        return acl.run(\n            adduser=adduser,\n            addgroup=addgroup,\n            removeuser=removeuser,\n            removegroup=removegroup\n        )\n\n    def serialize(self):\n        \"\"\"\n        Save the config file(s) to disk.\n        \"\"\"\n        self.log(\"serialize\")\n        return self._config.serialize()\n\n    def deserialize(self):\n        \"\"\"\n        Load the current configuration from config file(s)\n        \"\"\"\n        return self._config.deserialize()\n\n    def deserialize_raw(self,collection_name):\n        \"\"\"\n        Get the collection back just as raw data.\n        \"\"\"\n        return self._config.deserialize_raw(collection_name)\n\n    def get_module_by_name(self,module_name):\n        \"\"\"\n        Returns a loaded cobbler module named 'name', if one exists, else None.\n        \"\"\"\n        return module_loader.get_module_by_name(module_name)\n\n    def get_module_from_file(self,section,name,fallback=None):\n        \"\"\"\n        Looks in \/etc\/cobbler\/modules.conf for a section called 'section'\n        and a key called 'name', and then returns the module that corresponds\n        to the value of that key.\n        \"\"\"\n        return module_loader.get_module_from_file(section,name,fallback)\n\n    def get_modules_in_category(self,category):\n        \"\"\"\n        Returns all modules in a given category, for instance \"serializer\", or \"cli\".\n        \"\"\"\n        return module_loader.get_modules_in_category(category)\n\n    def authenticate(self,user,password):\n        \"\"\"\n        (Remote) access control.\n        \"\"\"\n        rc = self.authn.authenticate(self,user,password)\n        self.log(\"authenticate\",[user,rc])\n        return rc \n\n    def authorize(self,user,resource,arg1=None,arg2=None):\n        \"\"\"\n        (Remote) access control.\n        \"\"\"\n        rc = self.authz.authorize(self,user,resource,arg1,arg2)\n        self.log(\"authorize\",[user,resource,arg1,arg2,rc],debug=True)\n        return rc\n\n    def build_iso(self,iso=None,profiles=None,systems=None,tempdir=None):\n        builder = action_buildiso.BuildIso(self._config)\n        return builder.run(\n           iso=iso, profiles=profiles, systems=systems, tempdir=tempdir\n        )\n\n    def replicate(self, cobbler_master = None, sync_all=False, sync_kickstarts=False, sync_trees=False, sync_repos=False, sync_triggers=False, systems=False):\n        \"\"\"\n        Pull down metadata from a remote cobbler server that is a master to this server.\n        Optionally rsync data from it.\n        \"\"\"\n        replicator = action_replicate.Replicate(self._config)\n        return replicator.run(\n              cobbler_master = cobbler_master,\n              sync_all = sync_all,\n              sync_kickstarts = sync_kickstarts,\n              sync_trees = sync_trees,\n              sync_repos = sync_repos,\n              sync_triggers = sync_triggers,\n              include_systems = systems\n        )\n\n    def get_kickstart_templates(self):\n        return utils.get_kickstar_templates(self)\n\n","license":"gpl-2.0","hash":-6467063034696452932,"line_mean":34.25,"line_max":164,"alpha_frac":0.6163511476,"autogenerated":false},
{"repo_name":"whummer\/moto","path":"tests\/test_elb\/test_elb.py","copies":"1","size":"35092","content":"from __future__ import unicode_literals\nimport boto3\nimport botocore\nimport boto\nimport boto.ec2.elb\nfrom boto.ec2.elb import HealthCheck\nfrom boto.ec2.elb.attributes import (\n    ConnectionSettingAttribute,\n    ConnectionDrainingAttribute,\n    AccessLogAttribute,\n)\nfrom botocore.exceptions import ClientError\nfrom boto.exception import BotoServerError\nfrom nose.tools import assert_raises\nimport sure  # noqa\n\nfrom moto import mock_elb, mock_ec2, mock_elb_deprecated, mock_ec2_deprecated\n\n\n@mock_elb_deprecated\n@mock_ec2_deprecated\ndef test_create_load_balancer():\n    conn = boto.connect_elb()\n    ec2 = boto.ec2.connect_to_region(\"us-east-1\")\n\n    security_group = ec2.create_security_group('sg-abc987', 'description')\n\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb', zones, ports, scheme='internal', security_groups=[security_group.id])\n\n    balancers = conn.get_all_load_balancers()\n    balancer = balancers[0]\n    balancer.name.should.equal(\"my-lb\")\n    balancer.scheme.should.equal(\"internal\")\n    list(balancer.security_groups).should.equal([security_group.id])\n    set(balancer.availability_zones).should.equal(\n        set(['us-east-1a', 'us-east-1b']))\n    listener1 = balancer.listeners[0]\n    listener1.load_balancer_port.should.equal(80)\n    listener1.instance_port.should.equal(8080)\n    listener1.protocol.should.equal(\"HTTP\")\n    listener2 = balancer.listeners[1]\n    listener2.load_balancer_port.should.equal(443)\n    listener2.instance_port.should.equal(8443)\n    listener2.protocol.should.equal(\"TCP\")\n\n\n@mock_elb_deprecated\ndef test_getting_missing_elb():\n    conn = boto.connect_elb()\n    conn.get_all_load_balancers.when.called_with(\n        load_balancer_names='aaa').should.throw(BotoServerError)\n\n\n@mock_elb_deprecated\ndef test_create_elb_in_multiple_region():\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n\n    west1_conn = boto.ec2.elb.connect_to_region(\"us-west-1\")\n    west1_conn.create_load_balancer('my-lb', zones, ports)\n\n    west2_conn = boto.ec2.elb.connect_to_region(\"us-west-2\")\n    west2_conn.create_load_balancer('my-lb', zones, ports)\n\n    list(west1_conn.get_all_load_balancers()).should.have.length_of(1)\n    list(west2_conn.get_all_load_balancers()).should.have.length_of(1)\n\n\n@mock_elb_deprecated\ndef test_create_load_balancer_with_certificate():\n    conn = boto.connect_elb()\n\n    zones = ['us-east-1a']\n    ports = [\n        (443, 8443, 'https', 'arn:aws:iam:123456789012:server-certificate\/test-cert')]\n    conn.create_load_balancer('my-lb', zones, ports)\n\n    balancers = conn.get_all_load_balancers()\n    balancer = balancers[0]\n    balancer.name.should.equal(\"my-lb\")\n    balancer.scheme.should.equal(\"internet-facing\")\n    set(balancer.availability_zones).should.equal(set(['us-east-1a']))\n    listener = balancer.listeners[0]\n    listener.load_balancer_port.should.equal(443)\n    listener.instance_port.should.equal(8443)\n    listener.protocol.should.equal(\"HTTPS\")\n    listener.ssl_certificate_id.should.equal(\n        'arn:aws:iam:123456789012:server-certificate\/test-cert')\n\n\n@mock_elb\ndef test_create_and_delete_boto3_support():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n    list(client.describe_load_balancers()[\n         'LoadBalancerDescriptions']).should.have.length_of(1)\n\n    client.delete_load_balancer(\n        LoadBalancerName='my-lb'\n    )\n    list(client.describe_load_balancers()[\n         'LoadBalancerDescriptions']).should.have.length_of(0)\n\n\n@mock_elb\ndef test_create_load_balancer_with_no_listeners_defined():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    with assert_raises(ClientError):\n        client.create_load_balancer(\n            LoadBalancerName='my-lb',\n            Listeners=[],\n            AvailabilityZones=['us-east-1a', 'us-east-1b']\n        )\n\n\n@mock_elb\ndef test_describe_paginated_balancers():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    for i in range(51):\n        client.create_load_balancer(\n            LoadBalancerName='my-lb%d' % i,\n            Listeners=[\n                {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n            AvailabilityZones=['us-east-1a', 'us-east-1b']\n        )\n\n    resp = client.describe_load_balancers()\n    resp['LoadBalancerDescriptions'].should.have.length_of(50)\n    resp['NextMarker'].should.equal(resp['LoadBalancerDescriptions'][-1]['LoadBalancerName'])\n    resp2 = client.describe_load_balancers(Marker=resp['NextMarker'])\n    resp2['LoadBalancerDescriptions'].should.have.length_of(1)\n    assert 'NextToken' not in resp2.keys()\n\n\n@mock_elb\n@mock_ec2\ndef test_apply_security_groups_to_load_balancer():\n    client = boto3.client('elb', region_name='us-east-1')\n    ec2 = boto3.resource('ec2', region_name='us-east-1')\n\n    vpc = ec2.create_vpc(CidrBlock='10.0.0.0\/16')\n    security_group = ec2.create_security_group(\n        GroupName='sg01', Description='Test security group sg01', VpcId=vpc.id)\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n\n    response = client.apply_security_groups_to_load_balancer(\n        LoadBalancerName='my-lb',\n        SecurityGroups=[security_group.id])\n\n    assert response['SecurityGroups'] == [security_group.id]\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    assert balancer['SecurityGroups'] == [security_group.id]\n\n    # Using a not-real security group raises an error\n    with assert_raises(ClientError) as error:\n        response = client.apply_security_groups_to_load_balancer(\n            LoadBalancerName='my-lb',\n            SecurityGroups=['not-really-a-security-group'])\n    assert \"One or more of the specified security groups do not exist.\" in str(error.exception)\n\n\n@mock_elb_deprecated\ndef test_add_listener():\n    conn = boto.connect_elb()\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http')]\n    conn.create_load_balancer('my-lb', zones, ports)\n    new_listener = (443, 8443, 'tcp')\n    conn.create_load_balancer_listeners('my-lb', [new_listener])\n    balancers = conn.get_all_load_balancers()\n    balancer = balancers[0]\n    listener1 = balancer.listeners[0]\n    listener1.load_balancer_port.should.equal(80)\n    listener1.instance_port.should.equal(8080)\n    listener1.protocol.should.equal(\"HTTP\")\n    listener2 = balancer.listeners[1]\n    listener2.load_balancer_port.should.equal(443)\n    listener2.instance_port.should.equal(8443)\n    listener2.protocol.should.equal(\"TCP\")\n\n\n@mock_elb_deprecated\ndef test_delete_listener():\n    conn = boto.connect_elb()\n\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb', zones, ports)\n    conn.delete_load_balancer_listeners('my-lb', [443])\n    balancers = conn.get_all_load_balancers()\n    balancer = balancers[0]\n    listener1 = balancer.listeners[0]\n    listener1.load_balancer_port.should.equal(80)\n    listener1.instance_port.should.equal(8080)\n    listener1.protocol.should.equal(\"HTTP\")\n    balancer.listeners.should.have.length_of(1)\n\n\n@mock_elb\ndef test_create_and_delete_listener_boto3_support():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[{'Protocol': 'http',\n                    'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n    list(client.describe_load_balancers()[\n         'LoadBalancerDescriptions']).should.have.length_of(1)\n\n    client.create_load_balancer_listeners(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 443, 'InstancePort': 8443}]\n    )\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    list(balancer['ListenerDescriptions']).should.have.length_of(2)\n    balancer['ListenerDescriptions'][0][\n        'Listener']['Protocol'].should.equal('HTTP')\n    balancer['ListenerDescriptions'][0]['Listener'][\n        'LoadBalancerPort'].should.equal(80)\n    balancer['ListenerDescriptions'][0]['Listener'][\n        'InstancePort'].should.equal(8080)\n    balancer['ListenerDescriptions'][1][\n        'Listener']['Protocol'].should.equal('TCP')\n    balancer['ListenerDescriptions'][1]['Listener'][\n        'LoadBalancerPort'].should.equal(443)\n    balancer['ListenerDescriptions'][1]['Listener'][\n        'InstancePort'].should.equal(8443)\n\n    # Creating this listener with an conflicting definition throws error\n    with assert_raises(ClientError):\n        client.create_load_balancer_listeners(\n            LoadBalancerName='my-lb',\n            Listeners=[\n                {'Protocol': 'tcp', 'LoadBalancerPort': 443, 'InstancePort': 1234}]\n        )\n\n    client.delete_load_balancer_listeners(\n        LoadBalancerName='my-lb',\n        LoadBalancerPorts=[443])\n\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    list(balancer['ListenerDescriptions']).should.have.length_of(1)\n\n\n@mock_elb_deprecated\ndef test_set_sslcertificate():\n    conn = boto.connect_elb()\n\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb', zones, ports)\n    conn.set_lb_listener_SSL_certificate('my-lb', '443', 'arn:certificate')\n    balancers = conn.get_all_load_balancers()\n    balancer = balancers[0]\n    listener1 = balancer.listeners[0]\n    listener1.load_balancer_port.should.equal(443)\n    listener1.instance_port.should.equal(8443)\n    listener1.protocol.should.equal(\"TCP\")\n    listener1.ssl_certificate_id.should.equal(\"arn:certificate\")\n\n\n@mock_elb_deprecated\ndef test_get_load_balancers_by_name():\n    conn = boto.connect_elb()\n\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb1', zones, ports)\n    conn.create_load_balancer('my-lb2', zones, ports)\n    conn.create_load_balancer('my-lb3', zones, ports)\n\n    conn.get_all_load_balancers().should.have.length_of(3)\n    conn.get_all_load_balancers(\n        load_balancer_names=['my-lb1']).should.have.length_of(1)\n    conn.get_all_load_balancers(\n        load_balancer_names=['my-lb1', 'my-lb2']).should.have.length_of(2)\n\n\n@mock_elb_deprecated\ndef test_delete_load_balancer():\n    conn = boto.connect_elb()\n\n    zones = ['us-east-1a']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb', zones, ports)\n\n    balancers = conn.get_all_load_balancers()\n    balancers.should.have.length_of(1)\n\n    conn.delete_load_balancer(\"my-lb\")\n    balancers = conn.get_all_load_balancers()\n    balancers.should.have.length_of(0)\n\n\n@mock_elb_deprecated\ndef test_create_health_check():\n    conn = boto.connect_elb()\n\n    hc = HealthCheck(\n        interval=20,\n        healthy_threshold=3,\n        unhealthy_threshold=5,\n        target='HTTP:8080\/health',\n        timeout=23,\n    )\n\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    lb.configure_health_check(hc)\n\n    balancer = conn.get_all_load_balancers()[0]\n    health_check = balancer.health_check\n    health_check.interval.should.equal(20)\n    health_check.healthy_threshold.should.equal(3)\n    health_check.unhealthy_threshold.should.equal(5)\n    health_check.target.should.equal('HTTP:8080\/health')\n    health_check.timeout.should.equal(23)\n\n\n@mock_elb\ndef test_create_health_check_boto3():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[{'Protocol': 'http',\n                    'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n    client.configure_health_check(\n        LoadBalancerName='my-lb',\n        HealthCheck={\n            'Target': 'HTTP:8080\/health',\n            'Interval': 20,\n            'Timeout': 23,\n            'HealthyThreshold': 3,\n            'UnhealthyThreshold': 5\n        }\n    )\n\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    balancer['HealthCheck']['Target'].should.equal('HTTP:8080\/health')\n    balancer['HealthCheck']['Interval'].should.equal(20)\n    balancer['HealthCheck']['Timeout'].should.equal(23)\n    balancer['HealthCheck']['HealthyThreshold'].should.equal(3)\n    balancer['HealthCheck']['UnhealthyThreshold'].should.equal(5)\n\n\n@mock_ec2_deprecated\n@mock_elb_deprecated\ndef test_register_instances():\n    ec2_conn = boto.connect_ec2()\n    reservation = ec2_conn.run_instances('ami-1234abcd', 2)\n    instance_id1 = reservation.instances[0].id\n    instance_id2 = reservation.instances[1].id\n\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    lb.register_instances([instance_id1, instance_id2])\n\n    balancer = conn.get_all_load_balancers()[0]\n    instance_ids = [instance.id for instance in balancer.instances]\n    set(instance_ids).should.equal(set([instance_id1, instance_id2]))\n\n\n@mock_ec2\n@mock_elb\ndef test_register_instances_boto3():\n    ec2 = boto3.resource('ec2', region_name='us-east-1')\n    response = ec2.create_instances(\n        ImageId='ami-1234abcd', MinCount=2, MaxCount=2)\n    instance_id1 = response[0].id\n    instance_id2 = response[1].id\n\n    client = boto3.client('elb', region_name='us-east-1')\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[{'Protocol': 'http',\n                    'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n    client.register_instances_with_load_balancer(\n        LoadBalancerName='my-lb',\n        Instances=[\n            {'InstanceId': instance_id1},\n            {'InstanceId': instance_id2}\n        ]\n    )\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    instance_ids = [instance['InstanceId']\n                    for instance in balancer['Instances']]\n    set(instance_ids).should.equal(set([instance_id1, instance_id2]))\n\n\n@mock_ec2_deprecated\n@mock_elb_deprecated\ndef test_deregister_instances():\n    ec2_conn = boto.connect_ec2()\n    reservation = ec2_conn.run_instances('ami-1234abcd', 2)\n    instance_id1 = reservation.instances[0].id\n    instance_id2 = reservation.instances[1].id\n\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    lb.register_instances([instance_id1, instance_id2])\n\n    balancer = conn.get_all_load_balancers()[0]\n    balancer.instances.should.have.length_of(2)\n    balancer.deregister_instances([instance_id1])\n\n    balancer.instances.should.have.length_of(1)\n    balancer.instances[0].id.should.equal(instance_id2)\n\n\n@mock_ec2\n@mock_elb\ndef test_deregister_instances_boto3():\n    ec2 = boto3.resource('ec2', region_name='us-east-1')\n    response = ec2.create_instances(\n        ImageId='ami-1234abcd', MinCount=2, MaxCount=2)\n    instance_id1 = response[0].id\n    instance_id2 = response[1].id\n\n    client = boto3.client('elb', region_name='us-east-1')\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[{'Protocol': 'http',\n                    'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n    client.register_instances_with_load_balancer(\n        LoadBalancerName='my-lb',\n        Instances=[\n            {'InstanceId': instance_id1},\n            {'InstanceId': instance_id2}\n        ]\n    )\n\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    balancer['Instances'].should.have.length_of(2)\n\n    client.deregister_instances_from_load_balancer(\n        LoadBalancerName='my-lb',\n        Instances=[\n            {'InstanceId': instance_id1}\n        ]\n    )\n\n    balancer = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    balancer['Instances'].should.have.length_of(1)\n    balancer['Instances'][0]['InstanceId'].should.equal(instance_id2)\n\n\n@mock_elb_deprecated\ndef test_default_attributes():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    attributes = lb.get_attributes()\n\n    attributes.cross_zone_load_balancing.enabled.should.be.false\n    attributes.connection_draining.enabled.should.be.false\n    attributes.access_log.enabled.should.be.false\n    attributes.connecting_settings.idle_timeout.should.equal(60)\n\n\n@mock_elb_deprecated\ndef test_cross_zone_load_balancing_attribute():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    conn.modify_lb_attribute(\"my-lb\", \"CrossZoneLoadBalancing\", True)\n    attributes = lb.get_attributes(force=True)\n    attributes.cross_zone_load_balancing.enabled.should.be.true\n\n    conn.modify_lb_attribute(\"my-lb\", \"CrossZoneLoadBalancing\", False)\n    attributes = lb.get_attributes(force=True)\n    attributes.cross_zone_load_balancing.enabled.should.be.false\n\n\n@mock_elb_deprecated\ndef test_connection_draining_attribute():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    connection_draining = ConnectionDrainingAttribute()\n    connection_draining.enabled = True\n    connection_draining.timeout = 60\n\n    conn.modify_lb_attribute(\n        \"my-lb\", \"ConnectionDraining\", connection_draining)\n    attributes = lb.get_attributes(force=True)\n    attributes.connection_draining.enabled.should.be.true\n    attributes.connection_draining.timeout.should.equal(60)\n\n    connection_draining.timeout = 30\n    conn.modify_lb_attribute(\n        \"my-lb\", \"ConnectionDraining\", connection_draining)\n    attributes = lb.get_attributes(force=True)\n    attributes.connection_draining.timeout.should.equal(30)\n\n    connection_draining.enabled = False\n    conn.modify_lb_attribute(\n        \"my-lb\", \"ConnectionDraining\", connection_draining)\n    attributes = lb.get_attributes(force=True)\n    attributes.connection_draining.enabled.should.be.false\n\n\n@mock_elb_deprecated\ndef test_access_log_attribute():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    access_log = AccessLogAttribute()\n    access_log.enabled = True\n    access_log.s3_bucket_name = 'bucket'\n    access_log.s3_bucket_prefix = 'prefix'\n    access_log.emit_interval = 60\n\n    conn.modify_lb_attribute(\"my-lb\", \"AccessLog\", access_log)\n    attributes = lb.get_attributes(force=True)\n    attributes.access_log.enabled.should.be.true\n    attributes.access_log.s3_bucket_name.should.equal(\"bucket\")\n    attributes.access_log.s3_bucket_prefix.should.equal(\"prefix\")\n    attributes.access_log.emit_interval.should.equal(60)\n\n    access_log.enabled = False\n    conn.modify_lb_attribute(\"my-lb\", \"AccessLog\", access_log)\n    attributes = lb.get_attributes(force=True)\n    attributes.access_log.enabled.should.be.false\n\n\n@mock_elb_deprecated\ndef test_connection_settings_attribute():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n\n    connection_settings = ConnectionSettingAttribute(conn)\n    connection_settings.idle_timeout = 120\n\n    conn.modify_lb_attribute(\n        \"my-lb\", \"ConnectingSettings\", connection_settings)\n    attributes = lb.get_attributes(force=True)\n    attributes.connecting_settings.idle_timeout.should.equal(120)\n\n    connection_settings.idle_timeout = 60\n    conn.modify_lb_attribute(\n        \"my-lb\", \"ConnectingSettings\", connection_settings)\n    attributes = lb.get_attributes(force=True)\n    attributes.connecting_settings.idle_timeout.should.equal(60)\n\n\n@mock_elb_deprecated\ndef test_create_lb_cookie_stickiness_policy():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    cookie_expiration_period = 60\n    policy_name = \"LBCookieStickinessPolicy\"\n\n    lb.create_cookie_stickiness_policy(cookie_expiration_period, policy_name)\n\n    lb = conn.get_all_load_balancers()[0]\n    # There appears to be a quirk about boto, whereby it returns a unicode\n    # string for cookie_expiration_period, despite being stated in\n    # documentation to be a long numeric.\n    #\n    # To work around that, this value is converted to an int and checked.\n    cookie_expiration_period_response_str = lb.policies.lb_cookie_stickiness_policies[\n        0].cookie_expiration_period\n    int(cookie_expiration_period_response_str).should.equal(\n        cookie_expiration_period)\n    lb.policies.lb_cookie_stickiness_policies[\n        0].policy_name.should.equal(policy_name)\n\n\n@mock_elb_deprecated\ndef test_create_lb_cookie_stickiness_policy_no_expiry():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    policy_name = \"LBCookieStickinessPolicy\"\n\n    lb.create_cookie_stickiness_policy(None, policy_name)\n\n    lb = conn.get_all_load_balancers()[0]\n    lb.policies.lb_cookie_stickiness_policies[\n        0].cookie_expiration_period.should.be.none\n    lb.policies.lb_cookie_stickiness_policies[\n        0].policy_name.should.equal(policy_name)\n\n\n@mock_elb_deprecated\ndef test_create_app_cookie_stickiness_policy():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    cookie_name = \"my-stickiness-policy\"\n    policy_name = \"AppCookieStickinessPolicy\"\n\n    lb.create_app_cookie_stickiness_policy(cookie_name, policy_name)\n\n    lb = conn.get_all_load_balancers()[0]\n    lb.policies.app_cookie_stickiness_policies[\n        0].cookie_name.should.equal(cookie_name)\n    lb.policies.app_cookie_stickiness_policies[\n        0].policy_name.should.equal(policy_name)\n\n\n@mock_elb_deprecated\ndef test_create_lb_policy():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    policy_name = \"ProxyPolicy\"\n\n    lb.create_lb_policy(policy_name, 'ProxyProtocolPolicyType', {\n                        'ProxyProtocol': True})\n\n    lb = conn.get_all_load_balancers()[0]\n    lb.policies.other_policies[0].policy_name.should.equal(policy_name)\n\n\n@mock_elb_deprecated\ndef test_set_policies_of_listener():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    listener_port = 80\n    policy_name = \"my-stickiness-policy\"\n\n    # boto docs currently state that zero or one policy may be associated\n    # with a given listener\n\n    # in a real flow, it is necessary first to create a policy,\n    # then to set that policy to the listener\n    lb.create_cookie_stickiness_policy(None, policy_name)\n    lb.set_policies_of_listener(listener_port, [policy_name])\n\n    lb = conn.get_all_load_balancers()[0]\n    listener = lb.listeners[0]\n    listener.load_balancer_port.should.equal(listener_port)\n    # by contrast to a backend, a listener stores only policy name strings\n    listener.policy_names[0].should.equal(policy_name)\n\n\n@mock_elb_deprecated\ndef test_set_policies_of_backend_server():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', [], ports)\n    instance_port = 8080\n    policy_name = \"ProxyPolicy\"\n\n    # in a real flow, it is necessary first to create a policy,\n    # then to set that policy to the backend\n    lb.create_lb_policy(policy_name, 'ProxyProtocolPolicyType', {\n                        'ProxyProtocol': True})\n    lb.set_policies_of_backend_server(instance_port, [policy_name])\n\n    lb = conn.get_all_load_balancers()[0]\n    backend = lb.backends[0]\n    backend.instance_port.should.equal(instance_port)\n    # by contrast to a listener, a backend stores OtherPolicy objects\n    backend.policies[0].policy_name.should.equal(policy_name)\n\n\n@mock_ec2_deprecated\n@mock_elb_deprecated\ndef test_describe_instance_health():\n    ec2_conn = boto.connect_ec2()\n    reservation = ec2_conn.run_instances('ami-1234abcd', 2)\n    instance_id1 = reservation.instances[0].id\n    instance_id2 = reservation.instances[1].id\n\n    conn = boto.connect_elb()\n    zones = ['us-east-1a', 'us-east-1b']\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    lb = conn.create_load_balancer('my-lb', zones, ports)\n\n    instances_health = conn.describe_instance_health('my-lb')\n    instances_health.should.be.empty\n\n    lb.register_instances([instance_id1, instance_id2])\n\n    instances_health = conn.describe_instance_health('my-lb')\n    instances_health.should.have.length_of(2)\n    for instance_health in instances_health:\n        instance_health.instance_id.should.be.within(\n            [instance_id1, instance_id2])\n        instance_health.state.should.equal('InService')\n\n    instances_health = conn.describe_instance_health('my-lb', [instance_id1])\n    instances_health.should.have.length_of(1)\n    instances_health[0].instance_id.should.equal(instance_id1)\n    instances_health[0].state.should.equal('InService')\n\n\n@mock_ec2\n@mock_elb\ndef test_describe_instance_health_boto3():\n    elb = boto3.client('elb', region_name=\"us-east-1\")\n    ec2 = boto3.client('ec2', region_name=\"us-east-1\")\n    instances = ec2.run_instances(MinCount=2, MaxCount=2)['Instances']\n    lb_name = \"my_load_balancer\"\n    elb.create_load_balancer(\n        Listeners=[{\n            'InstancePort': 80,\n            'LoadBalancerPort': 8080,\n            'Protocol': 'HTTP'\n        }],\n        LoadBalancerName=lb_name,\n    )\n    elb.register_instances_with_load_balancer(\n        LoadBalancerName=lb_name,\n        Instances=[{'InstanceId': instances[0]['InstanceId']}]\n    )\n    instances_health = elb.describe_instance_health(\n        LoadBalancerName=lb_name,\n        Instances=[{'InstanceId': instance['InstanceId']} for instance in instances]\n    )\n    instances_health['InstanceStates'].should.have.length_of(2)\n    instances_health['InstanceStates'][0]['InstanceId'].\\\n        should.equal(instances[0]['InstanceId'])\n    instances_health['InstanceStates'][0]['State'].\\\n        should.equal('InService')\n    instances_health['InstanceStates'][1]['InstanceId'].\\\n        should.equal(instances[1]['InstanceId'])\n    instances_health['InstanceStates'][1]['State'].\\\n        should.equal('Unknown')\n\n\n@mock_elb\ndef test_add_remove_tags():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.add_tags.when.called_with(LoadBalancerNames=['my-lb'],\n                                     Tags=[{\n                                         'Key': 'a',\n                                         'Value': 'b'\n                                     }]).should.throw(botocore.exceptions.ClientError)\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n\n    list(client.describe_load_balancers()[\n         'LoadBalancerDescriptions']).should.have.length_of(1)\n\n    client.add_tags(LoadBalancerNames=['my-lb'],\n                    Tags=[{\n                        'Key': 'a',\n                        'Value': 'b'\n                    }])\n\n    tags = dict([(d['Key'], d['Value']) for d in client.describe_tags(\n        LoadBalancerNames=['my-lb'])['TagDescriptions'][0]['Tags']])\n    tags.should.have.key('a').which.should.equal('b')\n\n    client.add_tags(LoadBalancerNames=['my-lb'],\n                    Tags=[{\n                        'Key': 'a',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'b',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'c',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'd',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'e',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'f',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'g',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'h',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'i',\n                        'Value': 'b'\n                    }, {\n                        'Key': 'j',\n                        'Value': 'b'\n                    }])\n\n    client.add_tags.when.called_with(LoadBalancerNames=['my-lb'],\n                                     Tags=[{\n                                         'Key': 'k',\n                                         'Value': 'b'\n                                     }]).should.throw(botocore.exceptions.ClientError)\n\n    client.add_tags(LoadBalancerNames=['my-lb'],\n                    Tags=[{\n                        'Key': 'j',\n                        'Value': 'c'\n                    }])\n\n    tags = dict([(d['Key'], d['Value']) for d in client.describe_tags(\n        LoadBalancerNames=['my-lb'])['TagDescriptions'][0]['Tags']])\n\n    tags.should.have.key('a').which.should.equal('b')\n    tags.should.have.key('b').which.should.equal('b')\n    tags.should.have.key('c').which.should.equal('b')\n    tags.should.have.key('d').which.should.equal('b')\n    tags.should.have.key('e').which.should.equal('b')\n    tags.should.have.key('f').which.should.equal('b')\n    tags.should.have.key('g').which.should.equal('b')\n    tags.should.have.key('h').which.should.equal('b')\n    tags.should.have.key('i').which.should.equal('b')\n    tags.should.have.key('j').which.should.equal('c')\n    tags.shouldnt.have.key('k')\n\n    client.remove_tags(LoadBalancerNames=['my-lb'],\n                       Tags=[{\n                           'Key': 'a'\n                       }])\n\n    tags = dict([(d['Key'], d['Value']) for d in client.describe_tags(\n        LoadBalancerNames=['my-lb'])['TagDescriptions'][0]['Tags']])\n\n    tags.shouldnt.have.key('a')\n    tags.should.have.key('b').which.should.equal('b')\n    tags.should.have.key('c').which.should.equal('b')\n    tags.should.have.key('d').which.should.equal('b')\n    tags.should.have.key('e').which.should.equal('b')\n    tags.should.have.key('f').which.should.equal('b')\n    tags.should.have.key('g').which.should.equal('b')\n    tags.should.have.key('h').which.should.equal('b')\n    tags.should.have.key('i').which.should.equal('b')\n    tags.should.have.key('j').which.should.equal('c')\n\n    client.create_load_balancer(\n        LoadBalancerName='other-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 433, 'InstancePort': 8433}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n\n    client.add_tags(LoadBalancerNames=['other-lb'],\n                    Tags=[{\n                        'Key': 'other',\n                        'Value': 'something'\n                    }])\n\n    lb_tags = dict([(l['LoadBalancerName'], dict([(d['Key'], d['Value']) for d in l['Tags']]))\n                    for l in client.describe_tags(LoadBalancerNames=['my-lb', 'other-lb'])['TagDescriptions']])\n\n    lb_tags.should.have.key('my-lb')\n    lb_tags.should.have.key('other-lb')\n\n    lb_tags['my-lb'].shouldnt.have.key('other')\n    lb_tags[\n        'other-lb'].should.have.key('other').which.should.equal('something')\n\n\n@mock_elb\ndef test_create_with_tags():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b'],\n        Tags=[{\n            'Key': 'k',\n            'Value': 'v'\n        }]\n    )\n\n    tags = dict((d['Key'], d['Value']) for d in client.describe_tags(\n        LoadBalancerNames=['my-lb'])['TagDescriptions'][0]['Tags'])\n    tags.should.have.key('k').which.should.equal('v')\n\n\n@mock_elb\ndef test_modify_attributes():\n    client = boto3.client('elb', region_name='us-east-1')\n\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[{'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        AvailabilityZones=['us-east-1a', 'us-east-1b']\n    )\n\n    # Default ConnectionDraining timeout of 300 seconds\n    client.modify_load_balancer_attributes(\n        LoadBalancerName='my-lb',\n        LoadBalancerAttributes={\n            'ConnectionDraining': {'Enabled': True},\n        }\n    )\n    lb_attrs = client.describe_load_balancer_attributes(LoadBalancerName='my-lb')\n    lb_attrs['LoadBalancerAttributes']['ConnectionDraining']['Enabled'].should.equal(True)\n    lb_attrs['LoadBalancerAttributes']['ConnectionDraining']['Timeout'].should.equal(300)\n\n    # specify a custom ConnectionDraining timeout\n    client.modify_load_balancer_attributes(\n        LoadBalancerName='my-lb',\n        LoadBalancerAttributes={\n            'ConnectionDraining': {\n                'Enabled': True,\n                'Timeout': 45,\n            },\n        }\n    )\n    lb_attrs = client.describe_load_balancer_attributes(LoadBalancerName='my-lb')\n    lb_attrs['LoadBalancerAttributes']['ConnectionDraining']['Enabled'].should.equal(True)\n    lb_attrs['LoadBalancerAttributes']['ConnectionDraining']['Timeout'].should.equal(45)\n\n\n@mock_ec2\n@mock_elb\ndef test_subnets():\n    ec2 = boto3.resource('ec2', region_name='us-east-1')\n    vpc = ec2.create_vpc(\n        CidrBlock='172.28.7.0\/24',\n        InstanceTenancy='default'\n    )\n    subnet = ec2.create_subnet(\n        VpcId=vpc.id,\n        CidrBlock='172.28.7.192\/26'\n    )\n    client = boto3.client('elb', region_name='us-east-1')\n    client.create_load_balancer(\n        LoadBalancerName='my-lb',\n        Listeners=[\n            {'Protocol': 'tcp', 'LoadBalancerPort': 80, 'InstancePort': 8080}],\n        Subnets=[subnet.id]\n    )\n\n    lb = client.describe_load_balancers()['LoadBalancerDescriptions'][0]\n    lb.should.have.key('Subnets').which.should.have.length_of(1)\n    lb['Subnets'][0].should.equal(subnet.id)\n\n    lb.should.have.key('VPCId').which.should.equal(vpc.id)\n\n\n@mock_elb_deprecated\ndef test_create_load_balancer_duplicate():\n    conn = boto.connect_elb()\n    ports = [(80, 8080, 'http'), (443, 8443, 'tcp')]\n    conn.create_load_balancer('my-lb', [], ports)\n    conn.create_load_balancer.when.called_with(\n        'my-lb', [], ports).should.throw(BotoServerError)\n","license":"apache-2.0","hash":7194088126097973083,"line_mean":34.7352342159,"line_max":111,"alpha_frac":0.6331072609,"autogenerated":false},
{"repo_name":"dennytwix\/pukanoid_server","path":"fabfile.py","copies":"1","size":"1343","content":"# -*- coding: utf-8 -*-\n#\n# Author: Roman Savchenko <r.sav4enko@gmail.com>\n# Created: 2014-10-15\n#\n# Id: $Id$\n\nfrom fabric.api import *\nfrom fabric.decorators import *\n\nenv.hosts = ['pukanoid@pukanoid.dennytwix.com']\nclient = 'pukanoid'\n\n\n@task\ndef deploy():\n    with cd('\/home\/pukanoid\/pukanoid_server'):\n        with prefix('source \/home\/pukanoid\/pukanoid_server\/.env\/bin\/activate'):\n            run('git pull')\n            # run('.\/manage.py migrate')\n            # run('.\/manage.py collectstatic --noinput')\n            sudo('supervisorctl restart pukanoid')\n\n# @task\n# def getdb():\n#     run('pg_dump ipractice > \/tmp\/ipractice.sql')\n#     get('\/tmp\/ipractice.sql', '\/tmp\/ipractice.sql')\n#     run('rm \/tmp\/ipractice.sql')\n#\n# @task\n# def updatedb():\n#     local('dropdb ipractice')\n#     local('createdb ipractice')\n#     local('psql ipractice < \/tmp\/ipractice.sql')\n#     local('rm \/tmp\/ipractice.sql')\n#\n#\n# @task\n# def syncdb():\n#     getdb()\n#     updatedb()\n\n#@task\n#def deploy_sandbox():\n#    local('git push')\n#\n#    with cd('\/home\/%s\/api_sandbox' % client):\n#        with prefix('source \/home\/%s\/api_sandbox\/.env\/bin\/activate' % client):\n#            sudo('.\/ctl stop', user=client)\n#            sudo('git pull', user=client)\n#            sudo('.\/manage.py db_upgrade', user=client)\n#            sudo('.\/ctl start', user=client)\n","license":"gpl-2.0","hash":-6230691828581295614,"line_mean":24.8269230769,"line_max":79,"alpha_frac":0.5912137007,"autogenerated":false},
{"repo_name":"jterrace\/sphinxtr","path":"extensions\/singlehtml_toc.py","copies":"2","size":"1710","content":"\"\"\"\nFixes the table of contents in singlehtml mode so that section titles have the\ncorrect section number in front.\n\"\"\"\n\nfrom docutils import nodes\nfrom sphinx import addnodes\n\ndef stringize_secnum(secnum):\n    return '.'.join(map(str, secnum))\n\ndef doctree_resolved(app, doctree, fromdocname):\n    if app.builder.name == 'singlehtml':\n        secnums = app.builder.env.toc_secnumbers\n        for filenode in doctree.traverse(addnodes.start_of_file):\n            docname = filenode['docname']\n            if docname not in secnums:\n                continue\n\n            doc_secnums = secnums[docname]\n            first_title_node = filenode.next_node(nodes.title)\n            if first_title_node is not None and '' in doc_secnums:\n                file_secnum = stringize_secnum(doc_secnums[''])\n                title_text_node = first_title_node.children[0]\n                newtext = file_secnum + ' ' + title_text_node.astext()\n                first_title_node.replace(title_text_node, nodes.Text(newtext))\n\n            for section_node in filenode.traverse(nodes.section):\n                for id in section_node['ids']:\n                    if '#' + id in doc_secnums:\n                        subsection_num = stringize_secnum(doc_secnums['#' + id])\n                        first_title_node = section_node.next_node(nodes.title)\n                        if first_title_node is not None:\n                            title_text_node = first_title_node.children[0]\n                            newtext = subsection_num + ' ' + title_text_node.astext()\n                            first_title_node.replace(title_text_node, nodes.Text(newtext))\n\ndef setup(app):\n    app.connect('doctree-resolved', doctree_resolved)\n","license":"bsd-2-clause","hash":1361050774425016283,"line_mean":42.8461538462,"line_max":90,"alpha_frac":0.5888888889,"autogenerated":false},
{"repo_name":"naturalness\/unnaturalcode","path":"unnaturalcode\/mitlmCorpus.py","copies":"2","size":"3825","content":"#    Copyright 2013, 2014 Joshua Charles Campbell\n#\n#    This file is part of UnnaturalCode.\n#\n#    UnnaturalCode is free software: you can redistribute it and\/or modify\n#    it under the terms of the GNU Affero General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    UnnaturalCode is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with UnnaturalCode.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nfrom __future__ import print_function\nimport os\nimport os.path\nimport errno\nfrom unnaturalcode.unnaturalCode import *\nimport logging\nfrom logging import debug, info, warning, error, getLogger\nimport codecs\nimport pymitlm\n\nallWhitespace = re.compile('^\\s+$')\n\nucParanoid = os.getenv(\"PARANOID\", False)\n\nmitlmLogger = getLogger('MITLM')\n\nmitlmLogger.setLevel(logging.DEBUG)\n\nclass mitlmCorpus(object):\n    \"\"\"\n    Interface to an MITLM corpus.\n    \"\"\"\n\n    def __init__(self, readCorpus=None, writeCorpus=None, uc=unnaturalCode(), order=10):\n        self.readCorpus = (readCorpus or os.getenv(\"ucCorpus\", \"\/tmp\/ucCorpus\"))\n        self.writeCorpus = (writeCorpus or os.getenv(\"ucWriteCorpus\", self.readCorpus))\n        self.corpusFile = False\n        self.order = order\n        self.mitlm = None\n\n    def startMitlm(self):\n        \"\"\"\n        Called automatically. Initializes MITLM, however we're interfacing to\n        it nowadays.\n        \"\"\"\n        if self.mitlm == None:\n            self.mitlm = pymitlm.PyMitlm(self.readCorpus, self.order,\n                                         \"KN\", True)\n\n    def corpify(self, lexemes):\n        \"\"\"Stringify lexed source: produce space-seperated sequence of lexemes\"\"\"\n        assert isinstance(lexemes, list)\n        assert len(lexemes)\n        return u\" \".join(lexemes)\n\n    def openCorpus(self):\n        \"\"\"Opens the corpus (if necessary)\"\"\"\n        if (self.corpusFile):\n            assert not self.corpusFile.closed\n            return\n        self.corpusFile = codecs.open(self.writeCorpus, 'a', encoding='UTF-8')\n\n    def closeCorpus(self):\n        \"\"\"Closes the corpus (if necessary)\"\"\"\n        if (self.corpusFile):\n            self.corpusFile.close()\n            assert self.corpusFile.closed\n            self.corpusFile = None\n\n    def addToCorpus(self, lexemes):\n        \"\"\"Adds a string of lexemes to the corpus\"\"\"\n        assert isinstance(lexemes, list)\n        assert len(lexemes)\n        self.openCorpus()\n        cl = self.corpify(lexemes)\n        assert(len(cl))\n        assert (not allWhitespace.match(cl)), \"Adding blank line to corpus!\"\n        print(cl, file=self.corpusFile)\n        self.corpusFile.flush()\n        # MITLM cannot (as of now) update its model, so just throw out the old\n        # one.\n        self.mitlm = None\n\n    def queryCorpus(self, request):\n        self.startMitlm()\n        r = self.mitlm.xentropy((\" \".join(request)).encode(\"UTF-8\"))\n        if r >= 1.0e70:\n          qString = self.corpify(request)\n          warning(\"Infinity: %s\" % qString)\n          warning(str(r))\n          self.checkMitlm()\n        return r\n\n    def predictCorpus(self, lexemes):\n        return self.parsePredictionResult(\n            self.mitlm.predict(lexemes),\n            remove_prefix=len(lexemes)\n        )\n\n    def release(self):\n        \"\"\"Close files and stop MITLM\"\"\"\n        self.closeCorpus()\n\n    def __del__(self):\n        \"\"\"I am a destructor, but release should be called explictly.\"\"\"\n        assert not self.corpusFile, \"Destructor called before release()\"\n","license":"agpl-3.0","hash":6628320938139220288,"line_mean":32.8495575221,"line_max":88,"alpha_frac":0.6368627451,"autogenerated":false},
{"repo_name":"eliben\/deep-learning-samples","path":"softmax\/softmax.py","copies":"1","size":"6875","content":"# Softmax function, its gradient, and combination with other layers.\n#\n# Eli Bendersky (http:\/\/eli.thegreenplace.net)\n# This code is in the public domain\nfrom __future__ import print_function\nimport numpy as np\n\n\ndef softmax(z):\n    \"\"\"Computes softmax function.\n\n    z: array of input values.\n\n    Returns an array of outputs with the same shape as z.\"\"\"\n    # For numerical stability: make the maximum of z's to be 0.\n    shiftz = z - np.max(z)\n    exps = np.exp(shiftz)\n    return exps \/ np.sum(exps)\n\n\ndef softmax_gradient(z):\n    \"\"\"Computes the gradient of the softmax function.\n\n    z: (T, 1) array of input values where the gradient is computed. T is the\n       number of output classes.\n\n    Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\n    is DjSi - the partial derivative of Si w.r.t. input j.\n    \"\"\"\n    Sz = softmax(z)\n    # -SjSi can be computed using an outer product between Sz and itself. Then\n    # we add back Si for the i=j cases by adding a diagonal matrix with the\n    # values of Si on its diagonal.\n    D = -np.outer(Sz, Sz) + np.diag(Sz.flatten())\n    return D\n\n\ndef softmax_gradient_simple(z):\n    \"\"\"Unvectorized computation of the gradient of softmax.\n\n    z: (T, 1) column array of input values.\n\n    Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\n    is DjSi - the partial derivative of Si w.r.t. input j.\n    \"\"\"\n    Sz = softmax(z)\n    N = z.shape[0]\n    D = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            D[i, j] = Sz[i, 0] * (np.float32(i == j) - Sz[j, 0])\n    return D\n\n\ndef fully_connected_gradient(x, W):\n    \"\"\"Computes the gradient of a fully connected layer w.r.t. the weights.\n\n    x: (N, 1) input\n    W: (T, N) weights\n\n    A fully connected layer acting on the input x is: W.dot(x). This function\n    computes the full Jacobian matrix of this formula. The W matrix is flattened\n    in row-major order to rows of the Jacobian, such that DijFCt is the\n    derivative of output t (the t'th row of W.dot(x)) w.r.t. W[i, j].\n\n    Returns D (T, N * T)\n    \"\"\"\n    N = x.shape[0]\n    T = W.shape[0]\n    D = np.zeros((T, N * T))\n    for t in range(T):\n        for i in range(T):\n            for j in range(N):\n                # Computing gradient of the t'th output w.r.t. W[i, j]. Its\n                # index in the D matrix is: (t, i*N + j)\n                # The t'th output only depends on the t'th row in W. Otherwise\n                # the derivative is zero. In the t'th row, each weight is\n                # multiplied by the respective x.\n                if t == i:\n                    D[t, i*N + j] = x[j]\n    return D\n\n\ndef softmax_layer(x, W):\n    \"\"\"Computes a \"softmax layer\" for input vector x and weight matrix W.\n\n    A softmax layer is a fully connected layer followed by the softmax function.\n    Mathematically it's softmax(W.dot(x)).\n\n    x: (N, 1) input vector with N features.\n    W: (T, N) matrix of weights for N features and T output classes.\n\n    Returns s (T, 1) the result of applying softmax to W.dot(x)\n    \"\"\"\n    logits = W.dot(x)\n    return softmax(logits)\n\n\ndef softmax_layer_gradient(x, W):\n    \"\"\"Computes the gradient of a \"softmax layer\" for weight matrix W.\n\n    x: (N, 1) input\n    W: (T, N) weights\n\n    A fully connected layer acting on the input x is: W.dot(x). This function\n    computes the full Jacobian matrix of this formula. The W matrix is flattened\n    in row-major order to rows of the Jacobian, such that DijFCt is the\n    derivative of output t (the t'th row of W.dot(x)) w.r.t. W[i, j].\n\n    Returns D (T, N * T)\n    \"\"\"\n    logits = W.dot(x)\n    return softmax_gradient(logits).dot(fully_connected_gradient(x, W))\n\n\ndef softmax_layer_gradient_direct(x, W):\n    \"\"\"Computes the gradient of a \"softmax layer\" for weight matrix W.\n\n    Arguments and return value exactly the same as for softmax_layer_gradient.\n    The difference is that this function computes the Jacobian \"directly\" by\n    assigning each cell in the matrix, rather than explicitly computing the\n    matrix multiplication of the two composed Jacobians.\n    \"\"\"\n    N = x.shape[0]\n    T = W.shape[0]\n    S = softmax_layer(x, W)\n    D = np.zeros((T, N * T))\n    for t in range(T):\n        for i in range(T):\n            for j in range(N):\n                DiSt = S[t, 0] * (np.float32(i == t) - S[i, 0])\n                D[t, i*N + j] = DiSt * x[j, 0]\n    return D\n\n\ndef cross_entropy_loss(p, y):\n    \"\"\"Cross-entropy loss between predicted and expected probabilities.\n\n    p: vector of predicted probabilities.\n    y: vector of expected probabilities. Has to be the same shape as p.\n\n    Returns a scalar.\n    \"\"\"\n    assert(p.shape == y.shape)\n    return -np.sum(y * np.log(p))\n\n\ndef cross_entropy_loss_gradient(p, y):\n    \"\"\"Gradient of the cross-entropy loss function for p and y.\n\n    p: (T, 1) vector of predicted probabilities.\n    y: (T, 1) vector of expected probabilities; must be one-hot -- one and only\n              one element of y is 1; the rest are 0.\n\n    Returns a (1, T) Jacobian for this function.\n    \"\"\"\n    assert(p.shape == y.shape and p.shape[1] == 1)\n    # py is the value of p at the index where y == 1 (one and only one such\n    # index is expected for a one-hot y).\n    py = p[y == 1]\n    assert(py.size == 1)\n    # D is zeros everywhere except at the index where y == 1. The final D has\n    # to be a row-vector.\n    D = np.zeros_like(p)\n    D[y == 1] = -1\/py.flat[0]\n    return D.flatten()\n\n\ndef softmax_cross_entropy_loss_gradient(x, W, y):\n    \"\"\"Computes the gradient of a cross-entropy loss for a softmax layer.\n\n    x: (N, 1) input\n    W: (T, N) weights\n    y: (T, 1) correct labels (one-hot vector with one element 1.0, others 0.0)\n\n    Returns D (1, N * T)\n    \"\"\"\n    p = softmax_layer(x, W)\n    return cross_entropy_loss_gradient(p, y).dot(softmax_layer_gradient(x, W))\n\n\ndef softmax_cross_entropy_loss_gradient_direct(x, W, y):\n    \"\"\"Computes the gradient of a cross-entropy loss for a softmax layer.\n\n    Arguments and return value exactly the same as for\n    softmax_cross_entropy_loss_gradient. The difference is that this function\n    computes the Jacobian \"directly\" by assigning each cell in the matrix,\n    rather than explicitly computing the matrix multiplication of the two\n    composed Jacobians.\n    \"\"\"\n    N = x.shape[0]\n    T = W.shape[0]\n    S = softmax_layer(x, W)\n    D = np.zeros(N * T)\n    yindex = np.argwhere(y == 1)[0, 0]\n    for i in range(T):\n        for j in range(N):\n            D[i*N + j] = (S[i, 0] - np.float32(i == yindex)) * x[j, 0]\n    return D\n\n\nif __name__ == '__main__':\n    #pa = [2945.0, 2945.5]\n    #pa = np.array([[1000], [2000], [3000]])\n    #print(softmax(pa))\n    #print(stablesoftmax(pa))\n\n    W = np.array([\n        [2, 3, 4],\n        [3, 5, -1]])\n    x = np.array([\n        [0.1],\n        [-0.2],\n        [0.3]])\n    print(softmax_layer(x, W))\n","license":"unlicense","hash":2164973473198655176,"line_mean":30.976744186,"line_max":80,"alpha_frac":0.6097454545,"autogenerated":false},
{"repo_name":"Lilykos\/inspire-next","path":"inspire\/modules\/authors\/recordext\/functions\/sum_emails.py","copies":"2","size":"1641","content":"# -*- coding: utf-8 -*-\n##\n## This file is part of INSPIRE.\n## Copyright (C) 2015 CERN.\n##\n## INSPIRE is free software; you can redistribute it and\/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## INSPIRE is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n## General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with INSPIRE; if not, write to the Free Software Foundation, Inc.,\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\ndef sum_emails(self, public, current_private, old_private):\n    \"\"\"Get summed emails.\n\n    :param public: Name of public emails field\n    :param current_private: Name of current private emails field\n    :param old_private: Name of old private emails field\n\n    :return: ``List`` with emails and info about their privateness and status.\n    \"\"\"\n    result = []\n    if public in self:\n        for email in self[public]:\n            result.append(email)\n            result[-1]['private'] = False\n    if current_private in self:\n        for email in self[current_private]:\n            result.append(email)\n            result[-1]['current'] = 'current'\n            result[-1]['private'] = True\n    if old_private in self:\n        for email in self[old_private]:\n            result.append(email)\n            result[-1]['private'] = True\n\n    return result\n","license":"gpl-2.0","hash":-5785758299290168944,"line_mean":35.4666666667,"line_max":78,"alpha_frac":0.6654478976,"autogenerated":false},
{"repo_name":"cjbe\/artiqDrivers","path":"artiqDrivers\/devices\/coherentDds\/group_wrapper.py","copies":"1","size":"6187","content":"from artiq.language.core import *\r\nfrom artiq.language import us\r\nimport numpy as np\r\nfrom artiq.coredevice import spi2\r\n\r\nclass DdsGroup:\r\n    \"\"\"\r\n    Wraps multiple DDSs, each connected via a 'slow control' USB interface and\r\n    optionally a realtime SPI bus for profile switching.\r\n    The arguments are:\r\n        'devices', tuples of DDS USB interface device, and SPI\r\n            interface device or None,\r\n        'mappings', a dictionary mapping logical devices names to\r\n            (device,channel) tuples. The logical names must be valid python\r\n            attribute names (e.g. cannot start with a number)\r\n\r\n    This is a hacky interface. Profile 7 is reserved as an 'off' profile.\r\n    It is programmed whenever profile 0 is written to with the same parameters\r\n    but zero amplitude.\r\n\r\n    We assume that the startup experiment has setup the SPI buses by calling\r\n    set_xfer(1,8,0) and that the SPI clock_div is set to match the value in this\r\n    class.\r\n    \"\"\"\r\n    kernel_invariants = {\"core\", \"profile_delay_mu\", \"padding_mu\"}\r\n\r\n    def __init__(self, dmgr, devices, mappings, clock_div, invert=False):\r\n        self.core = dmgr.get(\"core\")\r\n\r\n        self.invert = invert\r\n\r\n        dds_devices = {}\r\n        spi_devices = {}\r\n        for (dds_name, spi_name) in devices:\r\n            dds_devices[dds_name] = dmgr.get(dds_name)\r\n            spi_devices[dds_name] = dmgr.get(spi_name) if spi_name else None\r\n\r\n        ref_period_mu = self.core.seconds_to_mu(self.core.coarse_ref_period)\r\n        write_period_mu = clock_div*ref_period_mu\r\n        xfer_period_mu = 8*write_period_mu\r\n        self.profile_delay_mu = self.core.seconds_to_mu(1.3*us) + \\\r\n                                    xfer_period_mu + write_period_mu\r\n        self.padding_mu = xfer_period_mu + write_period_mu + ref_period_mu\r\n\r\n        for channel in mappings:\r\n            dev_name = mappings[channel][0]\r\n            ch = mappings[channel][1]\r\n            dds_dev = dds_devices[dev_name]\r\n\r\n            spi_dev = spi_devices[dev_name]\r\n\r\n            channel_cls = DdsChannel(self.core, dds_dev, spi_dev, ch,\\\r\n                                    self._spi_write)\r\n            setattr(self, channel, channel_cls)\r\n\r\n    @kernel\r\n    def _spi_write(self, spi, data, delay):\r\n        if self.invert:\r\n            spi.set_config_mu((spi2.SPI_END|spi2.SPI_CLK_POLARITY|spi2.SPI_CS_POLARITY), 8, 10, 1)\r\n            # flags set: SPI_END sets cs to inactive at end of write, others do the inversion of everything but the signal\r\n            # 8: 8bits, write length; 10: speed, division of clock speed by 10, can be anything >2, 1: initial state of cs, ie cs active\r\n            spi.write(~(data<<24))\r\n        else:\r\n            spi.set_config_mu(spi2.SPI_END, 8, 10, 1)\r\n            spi.write(data<<24)\r\n        if delay:\r\n            delay_mu(self.padding_mu+self.profile_delay_mu)\r\n\r\n\r\nclass DdsChannel:\r\n    kernel_invariants = {\"spi\", \"ch\"}\r\n    def __init__(self, core, device, spi, channel, _spi_write):\r\n        self.core = core\r\n        self.dev = device\r\n        self.spi = spi\r\n        self.ch = channel\r\n        self._spi_write = _spi_write\r\n\r\n    def set(self, frequency, profile=0, amplitude=1, phase=0):\r\n        self.dev.setProfile(self.ch, profile, \\\r\n                            frequency, amp=amplitude, phase=phase)\r\n\r\n        self.dev.resetPhase()\r\n\r\n    def set_sensible_pulse_shape(self, duration):\r\n        self.dev.setSensiblePulseShape(duration,self.ch)\r\n\r\n    def get_lsb_freq(self):\r\n        return self.dev.get_lsb_freq()\r\n\r\n    def identity(self):\r\n        idn = self.dev.identity()\r\n        return idn\r\n\r\n    def serial_reset_phase(self):\r\n        self.dev.resetPhase()\r\n\r\n    @kernel\r\n    def use_profile(self, profile,delay = True):\r\n        # write via SPI\r\n        self._write_profile_select(self.spi, self.ch, profile, delay = delay)\r\n\r\n    @kernel\r\n    def pulse_enable(self,enable):\r\n        # write via SPI\r\n        self._write_pulse_enable(self.spi,self.ch,enable)\r\n\r\n    @kernel\r\n    def reset_phase(self):\r\n        # write via SPI\r\n        self._write_reset_phase(self.spi)\r\n\r\n    @kernel\r\n    def _write_profile_select(self, spi, ch, profile,delay = True):\r\n        \"\"\"Set the profile select for a given spi device and channel number.\"\"\"\r\n        # Wire format:\r\n        # cs low, clock in 8 bit word, cs high\r\n        # bits 7-6 : mode, 0=set profile select, 1=set pulse enable\r\n        # bits 5-4 : channel, 0-3 for DDS channel 0-3\r\n        # bits 3-0 : mode dependant data\r\n        # if mode=0 : data[2:0] is the profile select vector\r\n        # if mode=1 : data[0] is the pulse enable line\r\n        # The DDS control signals take effect on the rising edge of cs.\r\n\r\n        data = 0 << 6\r\n        data += (ch & 3) << 4\r\n        data += profile & 7\r\n        self._spi_write(spi,data,delay = delay)\r\n\r\n    @kernel\r\n    def _write_pulse_enable(self, spi, ch, enable):\r\n        \"\"\"Set the profile select for a given spi device and channel number.\"\"\"\r\n        # Wire format:\r\n        # cs low, clock in 8 bit word, cs high\r\n        # bits 7-6 : mode, 0=set profile select, 1=set pulse enable\r\n        # bits 5-4 : channel, 0-3 for DDS channel 0-3\r\n        # bits 3-0 : mode dependant data\r\n        # if mode=0 : data[2:0] is the profile select vector\r\n        # if mode=1 : data[0] is the pulse enable line\r\n        # The DDS control signals take effect on the rising edge of cs.\r\n\r\n        data = 1 << 6\r\n        data += (ch & 3) << 4\r\n        data += enable & 1\r\n        self._spi_write(spi,data,delay = False)\r\n\r\n    @kernel\r\n    def _write_reset_phase(self, spi):\r\n        \"\"\"Set the profile select for a given spi device and channel number.\"\"\"\r\n        # Wire format:\r\n        # cs low, clock in 8 bit word, cs high\r\n        # bits 7-6 : mode, 0=set profile select, 1=set pulse enable\r\n        # bits 5-4 : channel, 0-3 for DDS channel 0-3\r\n        # bits 3-0 : mode dependant data\r\n        # if mode=0 : data[2:0] is the profile select vector\r\n        # if mode=1 : data[0] is the pulse enable line\r\n        # The DDS control signals take effect on the rising edge of cs.\r\n\r\n        data = 2 << 6\r\n        self._spi_write(spi,data,delay = True)","license":"gpl-3.0","hash":-6903486446715371110,"line_mean":36.68125,"line_max":136,"alpha_frac":0.5839663811,"autogenerated":false},
{"repo_name":"MangoMangoDevelopment\/neptune","path":"lib\/xacro-1.11.0\/src\/xacro\/xmlutils.py","copies":"1","size":"5964","content":"# Copyright (c) 2015, Open Source Robotics Foundation, Inc.\n# Copyright (c) 2013, Willow Garage, Inc.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and\/or other materials provided with the distribution.\n#     * Neither the name of the Open Source Robotics Foundation, Inc.\n#       nor the names of its contributors may be used to endorse or promote\n#       products derived from this software without specific prior\n#       written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n# Authors: Stuart Glaser, William Woodall, Robert Haschke\n# Maintainer: Morgan Quigley <morgan@osrfoundation.org>\n\nimport xml\nfrom .color import warning\n\ndef first_child_element(elt):\n    c = elt.firstChild\n    while c and c.nodeType != xml.dom.Node.ELEMENT_NODE:\n        c = c.nextSibling\n    return c\n\n\ndef next_sibling_element(node):\n    c = node.nextSibling\n    while c and c.nodeType != xml.dom.Node.ELEMENT_NODE:\n        c = c.nextSibling\n    return c\n\n\ndef replace_node(node, by, content_only=False):\n    parent = node.parentNode\n\n    if by is not None:\n        if not isinstance(by, list):\n            by = [by]\n\n        # insert new content before node\n        for doc in by:\n            if content_only:\n                c = doc.firstChild\n                while c:\n                    n = c.nextSibling\n                    parent.insertBefore(c, node)\n                    c = n\n            else:\n                parent.insertBefore(doc, node)\n\n    # remove node\n    parent.removeChild(node)\n\n\ndef attribute(tag, a):\n    \"\"\"\n    Helper function to fetch a single attribute value from tag\n    :param tag (xml.dom.Element): DOM element node\n    :param a (str): attribute name\n    :return: attribute value if present, otherwise None\n    \"\"\"\n    if tag.hasAttribute(a):\n        # getAttribute returns empty string for non-existent attributes,\n        # which makes it impossible to distinguish with empty values\n        return tag.getAttribute(a)\n    else:\n        return None\n\n\ndef opt_attrs(tag, attrs):\n    \"\"\"\n    Helper routine for fetching optional tag attributes\n    :param tag (xml.dom.Element): DOM element node\n    :param attrs [str]: list of attributes to fetch\n    \"\"\"\n    return [attribute(tag, a) for a in attrs]\n\n\ndef reqd_attrs(tag, attrs):\n    \"\"\"\n    Helper routine for fetching required tag attributes\n    :param tag (xml.dom.Element): DOM element node\n    :param attrs [str]: list of attributes to fetch\n    :raise RuntimeError: if required attribute is missing\n    \"\"\"\n    result = opt_attrs(tag, attrs)\n    for (res, name) in zip(result, attrs):\n        if res is None:\n            raise RuntimeError(\"%s: missing attribute '%s'\" % (tag.nodeName, name))\n    return result\n\n\ndef check_attrs(tag, required, optional):\n    \"\"\"\n    Helper routine to fetch required and optional attributes\n    and complain about any additional attributes.\n    :param tag (xml.dom.Element): DOM element node\n    :param required [str]: list of required attributes\n    :param optional [str]: list of optional attributes\n    \"\"\"\n    result = reqd_attrs(tag, required)\n    result.extend(opt_attrs(tag, optional))\n    allowed = required + optional\n    extra = [a for a in tag.attributes.keys() if a not in allowed]\n    if extra:\n        warning(\"%s: unknown attribute(s): %s\" % (tag.nodeName, ', '.join(extra)))\n    return result\n\n\n# Better pretty printing of xml\n# Taken from http:\/\/ronrothman.com\/public\/leftbraned\/xml-dom-minidom-toprettyxml-and-silly-whitespace\/\ndef fixed_writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n    # indent = current indentation\n    # addindent = indentation to add to higher levels\n    # newl = newline string\n    writer.write(indent + \"<\" + self.tagName)\n\n    attrs = self._get_attributes()\n    a_names = list(attrs.keys())\n    a_names.sort()\n\n    for a_name in a_names:\n        writer.write(\" %s=\\\"\" % a_name)\n        xml.dom.minidom._write_data(writer, attrs[a_name].value)\n        writer.write(\"\\\"\")\n    if self.childNodes:\n        if len(self.childNodes) == 1 \\\n           and self.childNodes[0].nodeType == xml.dom.minidom.Node.TEXT_NODE:\n            writer.write(\">\")\n            self.childNodes[0].writexml(writer, \"\", \"\", \"\")\n            writer.write(\"<\/%s>%s\" % (self.tagName, newl))\n            return\n        writer.write(\">%s\" % newl)\n        for node in self.childNodes:\n            # skip whitespace-only text nodes\n            if node.nodeType == xml.dom.minidom.Node.TEXT_NODE and \\\n                    (not node.data or node.data.isspace()):\n                continue\n            node.writexml(writer, indent + addindent, addindent, newl)\n        writer.write(\"%s<\/%s>%s\" % (indent, self.tagName, newl))\n    else:\n        writer.write(\"\/>%s\" % newl)\n# replace minidom's function with ours\nxml.dom.minidom.Element.writexml = fixed_writexml\n\n\n","license":"bsd-3-clause","hash":921669574792270747,"line_mean":35.5889570552,"line_max":102,"alpha_frac":0.6611334675,"autogenerated":false},
{"repo_name":"anubhavshrimal\/FaceRecognition","path":"train.py","copies":"1","size":"1492","content":"import os\nfrom random import shuffle\nfrom sklearn import svm, neighbors\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nencoding_file_path = '.\/encoded-images-data.csv'\nlabels_fName = 'labels.pkl'\n\nif os.path.isfile(encoding_file_path):\n    df = pd.read_csv(encoding_file_path)\nelse:\n    print('\\x1b[0;37;41m' + '{} does not exist'.format(encoding_file_path) + '\\x1b[0m')\n    quit()\n\nif os.path.isfile(labels_fName):\n    with open(labels_fName, 'rb') as f:\n        le = pickle.load(f)\nelse:\n    print('\\x1b[0;37;41m' + '{} does not exist'.format(labels_fName) + '\\x1b[0m')\n    quit()\n\n# Read the dataframe into a numpy array\n# shuffle the dataset\nfull_data = np.array(df.astype(float).values.tolist())\nshuffle(full_data)\n\n# Extract features and labels\n# remove id column (0th column)\nX = np.array(full_data[:, 1:-1])\ny = np.array(full_data[:, -1:])\n\n# fit the data into a support vector machine\n# clf = svm.SVC(C=1, kernel='linear', probability=True)\nclf = neighbors.KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree', weights='distance')\nclf.fit(X, y.ravel())\n\n\nfName = \".\/classifier.pkl\"\n# if file with same name already exists, backup the old file\nif os.path.isfile(fName):\n    print('\\x1b[0;37;43m' + \"{} already exists. Backing up.\".format(fName) + '\\x1b[0m')\n    os.rename(fName, \"{}.bak\".format(fName))\n\n# save the classifier pickle\nwith open(fName, 'wb') as f:\n    pickle.dump((le, clf), f)\nprint('\\x1b[6;30;42m' + \"Saving classifier to '{}'\".format(fName) + '\\x1b[0m')\n\n","license":"mit","hash":-124771955606633767,"line_mean":28.84,"line_max":94,"alpha_frac":0.6762734584,"autogenerated":false},
{"repo_name":"pmghalvorsen\/gramps_branch","path":"gramps\/plugins\/lib\/libhtml.py","copies":"1","size":"20215","content":"#!\/usr\/bin\/python\n# -*- coding: utf-8 -*-\n#\n# Gramps - a GTK+\/GNOME based genealogy program\n#\n# Copyright (C) 2009 Gerald Britton <gerald.britton@gmail.com>\n#\n# This program is free software; you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n\n\"\"\"\nHTML operations.\n\nThis module exports the Html class\n\n\"\"\"\n\nfrom __future__ import print_function\n#------------------------------------------------------------------------\n# Python modules\n#------------------------------------------------------------------------\nimport re\n\n#------------------------------------------------------------------------\n#\n# GRAMPS modules\n#\n#------------------------------------------------------------------------\nfrom gramps.gen.const import GRAMPS_LOCALE as glocale\nfrom gramps.gen.constfunc import STRTYPE, cuni\n\n#------------------------------------------------------------------------\n#\n# Constants\n#\n#------------------------------------------------------------------------\n__all__ = ['Html']\n\n#------------------------------------------------------------------------\n#\n# XHTML DOCTYPE constants to be used in <!DOCTYPE ... > statements\n#\n# Reference: http:\/\/www.w3.org\/QA\/2002\/04\/valid-dtd-list.html\n#\n#------------------------------------------------------------------------\n\n_XHTML10_STRICT = '\"-\/\/W3C\/\/DTD XHTML 1.0 Strict\/\/EN\"\\n' \\\n                  '\\t\"http:\/\/www.w3.org\/TR\/xhtml1\/DTD\/xhtml1-strict.dtd\"'\n_XTHML10_TRANS = '\"-\/\/W3C\/\/DTD XHTML 1.0 Transitional\/\/EN\"\\n' \\\n                 '\\t\"http:\/\/www.w3.org\/TR\/xhtml1\/DTD\/xhtml1-transitional.dtd\"'\n_XHTML10_FRAME = '\"-\/\/W3C\/\/DTD XHTML 1.0 Frameset\/\/EN\"\\n' \\\n                 '\\t\"http:\/\/www.w3.org\/TR\/xhtml1\/DTD\/xhtml1-frameset.dtd\"'\n_XHTML11 = '\"-\/\/W3C\/\/DTD XHTML 1.1\/\/EN\"\\n' \\\n           '\\t\"http:\/\/www.w3.org\/TR\/xhtml11\/DTD\/xhtml11.dtd\"'\n_XHTML10_BASIC = '\"-\/\/W3C\/\/DTD XHTML Basic 1.0\/\/EN\"\\n' \\\n                 '\\t\"http:\/\/www.w3.org\/TR\/xhtml-basic\/xhtml-basic10.dtd\"'\n_XHTML11_BASIC = '\"-\/\/W3C\/\/DTD XHTML Basic 1.1\/\/EN\"\\n ' \\\n                 '\\t\"http:\/\/www.w3.org\/TR\/xhtml-basic\/xhtml-basic11.dtd\"'\n_HTML5 = \"\"\n\n#------------------------------------------------------------------------\n#\n# XML Namespace constant for use in <html xmlns=...> tags \n#\n#------------------------------------------------------------------------\n\n_XMLNS = \"http:\/\/www.w3.org\/1999\/xhtml\"\n\n#------------------------------------------------------------------------\n#\n# Local constants.\n#\n#------------------------------------------------------------------------\n\n# Set of html tags that do not use a complementary closing tag, but close with\n# \/> instead\n_START_CLOSE = set([\n    'area', \n    'base', \n    'br', \n    'frame', \n    'hr',\n    'img', \n    'input', \n    'link', \n    'meta', \n    'param'\n    ])\n\n#------------------------------------------------------------------------\n#\n# Html class.\n#\n#------------------------------------------------------------------------\n\nclass Html(list):\n    \"\"\"\n    HTML class: Manages a rooted tree of HTML objects\n    \"\"\"\n    __slots__ = ['items', 'indent', 'inline', 'close']\n#\n    @staticmethod\n    def xmldecl(version=1.0, encoding=\"UTF-8\", standalone=\"no\"):\n        \"\"\"\n        Build and return an XML declaration statement\n\n        :type  version: decimal number\n        :param version: version of XML to be used. Defaults to 1.0\n        :type  encoding: string\n        :param encoding: encoding method to be used. Defaults to \"UTF-8\"\n        :type  standalone: string\n        :param standalone: \"yes\" or \"no\".  Defaults to \"no\"\n        \"\"\"\n        return '<?xml %s %s %s?>' % (\n            'version=\"%s\"' % version,\n            'encoding=\"%s\"' % encoding,\n            'standalone=\"%s\"' % standalone\n            )\n#\n    @staticmethod\n    def doctype(name='HTML', public='', external_id=_HTML5):\n        \"\"\"\n        Build and return a DOCTYPE statement\n\n        :type  name: string\n        :param name: name of this DOCTYPE. Defaults to \"html\"\n        :type  public: string\n        :param public: class of this DOCTYPE. Defaults to 'PUBLIC\n        :type  external_id: string\n        :param external_id: external identifier of this DOCTYPE. \n                            Defaults to XHTML 1.0 STRICT\n        :type  args: object\n        :param args: 0 or more positional parameters to be added to this    \n                     DOCTYPE.\n        \"\"\"\n        return (\n            '<!DOCTYPE %s %s %s' % (\n                name,\n                public, \n                external_id,\n                )\n            ).rstrip() + '>'\n#\n    @staticmethod\n    def html(xmlns=_XMLNS, lang='en', *args, **keywargs):\n        \"\"\"\n        Build and return a properly-formated <html> object\n\n        :type  xmlns: string\n        :param xmlns: XML namespace string. Default = 'http:\/\/www.w3.org\/1999\/xhtml'\n        :type  lang: string\n        :param lang: language to be used. Defaul = 'en'\n        :rtype:   reference to new Html instance\n        :returns:  reference to the newly-created Html instances for <html> object\n        \"\"\"      \n        return Html('html',\n            indent=False,\n            xmlns=xmlns,\n            attr='xml:lang=\"%s\" lang=\"%s\"' % ((lang,)*2),\n            *args, **keywargs\n            )\n#\n    @staticmethod\n    def head(title=None, encoding='utf-8', html5=True, *args, **keywargs):\n        \"\"\"\n        Build and return a properly-formated <head> object\n    \n        :type  title: string or None\n        :param title: title for HTML page. Default=None. If None no \n                    title tag is written\n        :type  encoding: string\n        :param encoding: encoding to be used. Default = 'utf-8'\n        :param html5: generate html5 syntax. Default = True. Set to False\n                      if pre-html5 syntax required\n        :rtype:  reference to new Html instance\n        :returns: reference to the newly-created Html instances for <head> object\n        \"\"\" \n\n        head = Html('head', *args, **keywargs) \n        if title is not None: \n            head += (Html('title', title, inline=True, indent=True))\n        if html5:\n            head += Html('meta', charset=encoding, indent=True)\n        else:\n            meta1 = 'http-equiv=\"content-type\" content=\"text\/html;charset=%s\"'\n            meta2 = 'http-equiv=\"Content-Style-Type\" content=\"text\/css\"'\n            head += Html('meta', attr=meta1 % encoding, indent=True)\n            head += Html('meta', attr=meta2, indent=True)\n        return head\n#\n    @staticmethod\n    def page(title=None, encoding='utf-8', lang='en', html5=True, *args, **keywargs):\n        \"\"\"\n        This function prepares a new Html class based page and returns\n    \n        :type  title: string\n        :param title: title for HTML page. Default=None\n        :type  encoding: string\n        :param encoding: encoding to be used. Default = 'utf-8'\n        :type  lang: string\n        :param lang: language to be used. Defaul = 'en'\n        :param html5: generate html5 syntax. Default = True. Set to False\n                      if pre-html5 syntax required\n        :rtype:   three object references\n        :returns:  references to the newly-created Html instances for\n                  page, head and body\n        \"\"\"\n        page = Html.html(lang=lang, *args, **keywargs)\n        if html5:\n            page.addDOCTYPE(external_id=_HTML5)\n        else:\n            page.addXML(encoding=encoding)\n            page.addDOCTYPE(external_id=_XHTML10_STRICT)\n#\n        head = Html.head(title=title,\n               encoding=encoding,\n               lang=lang,\n               html5=html5,\n               indent=False,\n               *args, **keywargs \n               )\n#\n        body = Html('body', indent=False, *args, **keywargs)\n        page += (head, body)\n        return page, head, body\n#\n    def __init__(self, tag='html', *args, **keywargs):\n        \"\"\"\n        Class Constructor: Returns a new instance of the Html class\n        \n        :type  tag: string\n        :param tag: The HTML tag. Default is 'html'\n        :type  args: optional positional parameters\n        :param args: 0 more positional arguments to be inserted between\n                     opening and closing HTML tags.\n        :type  indent: boolean or None\n        :param indent: True  ==> indent this object with respect to its parent\n                       False ==> do not indent this object\n                       None  ==> no indent for this object (use eg for pre tag)\n                       Defaults to True\n        :type  inline: boolean\n        :param inline: True  ==> instructs the write() method to output this\n                                 object and any child objects as a single string\n                       False ==> output this object and its contents one string\n                                 at a time\n                       Defaults to False\n        :type  close: boolean or None\n        :param close: True  ==> this tag should be closed normally\n                                e.g. <tag>...<\/tag>\n                      False ==> this tag should be automatically closed\n                                e.g. <tag \/>\n                      None  ==> do not provide any closing for this tag\n        :type  keywargs: optional keyword parameters\n        :param keywargs: 0 or more keyword=argument pairs that should be\n                         copied into the opening tag as keyword=\"argument\"\n                         attributes\n        :rtype:   object reference\n        :returns:  reference to the newly-created Html instance\n        \n        For full usage of the Html class with examples, please see the wiki\n        page at: http:\/\/www.gramps-project.org\/wiki\/index.php?title=Libhtml\n        \"\"\"\n        # Replace super(Html, self) with list\n        # Issue with Python 2.6 and reload of plugin\n        list.__init__(self, [])                  # instantiate object\n        attr = ''\n        self.indent, self.close, self.inline = True, True, False\n#\n#       Handle keyword arguments passed to this constructor.\n#       Keywords that we process directly are handled. \n#       Keywords we don't recognize are saved for later \n#       addition to the opening tag as attributes.\n#\n        for keyw, arg in keywargs.items():\n            if (keyw in ['indent', 'close', 'inline'] and\n               arg in [True, False, None]):\n                setattr(self, keyw, arg)\n            elif keyw == 'attr':                        # pass attributes along\n                attr += ' ' + arg\n            elif keyw[-1] == '_':                       # avoid Python conflicts\n                attr += ' %s=\"%s\"' % (keyw[:-1], arg)   # pass keyword arg along\n            else:\n                attr += ' %s=\"%s\"' % (keyw, arg)        # pass keyword arg along\n#\n        if tag[0] == '<':               # if caller provided preformatted tag?\n            self[0:] = [tag]            #   add it in\n            self.close = None           #   caller must close the tag\n        else: \n            if tag in _START_CLOSE:     # if tag in special list\n                self.close = False      #   it needs no closing tag\n            begin = '<%s%s%s>' % (      # build opening tag with attributes\n                tag,\n                attr,\n                ('' if self.close is not False else ' \/')\n                )\n#\n            # Use slice syntax since we don't override slicing\n            self[0:] = [begin] + list(args)         # add beginning tag\n            if self.close:                          # if need closing tab\n                self[len(self):] = ['<\/%s>' % tag]         #   add it on the end\n#\n    def __add(self, value):\n        \"\"\"\n        Helper function for +, +=, operators and append() and extend()\n        methods\n\n        :type  value: object\n        :param value: object to be added\n\n        :rtype:  object reference\n        :returns: reference to object with new value added\n        \"\"\"\n        if (isinstance(value, Html) or not hasattr(value, '__iter__') or\n                isinstance(value, STRTYPE)):\n            value = [value]\n        index = len(self) - (1 if self.close else 0)\n        self[index:index] = value\n        return self\n#\n    __iadd__ = __add__ = __add\n#\n    def append(self, value):\n        \"\"\"\n        Append a new value\n        \"\"\"\n        self.__add(value)\n#\n    extend = append\n#\n    def replace(self, cur_value, value):\n        \"\"\"\n        Replace current value with new value\n\n        :type  cur_value: object\n        :param cur_value: value of object to be replaced\n        :type  value: object\n        :param value: replacement value\n\n        :rtype:  object reference\n        :returns: reference to object with new value added\n        \"\"\"\n        self[self.index(cur_value)] = value\n#\n    def __sub__(self, value):\n        \"\"\"\n        Overload function for - and -= operators\n        :type  value: object\n        :param value: object to be removed\n\n        :rtype:  object reference\n        :returns: reference to object with value removed\n        \"\"\"\n        del self[self.index(value)]\n        return self\n#\n    __isub__ = remove = __sub__\n#\n    def __str__(self):\n        \"\"\"\n        Returns string representation\n\n        :rtype:  string\n        :returns: string representation of object\n        \"\"\"\n        return '%s'*len(self) % tuple(self[:])\n#\n    def __iter__(self):\n        \"\"\"\n        Iterator function: returns a generator that performs an\n        insertion-order tree traversal and yields each item found.\n        \"\"\"\n        for item in self[:]:                    # loop through all list elements\n            if isinstance(item, Html):     # if nested list found\n                for sub_item in item:           #     recurse\n                    yield sub_item\n            else:\n                yield item\n#\n    iterkeys = itervalues = iteritems = __iter__\n#\n    def write(self, method=print, indent='\\t', tabs=''):\n        \"\"\"\n        Output function: performs an insertion-order tree traversal\n        and calls supplied method for each item found.\n\n        :type  method: function reference\n        :param method: function to call with each item found\n        :type  indent: string\n        :param indenf: string to use for indentation. Default = '\\t' (tab)\n        :type  tabs: string\n        :param tabs: starting indentation\n        \"\"\"\n        if self.indent is None:\n            tabs = ''\n        elif self.indent: \n            tabs += indent\n        if self.inline:                         # if inline, write all list and\n            method(cuni('%s%s' % (tabs, self)))       # nested list elements\n#\n        else:\n            for item in self[:]:                # else write one at a time\n                if isinstance(item, Html):      # recurse if nested Html class\n                    item.write(method=method, indent=indent, tabs=tabs)\n                else:\n                    method(cuni('%s%s' % (tabs, item)))  # else write the line\n#\n    def addXML(self, version=1.0, encoding=\"UTF-8\", standalone=\"no\"):\n        \"\"\"\n        Add an XML statement to the start of the list for this object\n\n        :type  version: decimal number\n        :param version: version of XML to be used. Defaults to 1.0\n        :type  encoding: string\n        :param encoding: encoding method to be used. Defaults to \"UTF-8\"\n        :type  standalone: string\n        :param standalone: \"yes\" or \"no\".  Defaults to \"no\"\n        \"\"\"\n        xmldecl = Html.xmldecl(\n            version=version,\n            encoding=encoding,\n            standalone=standalone\n            )\n        self[0:0] = [xmldecl]\n#\n    def addDOCTYPE(self, name='html', public='PUBLIC',\n            external_id=_HTML5, *args):\n        \"\"\"\n        Add a DOCTYPE statement to the start of the list\n\n        :type  name: string\n        :param name: name of this DOCTYPE. Defaults to \"html\"\n        :type  external_id: string\n        :param external_id: external identifier of this DOCTYPE. \n                            Defaults to XHTML 1.0 STRICT\n        :type  args: object\n        :param args: 0 or more positional parameters to be added to this    \n                     DOCTYPE.\n        \"\"\"\n        doctype = (\n            '<!DOCTYPE %s %s %s%s' % (\n                name,\n                ('' if external_id ==_HTML5 else public),\n                external_id,\n                ' %s'*len(args) % args\n                )\n            ).rstrip() + '>'\n        # Note: DOCTYPE declaration must follow XML declaration\n\n        if len(self) and self[0][:6] == '<?xml ':\n            self[1:1] = [doctype]\n        else:\n            self[0:0] = [doctype]\n#\n    def __gettag(self):\n        \"\"\"\n        Returns HTML tag for this object\n\n        :rtype:  string\n        :returns: HTML tag\n        \"\"\"\n        return self[0].split()[0].strip('< >')\n#\n    def __settag(self, newtag):\n        \"\"\"\n        Sets a new HTML tag for this object\n\n        :type  name: string\n        :param name: new HTML tag\n        \"\"\"\n        curtag = self.tag\n        \n        # Replace closing tag, if any\n        \n        if self[-1] == '<\/%s>' % curtag:\n            self[-1] = '<\/%s>' % newtag\n\n        # Replace opening tag\n\n        self[0] = self[0].replace('<' + curtag, '<' + newtag)\n    tag = property(__gettag, __settag)\n#\n    def __getattr(self):\n        \"\"\"\n        Returns HTML attributes for this object\n\n        :rtype:  string\n        :returns: HTML attributes\n        \"\"\"\n        attr = self[0].strip('<!?\/>').split(None, 1)\n        return attr[1] if len(attr) > 1 else ''\n#\n    def __setattr(self, value):\n        \"\"\"\n        Sets new HTML attributes for this object\n\n        :type  name: string\n        :param name: new HTML attributes\n        \"\"\"\n        beg = len(self.tag) + 1\n\n        # See if self-closed or normal\n        \n        end = -2 if self.close is False else -1\n        self[0] = self[0][:beg] + ' ' + value + self[0][end:]        \n#\n    def __delattr(self):\n        \"\"\"\n        Removes HTML attributes for this object\n        \"\"\"\n        self[0] = '<' + self.tag + (\n\n                # Set correct closing delimiter(s)\n        \n                ' \/>' if self.close is False else '>'\n                )\n#\n    attr = property(__getattr,  __setattr, __delattr)\n#\n    def __getinside(self):\n        \"\"\"\n        Returns list of items between opening and closing tags\n\n        :rtype:  list\n        :returns: list of items between opening and closing HTML tags \n        \"\"\"\n        return self[1:-1]\n#\n    def __setinside(self, value):\n        \"\"\"\n        Sets new contents between opening and closing tags\n\n        :type  name: list\n        :param name: new HTML contents\n        \"\"\"\n        if len(self) < 2:\n            raise AttributeError('No closing tag. Cannot set inside value')\n        if (isinstance(value, Html) or not hasattr(value, '__iter__') or\n                isinstance(value, STRTYPE)):\n            value = [value]\n        self[1:-1] = value\n#\n    def __delinside(self):\n        \"\"\"\n        Removes contents between opening and closing tag\n        \"\"\"\n        if len(self) > 2:\n            self[:] = self[:1] + self[-1:]\n    inside = property(__getinside, __setinside, __delinside)\n#\n    def __enter__(self):\n        return self\n#\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return exc_type is None\n\n#------------------------------------------------------------------------\n#\n# Functions\n#\n#------------------------------------------------------------------------\ndef xml_lang():\n    return glocale.lang[:5].replace('_', '-')\n\n#-------------------------------------------\n#\n#           Unit tests\n#\n#-------------------------------------------\n\ndef htmltest():\n    pass\n\nif __name__ == '__main__':\n    from libhtmltest import htmltest\n    htmltest()    \n","license":"gpl-2.0","hash":3004708834006547477,"line_mean":33.4378194208,"line_max":85,"alpha_frac":0.5024981449,"autogenerated":false},
{"repo_name":"apagac\/cfme_tests","path":"cfme\/tests\/v2v\/test_v2v_schedule_migrations.py","copies":"2","size":"2638","content":"\"\"\"Tests to validate schedule migration usecases\"\"\"\nimport fauxfactory\nimport pytest\n\nfrom cfme import test_requirements\nfrom cfme.cloud.provider.openstack import OpenStackProvider\nfrom cfme.fixtures.v2v_fixtures import get_migrated_vm\nfrom cfme.infrastructure.provider.rhevm import RHEVMProvider\nfrom cfme.infrastructure.provider.virtualcenter import VMwareProvider\nfrom cfme.markers.env_markers.provider import ONE_PER_TYPE\nfrom cfme.markers.env_markers.provider import ONE_PER_VERSION\nfrom cfme.utils.appliance.implementations.ui import navigate_to\n\npytestmark = [\n    test_requirements.v2v,\n    pytest.mark.provider(\n        classes=[RHEVMProvider, OpenStackProvider],\n        selector=ONE_PER_VERSION,\n        required_flags=[\"v2v\"],\n        scope=\"module\"\n    ),\n    pytest.mark.provider(\n        classes=[VMwareProvider],\n        selector=ONE_PER_TYPE,\n        fixture_name=\"source_provider\",\n        required_flags=[\"v2v\"],\n        scope=\"module\"\n    ),\n    pytest.mark.usefixtures(\"v2v_provider_setup\")\n]\n\n\n@pytest.mark.tier(3)\ndef test_schedule_migration(appliance, provider, mapping_data_vm_obj_mini, soft_assert):\n    \"\"\"\n    Test to validate schedule migration plan\n\n    Polarion:\n        assignee: ytale\n        initialEstimate: 1\/2h\n        caseimportance: medium\n        caseposneg: positive\n        testtype: functional\n        startsin: 5.10\n        casecomponent: V2V\n        testSteps:\n            1. Add source and target provider\n            2. Create infra map and migration plan\n            3. Schedule migration plan\n    \"\"\"\n    migration_plan_collection = appliance.collections.v2v_migration_plans\n    src_vm_obj = mapping_data_vm_obj_mini.vm_list[0]\n    migration_plan = migration_plan_collection.create(\n        name=\"plan_{}\".format(fauxfactory.gen_alphanumeric()),\n        description=\"desc_{}\".format(fauxfactory.gen_alphanumeric()),\n        infra_map=mapping_data_vm_obj_mini.infra_mapping_data.get(\"name\"),\n        target_provider=provider,\n        vm_list=mapping_data_vm_obj_mini.vm_list,\n        start_migration=False\n    )\n    view = navigate_to(migration_plan_collection, \"NotStarted\")\n    view.plans_not_started_list.schedule_migration(migration_plan.name)\n    soft_assert(\"Migration scheduled\" in view.plans_not_started_list.get_clock(migration_plan.name))\n\n    assert migration_plan.wait_for_state(\"Started\")\n    assert migration_plan.wait_for_state(\"In_Progress\")\n    assert migration_plan.wait_for_state(\"Completed\")\n    assert migration_plan.wait_for_state(\"Successful\")\n    migrated_vm = get_migrated_vm(src_vm_obj, provider)\n    soft_assert(src_vm_obj.mac_address == migrated_vm.mac_address)\n","license":"gpl-2.0","hash":-524129016343387688,"line_mean":36.6857142857,"line_max":100,"alpha_frac":0.7043214556,"autogenerated":false},
{"repo_name":"talkatv\/talkatv","path":"talkatv\/models.py","copies":"1","size":"5327","content":"# talkatv - Commenting backend for static pages\n# Copyright (C) 2012  talkatv contributors, see AUTHORS\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport bcrypt\n\nfrom datetime import datetime\n\nfrom migrate import changeset\nassert changeset  # silence code analysers\n\nfrom talkatv import db\nfrom talkatv.comment import parse_comment\n\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(60), unique=True)\n    email = db.Column(db.String(255), unique=True)\n    password = db.Column(db.String(60))\n\n    def __init__(self, username, email, password=None, openid=None):\n        self.username = username\n        self.email = email\n\n        if password:\n            self.set_password(password)\n\n        if openid:\n            self.openid = openid\n\n    def __repr__(self):\n        return '<User {0}>'.format(self.username)\n\n    def set_password(self, password):\n        self.password = bcrypt.hashpw(password, bcrypt.gensalt())\n\n    def check_password(self, password):\n        return bcrypt.hashpw(password, self.password) == self.password\n\n\nclass OpenID(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    url = db.Column(db.String())\n    created = db.Column(db.DateTime)\n\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\n    user = db.relationship('User',\n            backref=db.backref('openids', lazy='dynamic'))\n\n    def __init__(self, user, url):\n        self.created = datetime.utcnow()\n        self.user = user\n        self.url = url\n\n\nclass Item(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String())\n    url = db.Column(db.String(), unique=True)\n    created = db.Column(db.DateTime)\n\n    site_id = db.Column(db.Integer, db.ForeignKey('site.id'))\n    site = db.relationship('Site',\n            backref=db.backref('items', lazy='dynamic'))\n\n    def __init__(self, url, title, site=None):\n        if site:\n            self.site = site\n\n        self.title = title\n        self.url = url\n\n        self.created = datetime.utcnow()\n\n    def __repr__(self):\n        return '<Item {0} ({1})>'.format(\n                self.url,\n                self.site.owner.username if self.site else None)\n\n    def as_dict(self):\n        me = {\n                'id': self.id,\n                'title': self.title,\n                'url': self.url,\n                'created': self.created.isoformat()}\n        if self.site:\n            me.update({'owner': self.site.owner.id})\n\n        return me\n\n\nclass Site(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    created = db.Column(db.DateTime)\n    domain = db.Column(db.String)\n\n    owner_id = db.Column(db.Integer, db.ForeignKey('user.id'))\n    owner = db.relationship('User',\n            backref=db.backref('sites', lazy='dynamic'))\n\n    def __init__(self, owner, domain):\n        self.owner = owner\n        self.domain = domain\n\n        self.created = datetime.utcnow()\n\n    def __repr__(self):\n        return '<Site {0} ({1})>'.format(\n                self.domain,\n                self.owner.username)\n\n\nclass Comment(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    created = db.Column(db.DateTime)\n    text = db.Column(db.String())\n\n    item_id = db.Column(db.Integer, db.ForeignKey('item.id'))\n    item = db.relationship('Item',\n            backref=db.backref('comments', lazy='dynamic'))\n\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\n    user = db.relationship('User',\n            backref=db.backref('comments', lazy='dynamic'))\n\n    reply_to_id = db.Column(db.Integer, db.ForeignKey('comment.id'))\n    reply_to = db.relationship('Comment', remote_side=[id],\n            backref=db.backref('replies', lazy='dynamic'))\n\n    def __init__(self, item, user, text, reply_to=None):\n        self.item = item\n        self.user = user\n        self.text = text\n\n        if reply_to:\n            if reply_to is int:\n                self.reply_to_id = reply_to\n            else:\n                self.reply_to = reply_to\n\n        self.created = datetime.utcnow()\n\n    def __repr__(self):\n        return '<Comment {0} ({1})>'.format(\n                self.text[:25] + ('...' if len(self.text) > 25 else ''),\n                self.user.username)\n\n    def as_dict(self):\n        me = {\n                'id': self.id,\n                'item': self.item.id,\n                'user_id': self.user.id,\n                'username': self.user.username,\n                'text': self.text,\n                'html': parse_comment(self.text),\n                'reply_to': self.reply_to_id,\n                'created': self.created.isoformat()}\n        return me\n\nMODELS = [\n        User,\n        Comment,\n        Item,\n        OpenID,\n        Site]\n","license":"agpl-3.0","hash":-2850868067158696303,"line_mean":28.9269662921,"line_max":74,"alpha_frac":0.591514924,"autogenerated":false},
{"repo_name":"ListFranz\/speech_recognition","path":"speech_recognition\/__init__.py","copies":"1","size":"19357","content":"#!\/usr\/bin\/env python3\n\n\"\"\"Library for performing speech recognition with the Google Speech Recognition API.\"\"\"\n\n__author__ = \"Anthony Zhang (Uberi)\"\n__version__ = \"2.1.3\"\n__license__ = \"BSD\"\n\nimport io, os, subprocess, wave\nimport math, audioop, collections\nimport json\n\ntry: # try to use python2 module\n    from urllib2 import Request, urlopen, URLError\nexcept ImportError: # otherwise, use python3 module\n    from urllib.request import Request, urlopen\n    from urllib.error import URLError\n\n#wip: filter out clicks and other too short parts\n\nclass AudioSource(object):\n    def __init__(self):\n        raise NotImplementedError(\"this is an abstract class\")\n\n    def __enter__(self):\n        raise NotImplementedError(\"this is an abstract class\")\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        raise NotImplementedError(\"this is an abstract class\")\n\ntry:\n    import pyaudio\n    class Microphone(AudioSource):\n        \"\"\"\n        This is available if PyAudio is available, and is undefined otherwise.\n\n        Creates a new ``Microphone`` instance, which represents a physical microphone on the computer. Subclass of ``AudioSource``.\n\n        If ``device_index`` is unspecified or ``None``, the default microphone is used as the audio source. Otherwise, ``device_index`` should be the index of the device to use for audio input.\n        \"\"\"\n        def __init__(self, device_index = None):\n            assert device_index is None or isinstance(device_index, int), \"Device index must be None or an integer\"\n            if device_index is not None: # ensure device index is in range\n                audio = pyaudio.PyAudio(); count = audio.get_device_count(); audio.terminate() # obtain device count\n                assert 0 <= device_index < count, \"Device index out of range\"\n            self.device_index = device_index\n            self.format = pyaudio.paInt16 # 16-bit int sampling\n            self.SAMPLE_WIDTH = pyaudio.get_sample_size(self.format)\n            self.RATE = 16000 # sampling rate in Hertz\n            self.CHANNELS = 1 # mono audio\n            self.CHUNK = 1024 # number of frames stored in each buffer\n\n            self.audio = None\n            self.stream = None\n\n        def __enter__(self):\n            self.audio = pyaudio.PyAudio()\n            self.stream = self.audio.open(\n                input_device_index = self.device_index,\n                format = self.format, rate = self.RATE, channels = self.CHANNELS, frames_per_buffer = self.CHUNK,\n                input = True, # stream is an input stream\n            )\n            return self\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            self.stream.stop_stream()\n            self.stream.close()\n            self.stream = None\n            self.audio.terminate()\nexcept ImportError:\n    pass\n\nclass WavFile(AudioSource):\n    \"\"\"\n    Creates a new ``WavFile`` instance, which represents a WAV audio file. Subclass of ``AudioSource``.\n\n    If ``filename_or_fileobject`` is a string, then it is interpreted as a path to a WAV audio file (mono or stereo) on the filesystem. Otherwise, ``filename_or_fileobject`` should be a file-like object such as ``io.BytesIO`` or similar. In either case, the specified file is used as the audio source.\n    \"\"\"\n\n    def __init__(self, filename_or_fileobject):\n        if isinstance(filename_or_fileobject, str):\n            self.filename = filename_or_fileobject\n        else:\n            assert filename_or_fileobject.read, \"Given WAV file must be a filename string or a file object\"\n            self.filename = None\n            self.wav_file = filename_or_fileobject\n        self.stream = None\n        self.DURATION = None\n\n    def __enter__(self):\n        if self.filename: self.wav_file = open(self.filename, \"rb\")\n        self.wav_reader = wave.open(self.wav_file, \"rb\")\n        self.SAMPLE_WIDTH = self.wav_reader.getsampwidth()\n        self.RATE = self.wav_reader.getframerate()\n        self.CHANNELS = self.wav_reader.getnchannels()\n        assert self.CHANNELS == 1 or self.CHANNELS == 2, \"Audio must be mono or stereo\"\n        self.CHUNK = 4096\n        self.FRAME_COUNT = self.wav_reader.getnframes()\n        self.DURATION = self.FRAME_COUNT \/ float(self.RATE)\n        self.stream = WavFile.WavStream(self.wav_reader)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.filename: self.wav_file.close()\n        self.stream = None\n        self.DURATION = None\n\n    class WavStream(object):\n        def __init__(self, wav_reader):\n            self.wav_reader = wav_reader\n\n        def read(self, size = -1):\n            buffer = self.wav_reader.readframes(self.wav_reader.getnframes() if size == -1 else size)\n            if self.wav_reader.getnchannels() != 1: # stereo audio\n                buffer = audioop.tomono(buffer, self.wav_reader.getsampwidth(), 1, 1) # convert stereo audio data to mono\n            return buffer\n\nclass AudioData(object):\n    def __init__(self, rate, data):\n        self.rate = rate\n        self.data = data\n\nclass Recognizer(AudioSource):\n    def __init__(self, language = \"en-US\", key = \"AIzaSyBOti4mM-6x9WDnZIjIeyEU21OpBXqWBgw\"):\n        \"\"\"\n        Creates a new ``Recognizer`` instance, which represents a collection of speech recognition functionality.\n\n        The language is determined by ``language``, a standard language code like `\"en-US\"` or `\"en-GB\"`, and defaults to US English. A list of supported language codes can be found `here <http:\/\/stackoverflow.com\/questions\/14257598\/>`__. Basically, language codes can be just the language (``en``), or a language with a dialect (``en-US``).\n\n        The Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key that works out of the box.\n        \"\"\"\n        assert isinstance(language, str), \"Language code must be a string\"\n        assert isinstance(key, str), \"Key must be a string\"\n        self.key = key\n        self.language = language\n\n        self.energy_threshold = 300 # minimum audio energy to consider for recording\n        self.dynamic_energy_threshold = True\n        self.dynamic_energy_adjustment_damping = 0.15\n        self.dynamic_energy_ratio = 1.5\n        self.pause_threshold = 0.8 # seconds of quiet time before a phrase is considered complete\n        self.quiet_duration = 0.5 # amount of quiet time to keep on both sides of the recording\n\n    def samples_to_flac(self, source, frame_data):\n        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n        import platform, os, stat\n        with io.BytesIO() as wav_file:\n            wav_writer = wave.open(wav_file, \"wb\")\n            try: # note that we can't use context manager due to Python 2 not supporting it\n                wav_writer.setsampwidth(source.SAMPLE_WIDTH)\n                wav_writer.setnchannels(source.CHANNELS)\n                wav_writer.setframerate(source.RATE)\n                wav_writer.writeframes(frame_data)\n            finally:  # make sure resources are cleaned up\n                wav_writer.close()\n            wav_data = wav_file.getvalue()\n\n        # determine which converter executable to use\n        system = platform.system()\n        path = os.path.dirname(os.path.abspath(__file__)) # directory of the current module file, where all the FLAC bundled binaries are stored\n        flac_converter = shutil_which(\"flac\") # check for installed version first\n        if flac_converter is None: # flac utility is not installed\n            if system == \"Windows\" and platform.machine() in [\"i386\", \"x86\", \"x86_64\", \"AMD64\"]: # Windows NT, use the bundled FLAC conversion utility\n                flac_converter = os.path.join(path, \"flac-win32.exe\")\n            elif system == \"Linux\" and platform.machine() in [\"i386\", \"x86\", \"x86_64\", \"AMD64\"]:\n                flac_converter = os.path.join(path, \"flac-linux-i386\")\n            elif system == 'Darwin' and platform.machine() in [\"i386\", \"x86\", \"x86_64\", \"AMD64\"]:\n                flac_converter = os.path.join(path, \"flac-mac\")\n            else:\n                raise OSError(\"FLAC conversion utility not available - consider installing the FLAC command line application using `brew install flac` or your operating system's equivalent\")\n\n        # mark covnerter as executable\n        try:\n            stat_info = os.stat(flac_converter)\n            os.chmod(flac_converter, stat_info.st_mode | stat.S_IEXEC)\n        except OSError: pass\n\n        process = subprocess.Popen(\"\\\"%s\\\" --stdout --totally-silent --best -\" % flac_converter, stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True)\n        flac_data, stderr = process.communicate(wav_data)\n        return flac_data\n\n    def record(self, source, duration = None, offset = None):\n        \"\"\"\n        Records up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at ``offset`` (or at the beginning if not specified) into an ``AudioData`` instance, which it returns.\n\n        If ``duration`` is not specified, then it will record until there is no more audio input.\n        \"\"\"\n        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n\n        frames = io.BytesIO()\n        seconds_per_buffer = (source.CHUNK + 0.0) \/ source.RATE\n        elapsed_time = 0\n        offset_time = 0\n        offset_reached = False\n        while True: # loop for the total number of chunks needed\n            if offset and not offset_reached:\n                offset_time += seconds_per_buffer\n                if offset_time > offset:\n                    offset_reached = True\n\n            buffer = source.stream.read(source.CHUNK)\n            if len(buffer) == 0: break\n\n            if offset_reached or not offset:\n                elapsed_time += seconds_per_buffer\n                if duration and elapsed_time > duration: break\n\n                frames.write(buffer)\n\n        frame_data = frames.getvalue()\n        frames.close()\n        return AudioData(source.RATE, self.samples_to_flac(source, frame_data))\n\n    def adjust_for_ambient_noise(self, source, duration = 1):\n        \"\"\"\n        Adjusts the energy threshold dynamically using audio from ``source`` (an ``AudioSource`` instance) to account for ambient noise.\n\n        Intended to calibrate the energy threshold with the ambient energy level. Should be used on periods of audio without speech - will stop early if any speech is detected.\n\n        The ``duration`` parameter is the maximum number of seconds that it will dynamically adjust the threshold for before returning. This value should be at least 0.5 in order to get a representative sample of the ambient noise.\n        \"\"\"\n        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n\n        seconds_per_buffer = (source.CHUNK + 0.0) \/ source.RATE\n        elapsed_time = 0\n\n        # adjust energy threshold until a phrase starts\n        while True:\n            elapsed_time += seconds_per_buffer\n            if elapsed_time > duration: break\n            buffer = source.stream.read(source.CHUNK)\n\n            # check if the audio input has stopped being quiet\n            energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n\n            # dynamically adjust the energy threshold using assymmetric weighted average\n            damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n            target_energy = energy * self.dynamic_energy_ratio\n            self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n\n    def listen(self, source, timeout = None):\n        \"\"\"\n        Records a single phrase from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance, which it returns.\n\n        This is done by waiting until the audio has an energy above ``recognizer_instance.energy_threshold`` (the user has started speaking), and then recording until it encounters ``recognizer_instance.pause_threshold`` seconds of silence or there is no more audio input. The ending silence is not included.\n\n        The ``timeout`` parameter is the maximum number of seconds that it will wait for a phrase to start before giving up and throwing an ``OSError`` exception. If ``None``, it will wait indefinitely.\n        \"\"\"\n        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n\n        # record audio data as raw samples\n        frames = collections.deque()\n        assert self.pause_threshold >= self.quiet_duration >= 0\n        seconds_per_buffer = (source.CHUNK + 0.0) \/ source.RATE\n        pause_buffer_count = int(math.ceil(self.pause_threshold \/ seconds_per_buffer)) # number of buffers of quiet audio before the phrase is complete\n        quiet_buffer_count = int(math.ceil(self.quiet_duration \/ seconds_per_buffer)) # maximum number of buffers of quiet audio to retain before and after\n        elapsed_time = 0\n\n        # store audio input until the phrase starts\n        while True:\n            elapsed_time += seconds_per_buffer\n            if timeout and elapsed_time > timeout: # handle timeout if specified\n                raise OSError(\"listening timed out\")\n\n            buffer = source.stream.read(source.CHUNK)\n            if len(buffer) == 0: break # reached end of the stream\n            frames.append(buffer)\n\n            # check if the audio input has stopped being quiet\n            energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n            if energy > self.energy_threshold: break\n\n            # dynamically adjust the energy threshold using assymmetric weighted average\n            if self.dynamic_energy_threshold:\n                damping = self.dynamic_energy_adjustment_damping ** seconds_per_buffer # account for different chunk sizes and rates\n                target_energy = energy * self.dynamic_energy_ratio\n                self.energy_threshold = self.energy_threshold * damping + target_energy * (1 - damping)\n\n            if len(frames) > quiet_buffer_count: # ensure we only keep the needed amount of quiet buffers\n                frames.popleft()\n\n        # read audio input until the phrase ends\n        pause_count = 0\n        while True:\n            buffer = source.stream.read(source.CHUNK)\n            if len(buffer) == 0: break # reached end of the stream\n            frames.append(buffer)\n\n            # check if the audio input has gone quiet for longer than the pause threshold\n            energy = audioop.rms(buffer, source.SAMPLE_WIDTH) # energy of the audio signal\n            if energy > self.energy_threshold:\n                pause_count = 0\n            else:\n                pause_count += 1\n            if pause_count > pause_buffer_count: # end of the phrase\n                break\n\n         # obtain frame data\n        for i in range(quiet_buffer_count, pause_count): frames.pop() # remove extra quiet frames at the end\n        frame_data = b\"\".join(list(frames))\n\n        return AudioData(source.RATE, self.samples_to_flac(source, frame_data))\n\n    def recognize(self, audio_data, show_all = False):\n        \"\"\"\n        Performs speech recognition, using the Google Speech Recognition API, on ``audio_data`` (an ``AudioData`` instance).\n\n        Returns the most likely transcription if ``show_all`` is ``False``, otherwise it returns a ``dict`` of all possible transcriptions and their confidence levels.\n\n        Note: confidence is set to 0 if it isn't given by Google\n\n        Also raises a ``LookupError`` exception if the speech is unintelligible, a ``KeyError`` if the key isn't valid or the quota for the key has been maxed out, and ``IndexError`` if there is no internet connection.\n        \"\"\"\n        assert isinstance(audio_data, AudioData), \"Data must be audio data\"\n\n        url = \"http:\/\/www.google.com\/speech-api\/v2\/recognize?client=chromium&lang=%s&key=%s\" % (self.language, self.key)\n        self.request = Request(url, data = audio_data.data, headers = {\"Content-Type\": \"audio\/x-flac; rate=%s\" % audio_data.rate})\n\n        # check for invalid key response from the server\n        try:\n            response = urlopen(self.request)\n        except URLError:\n            raise IndexError(\"No internet connection available to transfer audio data\")\n        except:\n            raise KeyError(\"Server wouldn't respond (invalid key or quota has been maxed out)\")\n        response_text = response.read().decode(\"utf-8\")\n\n        # ignore any blank blocks\n        actual_result = []\n        for line in response_text.split(\"\\n\"):\n            if not line: continue\n            result = json.loads(line)[\"result\"]\n            if len(result) != 0:\n                actual_result = result[0]\n                break\n\n        # make sure we have a list of transcriptions\n        if \"alternative\" not in actual_result:\n            raise LookupError(\"Speech is unintelligible\")\n\n        # return the best guess unless told to do otherwise\n        if not show_all:\n            for prediction in actual_result[\"alternative\"]:\n                if \"transcript\" in prediction:\n                    return prediction[\"transcript\"]\n            raise LookupError(\"Speech is unintelligible\")\n\n\n        # return all the possibilities\n        spoken_text = []\n        for i, prediction in enumerate(actual_result[\"alternative\"]):\n            if \"transcript\" in prediction:\n                spoken_text.append({\"text\": prediction[\"transcript\"], \"confidence\": 1 if i == 0 else 0})\n        return spoken_text\n\n    def listen_in_background(self, source, callback):\n        \"\"\"\n        Spawns a thread to repeatedly record phrases from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance and call ``callback`` with that ``AudioData`` instance as soon as each phrase are detected.\n\n        Returns a function object that, when called, stops the background listener thread. The background thread is a daemon and will not stop the program from exiting if there are no other non-daemon threads.\n\n        Phrase recognition uses the exact same mechanism as ``recognizer_instance.listen(source)``.\n\n        The ``callback`` parameter is a function that should accept two parameters - the ``recognizer_instance``, and an ``AudioData`` instance representing the captured audio. Note that this function will be called from a non-main thread.\n        \"\"\"\n        assert isinstance(source, AudioSource), \"Source must be an audio source\"\n        import threading\n        running = [True]\n        def threaded_listen():\n            with source as s:\n                while running[0]:\n                    try: # try to detect speech for only one second to do another check if running is enabled\n                        audio = self.listen(s, 1)\n                    except OSError:\n                        pass\n                    else:\n                        if running[0]: callback(self, audio)\n        def stopper():\n            running[0] = False\n        listener_thread = threading.Thread(target=threaded_listen)\n        listener_thread.daemon = True\n        listener_thread.start()\n        return stopper\n\ndef shutil_which(pgm):\n    \"\"\"\n    python2 backport of python3's shutil.which()\n    \"\"\"\n    path = os.getenv('PATH')\n    for p in path.split(os.path.pathsep):\n        p = os.path.join(p, pgm)\n        if os.path.exists(p) and os.access(p, os.X_OK):\n            return p\n","license":"bsd-3-clause","hash":-1909344257993490697,"line_mean":48.2544529262,"line_max":341,"alpha_frac":0.6362556181,"autogenerated":false},
{"repo_name":"colincsl\/pyKinectTools","path":"pyKinectTools\/scripts\/Tracker.py","copies":"1","size":"18262","content":"\"\"\"\nMain file for training multi-camera pose\n\"\"\"\n\n#import os\n#import time\nimport itertools as it\nimport optparse\nimport cPickle as pickle\n\nimport numpy as np\nimport cv2\nimport scipy.misc as sm\nimport scipy.ndimage as nd\nimport Image\nimport skimage\nfrom skimage import color\nfrom skimage.draw import line, circle\nfrom skimage.color import rgb2gray,gray2rgb, rgb2lab\nfrom skimage.feature import hog, local_binary_pattern, match_template, peak_local_max\n\nfrom pyKinectTools.dataset_readers.KinectPlayer import KinectPlayer, display_help\nfrom pyKinectTools.utils.DepthUtils import *\nfrom pyKinectTools.utils.SkeletonUtils import display_skeletons, transform_skels, kinect_to_msr_skel, msr_to_kinect_skel\nfrom pyKinectTools.dataset_readers.MSR_DailyActivities import MSRPlayer\nfrom pyKinectTools.dataset_readers.MHADPlayer import MHADPlayer\nfrom pyKinectTools.dataset_readers.EVALPlayer import EVALPlayer\nfrom pyKinectTools.algs.GeodesicSkeleton import *\nfrom pyKinectTools.algs.HistogramOfOpticalFlow import hog2image\nfrom pyKinectTools.algs.BackgroundSubtraction import fill_image\nfrom pyKinectTools.algs.PoseTracking import *\nfrom pyKinectTools.algs.LocalOccupancyPattern import local_occupancy_pattern\n\nfrom IPython import embed\nnp.seterr(all='ignore')\n\nfrom joblib import Parallel, delayed\n\n\n# -------------------------MAIN------------------------------------------\n\ndef main(visualize=False, learn=False, patch_size=32, n_frames=2500):\n\n\tif 1:\n\t\tget_color = True\n\t\tcam = MHADPlayer(base_dir='\/Users\/colin\/Data\/BerkeleyMHAD\/', kinects=[1], actions=[1], subjects=[1], get_depth=True, get_color=True, get_skeleton=True, fill_images=False)\n\t\t# cam = KinectPlayer(base_dir='.\/', device=2, bg_subtraction=True, get_depth=True, get_color=True, get_skeleton=True, fill_images=False)\n\t\t# cam.bgSubtraction.backgroundModel = sm.imread('\/Users\/colin\/Data\/CIRL_28Feb2013\/depth\/59\/13\/47\/device_1\/depth_59_13_47_4_13_35507.png').clip(0, 4500)\n\t\t# bg = Image.open('\/Users\/colin\/Data\/JHU_RGBD_Pose\/Office_Background_B.tif')\n\t\t# bg = Image.open('\/Users\/colin\/Data\/JHU_RGBD_Pose\/CIRL_Background_B.tif')\n\t\t# bg = Image.open('\/Users\/colin\/Data\/JHU_RGBD_Pose\/Wall_Background_B.tif')\n\t\t# cam.bgSubtraction.backgroundModel = np.array(bg.getdata()).reshape([240,320]).clip(0, 4500)\n\t\t# embed()\n\t\t# cam = KinectPlayer(base_dir='.\/', device=2, bg_subtraction=True, get_depth=True, get_color=True, get_skeleton=True, fill_images=False)\n\telif 0:\n\t\tget_color = False\n\t\tcam = EVALPlayer(base_dir='\/Users\/colin\/Data\/EVAL\/', bg_subtraction=True, get_depth=True, get_skeleton=True, fill_images=False)\n\telif 0:\n\t\tget_color = False\n\t\tcam = MSRPlayer(base_dir='\/Users\/colin\/Data\/MSR_DailyActivities\/Data\/', actions=[1], subjects=[1,2,3,4,5], bg_subtraction=True, get_depth=True, get_color=True, get_skeleton=True, fill_images=False)\n\n\tembed()\n\theight, width = cam.depthIm.shape\n\n\tskel_names = np.array(['head', 'neck', 'torso', 'l_shoulder', 'l_elbow', 'l_hand', \\\n\t\t\t\t'r_shoulder', 'r_elbow', 'r_hand', 'l_hip', 'l_knee', 'l_foot',\\\n\t\t\t\t'r_hip', 'r_knee', 'r_foot'])\n\n\t# skel_init, joint_size, constraint_links, features_joints,convert_to_kinect = get_11_joint_properties()\n\tskel_init, joint_size, constraint_links, features_joints,skel_parts, convert_to_kinect = get_13_joint_properties()\n\t# skel_init, joint_size, constraint_links, features_joints,skel_parts, convert_to_kinect = get_14_joint_properties()\n\t# skel_init, joint_size, constraint_links, features_joints,convert_to_kinect = get_15_joint_properties()\n\tconstraint_values = []\n\tfor c in constraint_links:\n\t\tconstraint_values += [np.linalg.norm(skel_init[c[0]]-skel_init[c[1]], 2)]\n\tconstraint_values = np.array(constraint_values)\n\n\tskel_current = skel_init.copy()\n\tskel_previous = skel_current.copy()\n\n\tface_detector = FaceDetector()\n\thand_template = sm.imread('\/Users\/colin\/Desktop\/fist.png')[:,:,2]\n\thand_template = (255 - hand_template)\/255.\n\tif height == 240:\n\t\thand_template = cv2.resize(hand_template, (10,10))\n\telse:\n\t\thand_template = cv2.resize(hand_template, (20,20))\n\n\tframe_count = 0\n\tif get_color and height==240:\n\t\tcam.next(220)\n\n\taccuracy_measurements = {'overall':[], 'per_joint':[]}\n\n\t# Video writer\n\t# print '1'\n\tvideo_writer = cv2.VideoWriter(\"\/Users\/colin\/Desktop\/test.avi\", cv2.cv.CV_FOURCC('M','J','P','G'), 15, (320,240))\n\t# print '1'\n\n\t# embed()\n\twhile cam.next(1) and frame_count < n_frames:\n\t\tprint \"\"\n\t\tprint \"Frame #{0:d}\".format(frame_count)\n\t\t# Get rid of bad skeletons\n\t\tif type(cam.users)==dict:\n\t\t\tcam_skels = [np.array(cam.users[s]['jointPositions'].values()) for s in cam.users]\n\t\telse:\n\t\t\tcam_skels = [np.array(s) for s in cam.users]\n\t\tcam_skels = [s for s in cam_skels if np.all(s[0] != -1)]\n\n\t\t# Check for skeletons\n\t\t# if len(cam_skels) == 0:\n\t\t\t# continue\n\n\t\t# Apply mask to image\n\t\tmask = cam.get_person() > 0\n\t\t# if mask is False:\n\t\tif 1:\n\t\t\tif len(cam_skels) > 0:\n\t\t\t\t# cam.colorIm = display_skeletons(cam.colorIm[:,:,2], skel_msr_im, (255,), skel_type='Kinect')[:,:,None]\n\t\t\t\tcam.colorIm[:,:,1] = display_skeletons(cam.colorIm[:,:,2], skel2depth(cam_skels[0], cam.depthIm.shape), (255,), skel_type='Kinect')\n\n\t\t\t## Max P=31 for LBPs becuase of datatype\n\t\t\t# tmp = local_binary_pattern(-cam.depthIm, 1, 10)#*(cam.foregroundMask>0)\n\t\t\t# embed()\n\t\t\t# tmp = local_occupancy_pattern(cam.depthIm, 31, 20, px_diff_thresh=100)*(cam.foregroundMask>0)\n\n\t\t\t# cv2.imshow(\"LBP\", np.abs(tmp.astype(np.float))\/float(tmp.max()))\n\t\t\tcam.colorIm = cam.colorIm[:,:,[0,2,1]]\n\t\t\tcam.visualize()\n\t\t\tcontinue\n\n\t\t# Anonomize\n\t\t# c_masked = cam.colorIm*mask[:,:,None]\n\t\t# d_masked = cam.depthIm*mask\n\t\t# c_masked_neg = cam.colorIm*(-mask[:,:,None])\n\n\t\tim_depth =  cam.depthIm\n\t\tif get_color:\n\t\t\tim_color = cam.colorIm\n\t\t\tim_color *= mask[:,:,None]\n\t\t\tim_color = np.ascontiguousarray(im_color)\n\t\t\tim_color = im_color[:,:,[2,1,0]]\n\t\tif len(cam_skels) > 0:\n\t\t\tskel_msr_xyz = cam_skels[0]\n\t\t\tskel_msr_im = skel2depth(cam_skels[0], cam.depthIm.shape)\n\n\t\tbox = nd.find_objects(mask)[0]\n\t\td = 20\n\t\tbox = (slice(np.maximum(box[0].start-d, 0), \\\n\t\t\t\tnp.minimum(box[0].stop+d, height-1)), \\\n\t\t\t   slice(np.maximum(box[1].start-d, 0), \\\n\t\t\t\tnp.minimum(box[1].stop+d, width-1)))\n\n\t\t# Face and skin detection\n\t\tif get_color:\n\t\t\tface_detector.run(im_color[box])\n\t\t\tim_skin = rgb2lab(cam.colorIm[box].astype(np.int16))[:,:,1]\n\t\t\t# im_skin = skimage.exposure.equalize_hist(im_skin)\n\t\t\t# im_skin = skimage.exposure.rescale_intensity(im_skin, out_range=[0,1])\n\t\t\tim_skin *= im_skin > face_detector.min_threshold\n\t\t\tim_skin *= im_skin < face_detector.max_threshold\n\t\t\t# im_skin *= face_detector>.068\n\n\t\t\tskin_match_c = nd.correlate(im_skin, hand_template)\n\n\t\t\t# Display Predictions - Color Based matching\n\t\t\toptima = peak_local_max(skin_match_c, min_distance=20, num_peaks=3, exclude_border=False)\n\t\t\t# Visualize\n\t\t\tif len(optima) > 0:\n\t\t\t\toptima_values = skin_match_c[optima[:,0], optima[:,1]]\n\t\t\t\toptima_thresh = np.max(optima_values) \/ 2\n\t\t\t\toptima = optima.tolist()\n\n\t\t\t\tfor i,o in enumerate(optima):\n\t\t\t\t\tif optima_values[i] < optima_thresh:\n\t\t\t\t\t\toptima.pop(i)\n\t\t\t\t\t\tbreak\n\t\t\t\t\tjoint = np.array(o) + [box[0].start, box[1].start]\n\t\t\t\t\tcirc = np.array(circle(joint[0],joint[1], 5)).T\n\t\t\t\t\tcirc = circ.clip([0,0], [height-1, width-1])\n\t\t\t\t\tcam.colorIm[circ[:,0], circ[:,1]] = (0,120 - 30*i,0)#(255*(i==0),255*(i==1),255*(i==2))\n\t\t\tmarkers = optima\n\n\n\n\t\t# ---------------- Tracking Algorithm ----------------\n\t\t# ---- Preprocessing ----\n\t\tif get_color:\n\t\t\tim_pos = rgbIm2PosIm(cam.depthIm*mask)[box] * mask[box][:,:,None]\n\t\telse:\n\t\t\tim_pos = cam.posIm[box]\n\t\tcam.depthIm[cam.depthIm==4500] = 0\n\t\tim_pos_mean = np.array([\n\t\t\t\t\t\t\tim_pos[:,:,0][im_pos[:,:,2]!=0].mean(),\n\t\t\t\t\t\t\tim_pos[:,:,1][im_pos[:,:,2]!=0].mean(),\n\t\t\t\t\t\t\tim_pos[:,:,2][im_pos[:,:,2]!=0].mean()\n\t\t\t\t\t\t\t], dtype=np.int16)\n\n\t\t# Zero-center\n\t\tif skel_current[0,2] == 0:\n\t\t\tskel_current += im_pos_mean\n\t\t\tskel_previous += im_pos_mean\n\n\t\t# Calculate Geodesic Extrema\n\t\textrema = geodesic_extrema_MPI(im_pos, iterations=15, visualize=False)\n\t\tif len(extrema) > 0:\n\t\t\tfor i,o in enumerate(extrema):\n\t\t\t\tjoint = np.array(o) + [box[0].start, box[1].start]\n\t\t\t\tcirc = np.array(circle(joint[0],joint[1], 5)).T\n\t\t\t\tcirc = circ.clip([0,0], [height-1, width-1])\n\t\t\t\tcam.colorIm[circ[:,0], circ[:,1]] = (0,0,200-10*i)\n\n\t\t# Calculate Z-surface\n\t\tsurface_map = nd.distance_transform_edt(-mask[box], return_distances=False, return_indices=True)\n\n\t\t# Only sample some of the points\n\t\tif 1:\n\t\t\tmask_interval = 1\n\t\t\tfeature_radius = 10\n\t\telse:\n\t\t\tmask_interval = 3\n\t\t\tfeature_radius = 2\n\n\t\t# Modify the box wrt the sampling\n\t\tbox = (slice(box[0].start, box[0].stop, mask_interval),slice(box[1].start, box[1].stop, mask_interval))\n\t\tim_pos_full = im_pos.copy()\n\t\tim_pos = im_pos[::mask_interval,::mask_interval]\n\t\tbox_height, box_width,_ = im_pos.shape\n\n\t\tskel_img_box = world2rgb(skel_current, cam.depthIm.shape) - [box[0].start, box[1].start, 0]\n\t\tskel_img_box = skel_img_box.clip([0,0,0], [im_pos.shape[0]-1, im_pos.shape[1]-1, 9999])\n\n\t\tfeature_width = feature_radius*2+1\n\t\tall_features = [face_detector.face_position, optima, extrema]\n\t\ttotal_feature_count = np.sum([len(f) for f in all_features])\n\n\t\t# Loop through the rest of the constraints\n\t\tfor _ in range(1):\n\n\t\t\t# ---- (Step 1A) Find feature coordespondences ----\n\t\t\tcolor_feature_displacement = feature_joint_displacements(skel_current, im_pos, all_features[1], features_joints[1], distance_thresh=500)\n\t\t\tdepth_feature_displacement = feature_joint_displacements(skel_current, im_pos, all_features[2], features_joints[2], distance_thresh=500)\n\n\t\t\t# Alternative method: use kdtree\n\t\t\t## Calc euclidian distance between each pixel and all joints\n\t\t\tpx_corr = np.zeros([im_pos.shape[0], im_pos.shape[1], len(skel_current)])\n\t\t\tfor i,s in enumerate(skel_current):\n\t\t\t\tpx_corr[:,:,i] = np.sqrt(np.sum((im_pos - s)**2, -1))# \/ joint_size[i]**2\n\n\t\t\t## Handle occlusions by argmax'ing over set of skel parts\n\t\t\t# visible_configurations = list(it.product([0,1], repeat=5))[1:]\n\t\t\tvisible_configurations = [\n\t\t\t\t\t\t\t\t\t\t#[0,1,1,1,1],\n\t\t\t\t\t\t\t\t\t\t#[1,0,0,0,0],\n\t\t\t\t\t\t\t\t\t\t[1,1,1,1,1]\n\t\t\t\t\t\t\t\t\t]\n\t\t\tpx_visibility_label = np.zeros([im_pos.shape[0], im_pos.shape[1], len(visible_configurations)], dtype=np.uint8)\n\t\t\tvisible_scores = np.ones(len(visible_configurations))*np.inf\n\t\t\t# Try each occlusion configuration set\n\t\t\tfor i,v in enumerate(visible_configurations):\n\t\t\t\tvisible_joints = list(it.chain.from_iterable(skel_parts[np.array(v)>0]))\n\t\t\t\tpx_visibility_label[:,:,i] = np.argmin(px_corr[:,:,visible_joints], -1)#.reshape([im_pos.shape[0], im_pos.shape[1]])\n\t\t\t\tvisible_scores[i] = np.min(px_corr[:,:,visible_joints], -1).sum()\n\t\t\t# Choose best occlusion configuration\n\t\t\tocclusion_index = np.argmin(visible_scores)\n\t\t\tocclusion_configuration = visible_configurations[occlusion_index]\n\t\t\tocclusion_set = list(it.chain.from_iterable(skel_parts[np.array(visible_configurations[occlusion_index])>0]))\n\t\t\t# Choose label for pixels based on occlusion configuration\n\t\t\tpx_label = px_visibility_label[:,:,occlusion_index]*mask[box]\n\t\t\tpx_label_flat = px_visibility_label[:,:,occlusion_index][mask[box]].flatten()\n\n\t\t\tvisible_joints = [1 if x in occlusion_set else 0 for x in range(len(skel_current))]\n\n\t\t\t# Project distance to joint's radius\n\t\t\tpx_joint_displacement = skel_current[px_label_flat] - im_pos[mask[box]]\n\t\t\tpx_joint_magnitude = np.sqrt(np.sum(px_joint_displacement**2,-1))\n\t\t\tjoint_mesh_pos = skel_current[px_label_flat] + px_joint_displacement*(joint_size[px_label_flat]\/px_joint_magnitude)[:,None]\n\t\t\tpx_joint_displacement = joint_mesh_pos - im_pos[mask[box]]\n\n\t\t\t # Calc the correspondance change in position for each joint\n\t\t\tcorrespondence_displacement = np.zeros([len(skel_current), 3])\n\t\t\tii = 0\n\t\t\tfor i,_ in enumerate(skel_current):\n\t\t\t\tif i in occlusion_set:\n\t\t\t\t\tlabels = px_label_flat==i\n\t\t\t\t\tcorrespondence_displacement[i] = np.mean(px_joint_displacement[px_label_flat==ii], 0)\n\t\t\t\t\t# correspondence_displacement[ii] = np.sum(px_joint_displacement[px_label_flat==ii], 0)\n\t\t\t\t\tii+=1\n\t\t\tcorrespondence_displacement = np.nan_to_num(correspondence_displacement)\n\n\t\t\t# ---- (Step 2) Update pose state, x ----\n\t\t\tlambda_p = .0\n\t\t\tlambda_c = .3\n\t\t\tlambda_cf = .3\n\t\t\tlambda_df = .0\n\t\t\tskel_prev_difference = (skel_current - skel_previous)\n\t\t\t# embed()\n\t\t\tskel_current = skel_previous \\\n\t\t\t\t\t\t\t+ lambda_p  * skel_prev_difference \\\n\t\t\t\t\t\t\t- lambda_c  * correspondence_displacement\\\n\t\t\t\t\t\t\t- lambda_cf * color_feature_displacement\\\n\t\t\t\t\t\t\t- lambda_df * depth_feature_displacement\n\n\t\t\t# ---- (Step 3) Add constraints ----\n\t\t\t# A: Link lengths \/ geometry\n\t\t\tskel_current = link_length_constraints(skel_current, constraint_links, constraint_values, alpha=.5)\n\t\t\tskel_current = geometry_constraints(skel_current, joint_size, alpha=0.5)\n\t\t\t# skel_current = collision_constraints(skel_current, constraint_links)\n\n\t\t\tskel_img_box = (world2rgb(skel_current, cam.depthIm.shape) - [box[0].start, box[1].start, 0])\/mask_interval\n\t\t\tskel_img_box = skel_img_box.clip([0,0,0], [cam.depthIm.shape[0]-1, cam.depthIm.shape[1]-1, 9999])\n\t\t\t# B: Ray-cast constraints\n\t\t\tskel_current, skel_img_box = ray_cast_constraints(skel_current, skel_img_box, im_pos_full, surface_map, joint_size)\n\n\t\t# # Map back from mask to image\n\t\tskel_img = skel_img_box + [box[0].start, box[1].start, 0]\n\n\t\t# Update for next round\n\t\tskel_previous = skel_current.copy()\n\t\t# skel_previous = skel_init.copy()\n\n\t\t# ---------------------Accuracy --------------------------------------\n\n\t\t# Compute accuracy wrt standard Kinect data\n\t\t# skel_im_error = skel_msr_im[:,[1,0]] - skel_img[[0,2,3,4,5,6,7,8,9,10,11,12,13,14],:2]\n\t\tskel_current_kinect = convert_to_kinect(skel_current)\n\t\ttry:\n\t\t\tskel_msr_im_box = np.array([skel_msr_im[:,1]-box[0].start,skel_msr_im[:,0]-box[1].start]).T.clip([0,0],[box_height-1, box_width-1])\n\t\t\tskel_xyz_error = im_pos[skel_msr_im_box[:,0],skel_msr_im_box[:,1]] - skel_current_kinect#skel_current[[0,2,3,4,5,6,7,8,9,10,11,12],:]\n\t\t\tskel_l2 = np.sqrt(np.sum(skel_xyz_error**2, 1))\n\t\t\t# print skel_l2\n\t\t\tskel_correct = np.nonzero(skel_l2 < 150)[0]\n\t\t\taccuracy_measurements['per_joint'] += [skel_l2]\n\t\t\taccuracy_measurements['overall'] += [len(skel_correct)\/float(len(skel_current_kinect))*100]\n\t\t\tprint \"{0:0.2f}% joints correct\".format(len(skel_correct)\/float(len(skel_current_kinect))*100)\n\t\t\tprint \"Overall accuracy: \", np.mean(accuracy_measurements['overall'])\n\t\texcept:\n\t\t\tpass\n\n\t\tprint \"Visible:\", visible_joints\n\n\t\t# ----------------------Visualization-------------------------------------\n\t\t# l = line(skel_img_box[joint][0], skel_img_box[joint][1], features[feat][0], features[feat][1])\n\t\t# skimage.draw.set_color(cam.colorIm[box], l, (255,255,255))\n\n\t\t# Add circles to image\n\t\tcam.colorIm = np.ascontiguousarray(cam.colorIm)\n\t\tif 0:#get_color:\n\t\t\tcam.colorIm = display_skeletons(cam.colorIm, skel_img[:,[1,0,2]]*np.array(visible_joints)[:,None], (0,255,), skel_type='Other', skel_contraints=constraint_links)\n\t\t\tfor i,s in enumerate(skel_img):\n\t\t\t\t# if i not in skel_correct:\n\t\t\t\tif i not in occlusion_set:\n\t\t\t\t\tc = circle(s[0], s[1], 5)\n\t\t\t\t\tcam.colorIm[c[0], c[1]] = (255,0,0)\n\t\t\t# cam.colorIm = display_skeletons(cam.colorIm, world2rgb(skel_init+im_pos_mean, [240,320])[:,[1,0]], skel_type='Other', skel_contraints=constraint_links)\n\n\t\tif 1:\n\t\t\tif len(face_detector.face_position) > 0:\n\t\t\t\tfor (x, y) in face_detector.face_position:\n\t\t\t\t\tpt1 = (int(y)+box[1].start-15, int(x)+box[0].start-15)\n\t\t\t\t\tpt2 = (pt1[0]+int(15), pt1[1]+int(15))\n\t\t\t\t\tcv2.rectangle(cam.colorIm, pt1, pt2, (255, 0, 0), 3, 8, 0)\n\t\t\tif len(cam_skels) > 0:\n\t\t\t\t# cam.colorIm = display_skeletons(cam.colorIm[:,:,2], skel_msr_im, (255,), skel_type='Kinect')[:,:,None]\n\t\t\t\tcam.colorIm[:,:,1] = display_skeletons(cam.colorIm[:,:,2], skel_msr_im, (255,), skel_type='Kinect')\n\t\t\tcam.visualize()\n\t\t\tcam.depthIm = local_binary_pattern(cam.depthIm*cam.foregroundMask, 50, 10)\n\t\t\tcv2.imshow(\"Depth\", cam.depthIm\/float(cam.depthIm.max()))\n\t\t\t# cam2.visualize()\n\t\t\t# embed()\n\n\t\t# 3D Visualization\n\t\tif 0:\n\t\t\tfrom mayavi import mlab\n\t\t\t# figure = mlab.figure(1, bgcolor=(0,0,0), fgcolor=(1,1,1))\n\t\t\tfigure = mlab.figure(1, bgcolor=(1,1,1))\n\t\t\tfigure.scene.disable_render = True\n\t\t\tmlab.clf()\n\t\t\tmlab.view(azimuth=-45, elevation=45, roll=0, figure=figure)\n\t\t\tmlab.points3d(-skel_current[:,1], -skel_current[:,0], skel_current[:,2], scale_factor=100., color=(.5,.5,.5))\n\t\t\tfor c in constraint_links:\n\t\t\t\tx = np.array([skel_current[c[0]][0], skel_current[c[1]][0]])\n\t\t\t\ty = np.array([skel_current[c[0]][1], skel_current[c[1]][1]])\n\t\t\t\tz = np.array([skel_current[c[0]][2], skel_current[c[1]][2]])\n\t\t\t\tmlab.plot3d(-y,-x,z, tube_radius=25., color=(1,0,0))\n\t\t\tfigure.scene.disable_render = False\n\n\t\t# 3-panel view\n\t\tif 0:\n\t\t\tsubplot(2,2,1)\n\t\t\tscatter(skel_current[:,1], skel_current[:,2]);\n\t\t\tfor i,c in enumerate(constraint_links):\n\t\t\t\tplot([skel_current[c[0],1], skel_current[c[1],1]],[skel_current[c[0],2], skel_current[c[1],2]])\n\t\t\taxis('equal')\n\n\t\t\tsubplot(2,2,3)\n\t\t\tscatter(skel_current[:,1], -skel_current[:,0]);\n\t\t\tfor i,c in enumerate(constraint_links):\n\t\t\t\tplot([skel_current[c[0],1], skel_current[c[1],1]],[-skel_current[c[0],0], -skel_current[c[1],0]])\n\t\t\taxis('equal')\n\n\t\t\tsubplot(2,2,4)\n\t\t\tscatter(skel_current[:,2], -skel_current[:,0]);\n\t\t\tfor i,c in enumerate(constraint_links):\n\t\t\t\tplot([skel_current[c[0],2], skel_current[c[1],2]],[-skel_current[c[0],0], -skel_current[c[1],0]])\n\t\t\taxis('equal')\n\t\t\t# show()\n\n\t\t# ------------------------------------------------------------\n\n\t\tvideo_writer.write(cam.colorIm[:,:,[2,1,0]])\n\t\tframe_count+=1\n\tprint 'Done'\n\nif __name__==\"__main__\":\n\n\tparser = optparse.OptionParser()\n\tparser.add_option('-v', '--visualize', dest='viz', action=\"store_true\", default=False, help='Enable visualization')\n\tparser.add_option('-l', '--learn', dest='learn', action=\"store_true\", default=False, help='Training phase')\n\t(opt, args) = parser.parse_args()\n\n\tmain(visualize=opt.viz, learn=opt.learn)\n\n\n\n# Tracking extra\n# if 0:\n\t# Based on geodesic distance\n\t# skel_img = world2rgb(skel_current, cam.depthIm.shape) - [box[0].start, box[1].start, 0]\n\t# skel_img = skel_img.clip([0,0,0], [mask[box].shape[0]-1, mask[box].shape[1]-1, 999])\n\t# for i,s in enumerate(skel_img):\n\t# \t_, geodesic_map = geodesic_extrema_MPI(cam.depthIm[box]*mask[box], centroid=[s[0],s[1]], iterations=1, visualize=True)\n\t# \tpx_corr[:,:,i] = geodesic_map + (-mask[box])*9999\n\n\n","license":"bsd-2-clause","hash":-3910505569816542872,"line_mean":40.8853211009,"line_max":199,"alpha_frac":0.6580878327,"autogenerated":false},
{"repo_name":"miroi\/rle-diis","path":"cmake\/update.py","copies":"2","size":"19945","content":"#!\/usr\/bin\/env python\n\nimport os\nimport sys\nimport datetime\nimport ast\nimport collections\n\n# we do not use the nicer sys.version_info.major\n# for compatibility with Python < 2.7\nif sys.version_info[0] > 2:\n    from io import StringIO\n    from configparser import ConfigParser\n    import urllib.request\n\n    class URLopener(urllib.request.FancyURLopener):\n        def http_error_default(self, url, fp, errcode, errmsg, headers):\n            sys.stderr.write(\"ERROR: could not fetch %s\\n\" % url)\n            sys.exit(-1)\nelse:\n    from StringIO import StringIO\n    from ConfigParser import ConfigParser\n    import urllib\n\n    class URLopener(urllib.FancyURLopener):\n        def http_error_default(self, url, fp, errcode, errmsg, headers):\n            sys.stderr.write(\"ERROR: could not fetch %s\\n\" % url)\n            sys.exit(-1)\n\n\nAUTOCMAKE_GITHUB_URL = 'https:\/\/github.com\/scisoft\/autocmake'\n\n# ------------------------------------------------------------------------------\n\n\ndef fetch_url(src, dst):\n    \"\"\"\n    Fetch file from URL src and save it to dst.\n    \"\"\"\n    dirname = os.path.dirname(dst)\n    if dirname != '':\n        if not os.path.isdir(dirname):\n            os.makedirs(dirname)\n\n    opener = URLopener()\n    opener.retrieve(src, dst)\n\n# ------------------------------------------------------------------------------\n\n\ndef print_progress_bar(text, done, total, width):\n    \"\"\"\n    Print progress bar.\n    \"\"\"\n    n = int(float(width) * float(done) \/ float(total))\n    sys.stdout.write(\"\\r%s [%s%s] (%i\/%i)\" % (text, '#' * n,\n                                              ' ' * (width - n), done, total))\n    sys.stdout.flush()\n\n# ------------------------------------------------------------------------------\n\n\ndef align_options(options):\n    \"\"\"\n    Indents flags and aligns help texts.\n    \"\"\"\n    l = 0\n    for opt in options:\n        if len(opt[0]) > l:\n            l = len(opt[0])\n    s = []\n    for opt in options:\n        s.append('  %s%s  %s' % (opt[0], ' ' * (l - len(opt[0])), opt[1]))\n    return '\\n'.join(s)\n\n# ------------------------------------------------------------------------------\n\n\ndef gen_cmake_command(config):\n    \"\"\"\n    Generate CMake command.\n    \"\"\"\n    s = []\n\n    s.append(\"\\n\\ndef gen_cmake_command(options, arguments):\")\n    s.append('    \"\"\"')\n    s.append(\"    Generate CMake command based on options and arguments.\")\n    s.append('    \"\"\"')\n    s.append(\"    command = []\")\n\n    # take care of environment variables\n    for section in config.sections():\n        if config.has_option(section, 'export'):\n            for env in config.get(section, 'export').split('\\n'):\n                s.append('    command.append(%s)' % env)\n\n    s.append(\"    command.append('%s' % arguments['--cmake-executable'])\")\n\n    # take care of cmake definitions\n    for section in config.sections():\n        if config.has_option(section, 'define'):\n            for definition in config.get(section, 'define').split('\\n'):\n                s.append('    command.append(%s)' % definition)\n\n    s.append(\"    command.append('-DCMAKE_BUILD_TYPE=%s' % arguments['--type'])\")\n    s.append(\"    command.append('-G \\\"%s\\\"' % arguments['--generator'])\")\n    s.append(\"    if arguments['--cmake-options'] != \\\"''\\\":\")\n    s.append(\"        command.append('%s' % arguments['--cmake-options'])\")\n    s.append(\"    if arguments['--prefix']:\")\n    s.append(\"        command.append('-DCMAKE_INSTALL_PREFIX=\\\"{0}\\\"'.format(arguments['--prefix']))\")\n\n    s.append(\"\\n    return ' '.join(command)\")\n\n    return '\\n'.join(s)\n\n# ------------------------------------------------------------------------------\n\n\ndef autogenerated_notice():\n    start_year = 2015\n    year_range = str(start_year)\n    current_year = datetime.date.today().year\n    if current_year > start_year:\n        year_range += '-%s' % current_year\n    s = []\n    s.append('# This file is autogenerated by Autocmake http:\/\/autocmake.org')\n    s.append('# Copyright (c) %s by Radovan Bast and Jonas Juselius' % year_range)\n    return '\\n'.join(s)\n\n# ------------------------------------------------------------------------------\n\n\ndef gen_setup(config, relative_path, setup_script_name):\n    \"\"\"\n    Generate setup script.\n    \"\"\"\n    s = []\n    s.append('#!\/usr\/bin\/env python')\n    s.append('\\n%s' % autogenerated_notice())\n    s.append('\\nimport os')\n    s.append('import sys')\n\n    s.append(\"\\nsys.path.insert(0, '{0}')\".format(relative_path))\n    s.append(\"sys.path.insert(0, '{0}')\".format(os.path.join(relative_path, 'lib')))\n    s.append(\"sys.path.insert(0, '{0}')\".format(os.path.join(relative_path, 'lib', 'docopt')))\n\n    s.append('import config')\n    s.append('import docopt')\n\n    s.append('\\n\\noptions = \"\"\"')\n    s.append('Usage:')\n    s.append('  .\/{0} [options] [<builddir>]'.format(setup_script_name))\n    s.append('  .\/{0} (-h | --help)'.format(setup_script_name))\n    s.append('\\nOptions:')\n\n    options = []\n    for section in config.sections():\n        if config.has_option(section, 'docopt'):\n            for opt in config.get(section, 'docopt').split('\\n'):\n                first = opt.split()[0].strip()\n                rest = ' '.join(opt.split()[1:]).strip()\n                options.append([first, rest])\n\n    options.append(['--type=<TYPE>', 'Set the CMake build type (debug, release, or relwithdeb) [default: release].'])\n    options.append(['--generator=<STRING>', 'Set the CMake build system generator [default: Unix Makefiles].'])\n    options.append(['--show', 'Show CMake command and exit.'])\n    options.append(['--cmake-executable=<CMAKE_EXECUTABLE>', 'Set the CMake executable [default: cmake].'])\n    options.append(['--cmake-options=<STRING>', \"Define options to CMake [default: ''].\"])\n    options.append(['--prefix=<PATH>', 'Set the install path for make install.'])\n    options.append(['<builddir>', 'Build directory.'])\n    options.append(['-h --help', 'Show this screen.'])\n\n    s.append(align_options(options))\n\n    s.append('\"\"\"')\n\n    s.append(gen_cmake_command(config))\n\n    s.append(\"\\n\")\n    s.append(\"# parse command line args\")\n    s.append(\"try:\")\n    s.append(\"    arguments = docopt.docopt(options, argv=None)\")\n    s.append(\"except docopt.DocoptExit:\")\n    s.append(r\"    sys.stderr.write('ERROR: bad input to %s\\n' % sys.argv[0])\")\n    s.append(\"    sys.stderr.write(options)\")\n    s.append(\"    sys.exit(-1)\")\n    s.append(\"\\n\")\n    s.append(\"# use extensions to validate\/post-process args\")\n    s.append(\"if config.module_exists('extensions'):\")\n    s.append(\"    import extensions\")\n    s.append(\"    arguments = extensions.postprocess_args(sys.argv, arguments)\")\n    s.append(\"\\n\")\n    s.append(\"root_directory = os.path.dirname(os.path.realpath(__file__))\")\n    s.append(\"\\n\")\n    s.append(\"build_path = arguments['<builddir>']\")\n    s.append(\"\\n\")\n    s.append(\"# create cmake command\")\n    s.append(\"cmake_command = '%s %s' % (gen_cmake_command(options, arguments), root_directory)\")\n    s.append(\"\\n\")\n    s.append(\"# run cmake\")\n    s.append(\"config.configure(root_directory, build_path, cmake_command, arguments['--show'])\")\n\n    return s\n\n# ------------------------------------------------------------------------------\n\n\ndef gen_cmakelists(project_name, min_cmake_version, relative_path, modules):\n    \"\"\"\n    Generate CMakeLists.txt.\n    \"\"\"\n    s = []\n\n    s.append(autogenerated_notice())\n\n    s.append('\\n# set minimum cmake version')\n    s.append('cmake_minimum_required(VERSION %s FATAL_ERROR)' % min_cmake_version)\n\n    s.append('\\n# project name')\n    s.append('project(%s)' % project_name)\n\n    s.append('\\n# do not rebuild if rules (compiler flags) change')\n    s.append('set(CMAKE_SKIP_RULE_DEPENDENCY TRUE)')\n\n    s.append('\\n# if CMAKE_BUILD_TYPE undefined, we set it to Debug')\n    s.append('if(NOT CMAKE_BUILD_TYPE)')\n    s.append('    set(CMAKE_BUILD_TYPE \"Debug\")')\n    s.append('endif()')\n\n    if len(modules) > 0:\n        s.append('\\n# directories which hold included cmake modules')\n\n    module_paths = [module.path for module in modules]\n    module_paths.append('downloaded')  # this is done to be able to find fetched modules when testing\n    module_paths = list(set(module_paths))\n    module_paths.sort()  # we do this to always get the same order and to minimize diffs\n    for directory in module_paths:\n        rel_cmake_module_path = os.path.join(relative_path, directory)\n        # on windows cmake corrects this so we have to make it wrong again\n        rel_cmake_module_path = rel_cmake_module_path.replace('\\\\', '\/')\n        s.append('set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${PROJECT_SOURCE_DIR}\/%s)' % rel_cmake_module_path)\n\n    if len(modules) > 0:\n        s.append('\\n# included cmake modules')\n    for module in modules:\n        s.append('include(%s)' % os.path.splitext(module.name)[0])\n\n    return s\n\n# ------------------------------------------------------------------------------\n\n\ndef prepend_or_set(config, section, option, value, defaults):\n    \"\"\"\n    If option is already set, then value is prepended.\n    If option is not set, then it is created and set to value.\n    This is used to prepend options with values which come from the module documentation.\n    \"\"\"\n    if value:\n        if config.has_option(section, option):\n            value += '\\n%s' % config.get(section, option, 0, defaults)\n        config.set(section, option, value)\n    return config\n\n# ------------------------------------------------------------------------------\n\n\ndef fetch_modules(config, relative_path):\n    \"\"\"\n    Assemble modules which will\n    be included in CMakeLists.txt.\n    \"\"\"\n\n    download_directory = 'downloaded'\n    if not os.path.exists(download_directory):\n        os.makedirs(download_directory)\n\n    l = list(filter(lambda x: config.has_option(x, 'source'),\n                    config.sections()))\n    n = len(l)\n\n    modules = []\n    Module = collections.namedtuple('Module', 'path name')\n\n    warnings = []\n\n    if n > 0:  # otherwise division by zero in print_progress_bar\n        i = 0\n        print_progress_bar(text='- assembling modules:', done=0, total=n, width=30)\n        for section in config.sections():\n            if config.has_option(section, 'source'):\n                for src in config.get(section, 'source').split('\\n'):\n                    module_name = os.path.basename(src)\n                    if 'http' in src:\n                        path = download_directory\n                        name = 'autocmake_%s' % module_name\n                        dst = os.path.join(download_directory, 'autocmake_%s' % module_name)\n                        fetch_url(src, dst)\n                        file_name = dst\n                        fetch_dst_directory = download_directory\n                    else:\n                        if os.path.exists(src):\n                            path = os.path.dirname(src)\n                            name = module_name\n                            file_name = src\n                            fetch_dst_directory = path\n                        else:\n                            sys.stderr.write(\"ERROR: %s does not exist\\n\" % src)\n                            sys.exit(-1)\n\n                    if config.has_option(section, 'override'):\n                        defaults = ast.literal_eval(config.get(section, 'override'))\n                    else:\n                        defaults = {}\n\n                    # we infer config from the module documentation\n                    with open(file_name, 'r') as f:\n                        parsed_config = parse_cmake_module(f.read(), defaults)\n                        if parsed_config['warning']:\n                            warnings.append('WARNING from {0}: {1}'.format(module_name, parsed_config['warning']))\n                        config = prepend_or_set(config, section, 'docopt', parsed_config['docopt'], defaults)\n                        config = prepend_or_set(config, section, 'define', parsed_config['define'], defaults)\n                        config = prepend_or_set(config, section, 'export', parsed_config['export'], defaults)\n                        if parsed_config['fetch']:\n                            for src in parsed_config['fetch'].split('\\n'):\n                                dst = os.path.join(fetch_dst_directory, os.path.basename(src))\n                                fetch_url(src, dst)\n\n                    modules.append(Module(path=path, name=name))\n                i += 1\n                print_progress_bar(\n                    text='- assembling modules:',\n                    done=i,\n                    total=n,\n                    width=30\n                )\n            if config.has_option(section, 'fetch'):\n                # when we fetch directly from autocmake.cfg\n                # we download into downloaded\/\n                for src in config.get(section, 'fetch').split('\\n'):\n                    dst = os.path.join(download_directory, os.path.basename(src))\n                    fetch_url(src, dst)\n        print('')\n\n    if warnings != []:\n        print('- %s' % '\\n- '.join(warnings))\n\n    return modules\n\n# ------------------------------------------------------------------------------\n\n\ndef main(argv):\n    \"\"\"\n    Main function.\n    \"\"\"\n    if len(argv) != 2:\n        sys.stderr.write(\"\\nYou can update a project in two steps.\\n\\n\")\n        sys.stderr.write(\"Step 1: Update or create infrastructure files\\n\")\n        sys.stderr.write(\"        which will be needed to configure and build the project:\\n\")\n        sys.stderr.write(\"        $ %s --self\\n\\n\" % argv[0])\n        sys.stderr.write(\"Step 2: Create CMakeLists.txt and setup script in PROJECT_ROOT:\\n\")\n        sys.stderr.write(\"        $ %s <PROJECT_ROOT>\\n\" % argv[0])\n        sys.stderr.write(\"        example:\\n\")\n        sys.stderr.write(\"        $ %s ..\\n\" % argv[0])\n        sys.exit(-1)\n\n    if argv[1] in ['-h', '--help']:\n        print('Usage:')\n        print('  python update.py --self         Update this script and fetch or update infrastructure files under lib\/.')\n        print('  python update.py <builddir>     (Re)generate CMakeLists.txt and setup script and fetch or update CMake modules.')\n        print('  python update.py (-h | --help)  Show this help text.')\n        sys.exit(0)\n\n    if argv[1] == '--self':\n        # update self\n        if not os.path.isfile('autocmake.cfg'):\n            print('- fetching example autocmake.cfg')\n            fetch_url(\n                src='%s\/raw\/master\/example\/autocmake.cfg' % AUTOCMAKE_GITHUB_URL,\n                dst='autocmake.cfg'\n            )\n        if not os.path.isfile('.gitignore'):\n            print('- creating .gitignore')\n            with open('.gitignore', 'w') as f:\n                f.write('*.pyc\\n')\n        print('- fetching lib\/config.py')\n        fetch_url(\n            src='%s\/raw\/master\/lib\/config.py' % AUTOCMAKE_GITHUB_URL,\n            dst='lib\/config.py'\n        )\n        print('- fetching lib\/docopt\/docopt.py')\n        fetch_url(\n            src='%s\/raw\/master\/lib\/docopt\/docopt.py' % AUTOCMAKE_GITHUB_URL,\n            dst='lib\/docopt\/docopt.py'\n        )\n        print('- fetching update.py')\n        fetch_url(\n            src='%s\/raw\/master\/update.py' % AUTOCMAKE_GITHUB_URL,\n            dst='update.py'\n        )\n        sys.exit(0)\n\n    project_root = argv[1]\n    if not os.path.isdir(project_root):\n        sys.stderr.write(\"ERROR: %s is not a directory\\n\" % project_root)\n        sys.exit(-1)\n\n    # read config file\n    print('- parsing autocmake.cfg')\n    config = ConfigParser(dict_type=collections.OrderedDict)\n    config.read('autocmake.cfg')\n\n    if not config.has_option('project', 'name'):\n        sys.stderr.write(\"ERROR: you have to specify the project name\\n\")\n        sys.stderr.write(\"       in autocmake.cfg under [project]\\n\")\n        sys.exit(-1)\n    project_name = config.get('project', 'name')\n    if ' ' in project_name.rstrip():\n        sys.stderr.write(\"ERROR: project name contains a space\\n\")\n        sys.exit(-1)\n\n    if not config.has_option('project', 'min_cmake_version'):\n        sys.stderr.write(\"ERROR: you have to specify the min_cmake_version for CMake\\n\")\n        sys.stderr.write(\"       in autocmake.cfg under [project]\\n\")\n        sys.exit(-1)\n    min_cmake_version = config.get('project', 'min_cmake_version')\n\n    if config.has_option('project', 'setup_script'):\n        setup_script_name = config.get('project', 'setup_script')\n    else:\n        setup_script_name = 'setup'\n\n    # get relative path from setup script to this directory\n    relative_path = os.path.relpath(os.path.abspath('.'), project_root)\n\n    # fetch modules from the web or from relative paths\n    modules = fetch_modules(config, relative_path)\n\n    # create CMakeLists.txt\n    print('- generating CMakeLists.txt')\n    s = gen_cmakelists(project_name, min_cmake_version, relative_path, modules)\n    with open(os.path.join(project_root, 'CMakeLists.txt'), 'w') as f:\n        f.write('%s\\n' % '\\n'.join(s))\n\n    # create setup script\n    print('- generating setup script')\n    s = gen_setup(config, relative_path, setup_script_name)\n    file_path = os.path.join(project_root, setup_script_name)\n    with open(file_path, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(s))\n    if sys.platform != 'win32':\n        make_executable(file_path)\n\n# ------------------------------------------------------------------------------\n\n\n# http:\/\/stackoverflow.com\/a\/30463972\ndef make_executable(path):\n    mode = os.stat(path).st_mode\n    mode |= (mode & 0o444) >> 2    # copy R bits to X\n    os.chmod(path, mode)\n\n# ------------------------------------------------------------------------------\n\n\ndef parse_cmake_module(s_in, defaults={}):\n\n    parsed_config = collections.defaultdict(lambda: None)\n\n    if 'autocmake.cfg configuration::' not in s_in:\n        return parsed_config\n\n    s_out = []\n    is_rst_line = False\n    for line in s_in.split('\\n'):\n        if is_rst_line:\n            if len(line) > 0:\n                if line[0] != '#':\n                    is_rst_line = False\n            else:\n                is_rst_line = False\n        if is_rst_line:\n            s_out.append(line[2:])\n        if '#.rst:' in line:\n            is_rst_line = True\n\n    autocmake_entry = '\\n'.join(s_out).split('autocmake.cfg configuration::')[1]\n    autocmake_entry = autocmake_entry.replace('\\n  ', '\\n')\n\n    # we prepend a fake section heading so that we can parse it with configparser\n    autocmake_entry = '[foo]\\n' + autocmake_entry\n\n    buf = StringIO(autocmake_entry)\n    config = ConfigParser(dict_type=collections.OrderedDict)\n    config.readfp(buf)\n\n    for section in config.sections():\n        for s in ['docopt', 'define', 'export', 'fetch', 'warning']:\n            if config.has_option(section, s):\n                parsed_config[s] = config.get(section, s, 0, defaults)\n\n    return parsed_config\n\n# ------------------------------------------------------------------------------\n\n\ndef test_parse_cmake_module():\n\n    s = '''#.rst:\n#\n# Foo ...\n#\n# autocmake.cfg configuration::\n#\n#   docopt: --cxx=<CXX> C++ compiler [default: g++].\n#           --extra-cxx-flags=<EXTRA_CXXFLAGS> Extra C++ compiler flags [default: ''].\n#   export: 'CXX=%s' % arguments['--cxx']\n#   define: '-DEXTRA_CXXFLAGS=\"%s\"' % arguments['--extra-cxx-flags']\n\nenable_language(CXX)\n\nif(NOT DEFINED CMAKE_C_COMPILER_ID)\n    message(FATAL_ERROR \"CMAKE_C_COMPILER_ID variable is not defined!\")\nendif()'''\n\n    parsed_config = parse_cmake_module(s)\n    assert parsed_config['docopt'] == \"--cxx=<CXX> C++ compiler [default: g++].\\n--extra-cxx-flags=<EXTRA_CXXFLAGS> Extra C++ compiler flags [default: ''].\"\n\n    s = '''#.rst:\n#\n# Foo ...\n#\n# Bar ...\n\nenable_language(CXX)\n\nif(NOT DEFINED CMAKE_C_COMPILER_ID)\n    message(FATAL_ERROR \"CMAKE_C_COMPILER_ID variable is not defined!\")\nendif()'''\n\n    parsed_config = parse_cmake_module(s)\n    assert parsed_config['docopt'] is None\n\n# ------------------------------------------------------------------------------\n\n\nif __name__ == '__main__':\n    main(sys.argv)\n","license":"mit","hash":-6121790036608266661,"line_mean":35.3296903461,"line_max":156,"alpha_frac":0.5430935071,"autogenerated":false},
{"repo_name":"kimvais\/nimismies","path":"nimismies\/__init__.py","copies":"1","size":"1159","content":"#\n# -*- coding: utf-8 -*-\n#\n# Copyright \u00a9 2013 Kimmo Parviainen-Jalanko <k@77.fi>\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\nversion = '0.0.1'\n","license":"mit","hash":2133824050675406705,"line_mean":47.25,"line_max":79,"alpha_frac":0.7651122625,"autogenerated":false},
{"repo_name":"realworldtech\/radius_restserver","path":"src\/freeradius\/admin.py","copies":"2","size":"2580","content":"from django.contrib import admin\nfrom freeradius.models import *\n\nclass BadUsersAdmin(admin.ModelAdmin):\n    model = BadUsers\n\nclass NASAdmin(admin.ModelAdmin):\n    model = NAS\n\nclass RadAcctAdmin(admin.ModelAdmin):\n    model = RadAcct\n    list_display = ('username', 'groupname', 'realm', 'nasipaddress', 'nasportid', 'acctstarttime',\n    \t'acctstoptime', 'acctsessiontime', 'acctinputoctets', 'acctoutputoctets', 'acctterminatecause')\n\nclass RadCheckAdmin(admin.ModelAdmin):\n    model = RadCheck\n    search_fields = ['username__username']\n    list_display = ('username', 'attribute', 'op', 'value')\n\nclass RadGroupCheckAdmin(admin.ModelAdmin):\n    model = RadGroupCheck\n\nclass RadGroupReplyAdmin(admin.ModelAdmin):\n    model = RadGroupReply\n\nclass RadIPPoolAdmin(admin.ModelAdmin):\n    model = RadIPPool\n\nclass RadPostAuthAdmin(admin.ModelAdmin):\n    model = RadPostAuth\n\nclass RadReplyAdmin(admin.ModelAdmin):\n    model = RadReply\n\nclass RadUserGroupAdmin(admin.ModelAdmin):\n    model = RadUserGroup\n\nclass TotAcctAdmin(admin.ModelAdmin):\n    model = TotAcct\n\nclass UserBillingDetailAdmin(admin.ModelAdmin):\n    model = UserBillingDetail\n    list_display = ('username', 'anniversary_day', 'action', 'status')\n\nclass UserDataAdmin(admin.ModelAdmin):\n    model = UserData\n    list_display = ('username', 'date', 'data_hour', 'tdata')\n\n    def tdata(self, obj):\n    \treturn \"%sGB\" % (obj.totaldata\/1024\/1024\/1024);\n\nclass UserQuotaAdmin(admin.ModelAdmin):\n    model = UserQuota\n    list_display = ('username', 'quota_date', 'quota')\n\nclass UserStatsAdmin(admin.ModelAdmin):\n    model = UserStats\n    list_display = ('username', 'acctsessionid', 'timestamp', 'acctinputoctets', 'acctoutputoctets')\n\nclass UserInfoAdmin(admin.ModelAdmin):\n    model = UserInfo\n    search_fields = ['username', 'name']\n\n\nadmin.site.register(BadUsers, BadUsersAdmin)\n\nadmin.site.register(NAS, NASAdmin)\n\nadmin.site.register(RadAcct, RadAcctAdmin)\n\nadmin.site.register(RadCheck, RadCheckAdmin)\n\nadmin.site.register(RadGroupCheck, RadGroupCheckAdmin)\n\nadmin.site.register(RadGroupReply, RadGroupReplyAdmin)\n\nadmin.site.register(RadIPPool, RadIPPoolAdmin)\n\nadmin.site.register(RadPostAuth, RadPostAuthAdmin)\n\nadmin.site.register(RadReply, RadReplyAdmin)\n\nadmin.site.register(RadUserGroup, RadUserGroupAdmin)\n\nadmin.site.register(TotAcct, TotAcctAdmin)\n\nadmin.site.register(UserBillingDetail, UserBillingDetailAdmin)\n\nadmin.site.register(UserData, UserDataAdmin)\n\nadmin.site.register(UserQuota, UserQuotaAdmin)\n\nadmin.site.register(UserStats, UserStatsAdmin)\n\nadmin.site.register(UserInfo, UserInfoAdmin)\n","license":"mit","hash":-8105512373409133960,"line_mean":26.1578947368,"line_max":100,"alpha_frac":0.7558139535,"autogenerated":false},
{"repo_name":"plotly\/python-api","path":"packages\/python\/plotly\/plotly\/validators\/waterfall\/__init__.py","copies":"1","size":"5643","content":"import sys\n\nif sys.version_info < (3, 7):\n    from ._ysrc import YsrcValidator\n    from ._yaxis import YaxisValidator\n    from ._y0 import Y0Validator\n    from ._y import YValidator\n    from ._xsrc import XsrcValidator\n    from ._xaxis import XaxisValidator\n    from ._x0 import X0Validator\n    from ._x import XValidator\n    from ._widthsrc import WidthsrcValidator\n    from ._width import WidthValidator\n    from ._visible import VisibleValidator\n    from ._uirevision import UirevisionValidator\n    from ._uid import UidValidator\n    from ._totals import TotalsValidator\n    from ._texttemplatesrc import TexttemplatesrcValidator\n    from ._texttemplate import TexttemplateValidator\n    from ._textsrc import TextsrcValidator\n    from ._textpositionsrc import TextpositionsrcValidator\n    from ._textposition import TextpositionValidator\n    from ._textinfo import TextinfoValidator\n    from ._textfont import TextfontValidator\n    from ._textangle import TextangleValidator\n    from ._text import TextValidator\n    from ._stream import StreamValidator\n    from ._showlegend import ShowlegendValidator\n    from ._selectedpoints import SelectedpointsValidator\n    from ._outsidetextfont import OutsidetextfontValidator\n    from ._orientation import OrientationValidator\n    from ._opacity import OpacityValidator\n    from ._offsetsrc import OffsetsrcValidator\n    from ._offsetgroup import OffsetgroupValidator\n    from ._offset import OffsetValidator\n    from ._name import NameValidator\n    from ._metasrc import MetasrcValidator\n    from ._meta import MetaValidator\n    from ._measuresrc import MeasuresrcValidator\n    from ._measure import MeasureValidator\n    from ._legendgroup import LegendgroupValidator\n    from ._insidetextfont import InsidetextfontValidator\n    from ._insidetextanchor import InsidetextanchorValidator\n    from ._increasing import IncreasingValidator\n    from ._idssrc import IdssrcValidator\n    from ._ids import IdsValidator\n    from ._hovertextsrc import HovertextsrcValidator\n    from ._hovertext import HovertextValidator\n    from ._hovertemplatesrc import HovertemplatesrcValidator\n    from ._hovertemplate import HovertemplateValidator\n    from ._hoverlabel import HoverlabelValidator\n    from ._hoverinfosrc import HoverinfosrcValidator\n    from ._hoverinfo import HoverinfoValidator\n    from ._dy import DyValidator\n    from ._dx import DxValidator\n    from ._decreasing import DecreasingValidator\n    from ._customdatasrc import CustomdatasrcValidator\n    from ._customdata import CustomdataValidator\n    from ._constraintext import ConstraintextValidator\n    from ._connector import ConnectorValidator\n    from ._cliponaxis import CliponaxisValidator\n    from ._base import BaseValidator\n    from ._alignmentgroup import AlignmentgroupValidator\nelse:\n    from _plotly_utils.importers import relative_import\n\n    __all__, __getattr__, __dir__ = relative_import(\n        __name__,\n        [],\n        [\n            \"._ysrc.YsrcValidator\",\n            \"._yaxis.YaxisValidator\",\n            \"._y0.Y0Validator\",\n            \"._y.YValidator\",\n            \"._xsrc.XsrcValidator\",\n            \"._xaxis.XaxisValidator\",\n            \"._x0.X0Validator\",\n            \"._x.XValidator\",\n            \"._widthsrc.WidthsrcValidator\",\n            \"._width.WidthValidator\",\n            \"._visible.VisibleValidator\",\n            \"._uirevision.UirevisionValidator\",\n            \"._uid.UidValidator\",\n            \"._totals.TotalsValidator\",\n            \"._texttemplatesrc.TexttemplatesrcValidator\",\n            \"._texttemplate.TexttemplateValidator\",\n            \"._textsrc.TextsrcValidator\",\n            \"._textpositionsrc.TextpositionsrcValidator\",\n            \"._textposition.TextpositionValidator\",\n            \"._textinfo.TextinfoValidator\",\n            \"._textfont.TextfontValidator\",\n            \"._textangle.TextangleValidator\",\n            \"._text.TextValidator\",\n            \"._stream.StreamValidator\",\n            \"._showlegend.ShowlegendValidator\",\n            \"._selectedpoints.SelectedpointsValidator\",\n            \"._outsidetextfont.OutsidetextfontValidator\",\n            \"._orientation.OrientationValidator\",\n            \"._opacity.OpacityValidator\",\n            \"._offsetsrc.OffsetsrcValidator\",\n            \"._offsetgroup.OffsetgroupValidator\",\n            \"._offset.OffsetValidator\",\n            \"._name.NameValidator\",\n            \"._metasrc.MetasrcValidator\",\n            \"._meta.MetaValidator\",\n            \"._measuresrc.MeasuresrcValidator\",\n            \"._measure.MeasureValidator\",\n            \"._legendgroup.LegendgroupValidator\",\n            \"._insidetextfont.InsidetextfontValidator\",\n            \"._insidetextanchor.InsidetextanchorValidator\",\n            \"._increasing.IncreasingValidator\",\n            \"._idssrc.IdssrcValidator\",\n            \"._ids.IdsValidator\",\n            \"._hovertextsrc.HovertextsrcValidator\",\n            \"._hovertext.HovertextValidator\",\n            \"._hovertemplatesrc.HovertemplatesrcValidator\",\n            \"._hovertemplate.HovertemplateValidator\",\n            \"._hoverlabel.HoverlabelValidator\",\n            \"._hoverinfosrc.HoverinfosrcValidator\",\n            \"._hoverinfo.HoverinfoValidator\",\n            \"._dy.DyValidator\",\n            \"._dx.DxValidator\",\n            \"._decreasing.DecreasingValidator\",\n            \"._customdatasrc.CustomdatasrcValidator\",\n            \"._customdata.CustomdataValidator\",\n            \"._constraintext.ConstraintextValidator\",\n            \"._connector.ConnectorValidator\",\n            \"._cliponaxis.CliponaxisValidator\",\n            \"._base.BaseValidator\",\n            \"._alignmentgroup.AlignmentgroupValidator\",\n        ],\n    )\n","license":"mit","hash":4269850729304457096,"line_mean":41.75,"line_max":60,"alpha_frac":0.6723374092,"autogenerated":false},
{"repo_name":"RoboCupULaval\/StrategyIA","path":"tests\/Util\/test_timing.py","copies":"1","size":"1621","content":"import time\nimport pytest\nfrom Util import timing\n\n\ndef test_near_full_load_fps_timer():\n    # example preliminary test\n    start = time.time()\n    time_total = 2\n    fps = 10\n    num_iterations = time_total*fps\n    sleep = timing.create_fps_timer(fps)\n    sleep()  # to initialize it\n    for _ in range(num_iterations-1):  # -1 because of the first sleep\n        time.sleep(1\/(fps+0.05))  # simulate load very close to full load\n        sleep()\n    end_time = time.time()-start\n    assert end_time == pytest.approx(time_total, 0.1)\n\n\ndef test_no_load_fps_timer():\n    # example preliminary test\n    start = time.time()\n    time_total = 2\n    fps = 10\n    num_iterations = time_total*fps\n    sleep = timing.create_fps_timer(fps)\n    sleep()  # to initialize it\n    for _ in range(num_iterations-1):  # -1 because of the first sleep\n        sleep()\n    end_time = time.time()-start\n    assert end_time == pytest.approx(time_total, 0.1)\n\ndef test_callback_called_fps_timer():\n    fps = 10\n    callback_called = False\n\n    def mock_callback(time_var):  # pylint: disable=unused-argument\n        nonlocal callback_called\n        callback_called = True\n    sleep = timing.create_fps_timer(fps, on_miss_callback=mock_callback)\n    sleep()\n    time.sleep(0.2)\n    sleep()\n    assert callback_called is True\n\ndef test_callback_delay_fps_timer():\n    fps = 10\n    callback_time = None\n\n    def mock_callback(time_var):\n        nonlocal callback_time\n        callback_time = time_var\n    sleep = timing.create_fps_timer(fps, on_miss_callback=mock_callback)\n    sleep()\n    time.sleep(0.2)\n    sleep()\n    assert callback_time >= 0\n","license":"mit","hash":-2674908889365512712,"line_mean":26.9482758621,"line_max":73,"alpha_frac":0.6440468846,"autogenerated":false},
{"repo_name":"tensorflow\/model-optimization","path":"tensorflow_model_optimization\/python\/core\/quantization\/keras\/default_8bit\/default_8bit_quantize_layout_transform.py","copies":"1","size":"3184","content":"# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Default 8-bit layout transformation for quantization.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize_layout_transform\nfrom tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_transforms\nfrom tensorflow_model_optimization.python.core.quantization.keras.graph_transformations import model_transformer\n\nkeras = tf.keras\n\n\nclass Default8BitQuantizeLayoutTransform(\n    quantize_layout_transform.QuantizeLayoutTransform):\n  \"\"\"Default model transformations.\"\"\"\n\n  def apply(self, model, layer_quantize_map):\n    \"\"\"Implement default 8-bit transforms.\n\n    Currently this means the following.\n      1. Pull activations into layers, and apply fuse activations. (TODO)\n      2. Modify range in incoming layers for Concat. (TODO)\n      3. Fuse Conv2D\/DepthwiseConv2D + BN into single layer.\n\n    Args:\n      model: Keras model to be quantized.\n      layer_quantize_map: Map with keys as layer names, and values as dicts\n        containing custom `QuantizeConfig`s which may have been passed with\n        layers.\n\n    Returns:\n      (Transformed Keras model to better match TensorFlow Lite backend, updated\n      layer quantize map.)\n    \"\"\"\n\n    transforms = [\n        default_8bit_transforms.InputLayerQuantize(),\n        default_8bit_transforms.SeparableConv1DQuantize(),\n        default_8bit_transforms.SeparableConvQuantize(),\n        default_8bit_transforms.Conv2DReshapeBatchNormReLUQuantize(),\n        default_8bit_transforms.Conv2DReshapeBatchNormActivationQuantize(),\n        default_8bit_transforms.Conv2DBatchNormReLUQuantize(),\n        default_8bit_transforms.Conv2DBatchNormActivationQuantize(),\n        default_8bit_transforms.Conv2DReshapeBatchNormQuantize(),\n        default_8bit_transforms.Conv2DBatchNormQuantize(),\n        default_8bit_transforms.ConcatTransform6Inputs(),\n        default_8bit_transforms.ConcatTransform5Inputs(),\n        default_8bit_transforms.ConcatTransform4Inputs(),\n        default_8bit_transforms.ConcatTransform3Inputs(),\n        default_8bit_transforms.ConcatTransform(),\n        default_8bit_transforms.LayerReLUQuantize(),\n        default_8bit_transforms.LayerReluActivationQuantize(),\n    ]\n    return model_transformer.ModelTransformer(\n        model, transforms,\n        set(layer_quantize_map.keys()), layer_quantize_map).transform()\n","license":"apache-2.0","hash":7139579444714733050,"line_mean":42.6164383562,"line_max":112,"alpha_frac":0.7317839196,"autogenerated":false},
{"repo_name":"igatoolsProject\/igatools","path":"source\/geometry\/grid.inst.py","copies":"1","size":"2551","content":"#-+--------------------------------------------------------------------\n# Igatools a general purpose Isogeometric analysis library.\n# Copyright (C) 2012-2016  by the igatools authors (see authors.txt).\n#\n# This file is part of the igatools library.\n#\n# The igatools library is free software: you can use it, redistribute\n# it and\/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, either\n# version 3 of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#-+--------------------------------------------------------------------\n\nfrom init_instantiation_data import *\n\ninclude_files = ['geometry\/grid_element.h']\ndata = Instantiation(include_files)\n(f, inst) = (data.file_output, data.inst)\n\nsub_dim_members = ['Grid<dim>::template BoundaryNormals<k> ' +\n                   'Grid<dim>::get_boundary_normals<k>(const int s_id) const;',\n                   'std::shared_ptr<Grid<k>> Grid<dim>::'+\n                   'get_sub_grid<k>(const int sub_elem_id, typename Grid<dim>::template SubGridMap<k> &elem_map) const;']\n\ngrids = [] \nfor dim in inst.sub_domain_dims:\n    grid = 'Grid<%d>' %(dim)\n    grids.append(grid)\n    f.write('template class %s; \\n' % (grid))\n    for fun in sub_dim_members:\n        for k in range(0,max(dim-1,0)+1):\n#        k = dim\n            s = fun.replace('k', '%d' % (k)).replace('dim', '%d' % (dim));\n            f.write('template ' + s + '\\n')  \n    \nfor dim in inst.domain_dims:\n    grid = 'Grid<%d>' %(dim)   \n    grids.append(grid)\n    f.write('template class %s; \\n' % (grid))\n    for fun in sub_dim_members:\n        for k in range(0,max(dim-1,0)+1):\n#        for k in inst.sub_dims(dim):\n            s = fun.replace('k', '%d' % (k)).replace('dim', '%d' % (dim));\n            f.write('template ' + s + '\\n')\n       \n\n\n\n#---------------------------------------------------\nf.write('#ifdef IGATOOLS_WITH_SERIALIZATION\\n')\narchives = ['OArchive','IArchive']\n\nfor grid in unique(grids):\n    for ar in archives:\n        f.write('template void %s::serialize(%s&);\\n' %(grid,ar))\nf.write('#endif \/\/ IGATOOLS_WITH_SERIALIZATION\\n')\n#---------------------------------------------------\n","license":"gpl-3.0","hash":-6941081620431806906,"line_mean":38.859375,"line_max":121,"alpha_frac":0.5711485692,"autogenerated":false},
{"repo_name":"thundernet8\/Plog","path":"app\/core\/main\/views.py","copies":"1","size":"4256","content":"# coding=utf-8\n\nfrom datetime import datetime\nimport json\n\nfrom flask import current_app\nfrom flask import send_file\nfrom flask import request\nfrom flask import render_template\nfrom flask import session\nfrom flask import redirect\nfrom flask import abort\nfrom flask.ext.login import current_user\n\nfrom . import main\nfrom app.core.models.helpers.redis_cache_decorator import redis_cached\nfrom app.core.models.settings import Setting\nfrom app.core.models.posts import Post\nfrom app.core.models.posts import Posts\nfrom app.core.models.users import User\nfrom app.core.models.tags import Tag\nfrom .forms import CommentForm\n\n\n@main.route('\/favicon.ico')\ndef favicon():\n    \"\"\"\n    \u6536\u85cf\u5939\u680f\u56fe\u6807\n    :return:\n    \"\"\"\n    return send_file('static\/dist\/images\/favicon.ico', as_attachment=False)\n\n\n@main.route('\/kill-ie.html')\ndef kill_ie():\n    \"\"\"\n    kill ie\n    :return:\n    \"\"\"\n    return render_template('utils\/kill-ie.html', blog_name=Setting.get_setting('blog_name', 'Plog'))\n\n\n# \u641c\u7d22\n@main.route('\/search', methods=['GET', 'POST'])\ndef search():\n    if request.method == 'POST':\n        s = request.form.get('s')\n        return redirect('\/search?s='+s)\n    else:\n        s = request.args.get('s')\n    return s  # TODO search\n\n\n# \u9996\u9875\n@main.route('\/')\n@redis_cached(timeout=30, key_prefix='home_html')  # TODO \u7f13\u5b58\u65f6\u95f4\ndef index():\n    pagenation = Posts(filters={'status': 'published'}).pagination()\n    posts = pagenation.items if pagenation else []\n    return render_template('home.html', posts=posts, pagenation=pagenation)\n\n\n# \u9996\u9875(\u5e26\u5206\u9875)\n@main.route('\/page\/<int:page>')\n@redis_cached(timeout=30, key_prefix='home_html_%s')\ndef index_paged(page):\n    pagenation = Posts(filters={'status': 'published'}).pagination(page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('home.html', posts=posts, pagenation=pagenation)\n\n\n# \u6587\u7ae0\u8be6\u60c5\u9875\n@main.route('\/article\/<int:post_id>.html')\n@redis_cached(timeout=30, key_prefix='article_%s')\ndef article_detail(post_id):\n    post = Post.get_post(post_id)\n    if not post or not post.post_id or post.status != 'published':\n        abort(404)\n    author = User(user_id=post.author_id)\n    comment_form = CommentForm()\n    return render_template('article.html', post=post, author=author, comment_form=comment_form)\n\n\n# \u7528\u6237\/\u4f5c\u8005\u4e3b\u9875\n@main.route('\/author\/<int:user_id>')\n@redis_cached(timeout=300, key_prefix='author_%s')\ndef user_homepage(user_id):\n    author = User.get_user_by_id(user_id)\n    if not author:\n        abort(404)\n    pagenation = User.get_user_posts_pagenation(user_id)\n    posts = pagenation.items if pagenation else []\n    return render_template('author.html', author=author, posts=posts, pagenation=pagenation)  # TODO \/\u8003\u8651\u4f7f\u7528\u7528\u6237\u540d\u6216\u6635\u79f0\u66ff\u4ee3\u7528\u6237 id \u4f5c\u4e3a\u94fe\u63a5\u6807\u8bc6\n\n\n# \u7528\u6237\/\u4f5c\u8005\u4e3b\u9875(\u5e26\u5206\u9875)\n@main.route('\/author\/<int:user_id>\/page\/<int:page>')\n@redis_cached(timeout=300, key_prefix='author_%s')\ndef user_homepage_paged(user_id, page):\n    author = User.get_user_by_id(user_id)\n    if not author:\n        abort(404)\n    pagenation = User.get_user_posts_pagenation(user_id, page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('author.html', author=author, posts=posts, pagenation=pagenation)\n\n\n# RSS\n@main.route('\/rss')\n@redis_cached(timeout=600, key_prefix='rss')\ndef rss():\n    return 'rss'  # TODO rss\n\n\n# TAG\n@main.route('\/tag\/<int:tag_id>')\n@redis_cached(timeout=600, key_prefix='tag_%s')\ndef tag(tag_id):\n    tag = Tag.get_tag_by_id(tag_id)\n    if not tag:\n        abort(404)\n    pagenation = Tag.get_tag_posts(tag_id)\n    posts = pagenation.items if pagenation else []\n    return render_template('tag.html', tag=tag, posts=posts, pagenation=pagenation)\n\n\n# TAG(\u5e26\u5206\u9875)\n@main.route('\/tag\/<int:tag_id>\/page\/<int:page>')\n@redis_cached(timeout=600, key_prefix='tag_%s')\ndef tag_paged(tag_id, page):\n    tag = Tag.get_tag_by_id(tag_id)\n    if not tag:\n        abort(404)\n    pagenation = Tag.get_tag_posts(tag_id, page=page)\n    posts = pagenation.items if pagenation else []\n    return render_template('tag.html', tag=tag, posts=posts, pagenation=pagenation)\n\n\n# 404\n@main.errorhandler(404)\ndef main_404(e):\n    return render_template('error_pages\/404.html'), 404\n\n\n\n\n\n","license":"gpl-3.0","hash":2699200856079543198,"line_mean":27.1088435374,"line_max":126,"alpha_frac":0.6878025169,"autogenerated":false},
{"repo_name":"lmazuel\/azure-sdk-for-python","path":"azure-mgmt-datalake-analytics\/azure\/mgmt\/datalake\/analytics\/account\/models\/compute_policy.py","copies":"1","size":"2664","content":"# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom .sub_resource import SubResource\n\n\nclass ComputePolicy(SubResource):\n    \"\"\"Data Lake Analytics compute policy information.\n\n    Variables are only populated by the server, and will be ignored when\n    sending a request.\n\n    :ivar id: The resource identifier.\n    :vartype id: str\n    :ivar name: The resource name.\n    :vartype name: str\n    :ivar type: The resource type.\n    :vartype type: str\n    :ivar object_id: The AAD object identifier for the entity to create a\n     policy for.\n    :vartype object_id: str\n    :ivar object_type: The type of AAD object the object identifier refers to.\n     Possible values include: 'User', 'Group', 'ServicePrincipal'\n    :vartype object_type: str or\n     ~azure.mgmt.datalake.analytics.account.models.AADObjectType\n    :ivar max_degree_of_parallelism_per_job: The maximum degree of parallelism\n     per job this user can use to submit jobs.\n    :vartype max_degree_of_parallelism_per_job: int\n    :ivar min_priority_per_job: The minimum priority per job this user can use\n     to submit jobs.\n    :vartype min_priority_per_job: int\n    \"\"\"\n\n    _validation = {\n        'id': {'readonly': True},\n        'name': {'readonly': True},\n        'type': {'readonly': True},\n        'object_id': {'readonly': True},\n        'object_type': {'readonly': True},\n        'max_degree_of_parallelism_per_job': {'readonly': True, 'minimum': 1},\n        'min_priority_per_job': {'readonly': True, 'minimum': 1},\n    }\n\n    _attribute_map = {\n        'id': {'key': 'id', 'type': 'str'},\n        'name': {'key': 'name', 'type': 'str'},\n        'type': {'key': 'type', 'type': 'str'},\n        'object_id': {'key': 'properties.objectId', 'type': 'str'},\n        'object_type': {'key': 'properties.objectType', 'type': 'str'},\n        'max_degree_of_parallelism_per_job': {'key': 'properties.maxDegreeOfParallelismPerJob', 'type': 'int'},\n        'min_priority_per_job': {'key': 'properties.minPriorityPerJob', 'type': 'int'},\n    }\n\n    def __init__(self):\n        super(ComputePolicy, self).__init__()\n        self.object_id = None\n        self.object_type = None\n        self.max_degree_of_parallelism_per_job = None\n        self.min_priority_per_job = None\n","license":"mit","hash":5015207261158012778,"line_mean":38.7611940299,"line_max":111,"alpha_frac":0.6024774775,"autogenerated":false},
{"repo_name":"tobetter\/linaro-image-tools_packaging","path":"linaro_image_tools\/utils.py","copies":"1","size":"13217","content":"# Copyright (C) 2010, 2011 Linaro\n#\n# Author: Guilherme Salgado <guilherme.salgado@linaro.org>\n#\n# This file is part of Linaro Image Tools.\n#\n# Linaro Image Tools is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Linaro Image Tools is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Linaro Image Tools.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport os\nimport platform\nimport subprocess\nimport re\nimport logging\nimport tempfile\nimport tarfile\nimport sys\n\nfrom linaro_image_tools import cmd_runner\n\nDEFAULT_LOGGER_NAME = 'linaro_image_tools'\n\n# The boot path in the boot tarball.\nBOOT_DIR_IN_TARBALL = \"boot\"\n# The name of the hwpack file found in the boot tarball.\nHWPACK_NAME = \"config\"\n\n\n# try_import was copied from python-testtools 0.9.12 and was originally\n# licensed under a MIT-style license but relicensed under the GPL in Linaro\n# Image Tools.\n# Copyright (c) 2011 Jonathan M. Lange <jml@mumak.net>.\ndef try_import(name, alternative=None, error_callback=None):\n    \"\"\"Attempt to import ``name``.  If it fails, return ``alternative``.\n\n    When supporting multiple versions of Python or optional dependencies, it\n    is useful to be able to try to import a module.\n\n    :param name: The name of the object to import, e.g. ``os.path`` or\n        ``os.path.join``.\n    :param alternative: The value to return if no module can be imported.\n        Defaults to None.\n    :param error_callback: If non-None, a callable that is passed the\n        ImportError when the module cannot be loaded.\n    \"\"\"\n    module_segments = name.split('.')\n    last_error = None\n    while module_segments:\n        module_name = '.'.join(module_segments)\n        try:\n            module = __import__(module_name)\n        except ImportError:\n            last_error = sys.exc_info()[1]\n            module_segments.pop()\n            continue\n        else:\n            break\n    else:\n        if last_error is not None and error_callback is not None:\n            error_callback(last_error)\n        return alternative\n    nonexistent = object()\n    for segment in name.split('.')[1:]:\n        module = getattr(module, segment, nonexistent)\n        if module is nonexistent:\n            if last_error is not None and error_callback is not None:\n                error_callback(last_error)\n            return alternative\n    return module\n\n\nCommandNotFound = try_import('CommandNotFound.CommandNotFound')\n\n\ndef path_in_tarfile_exists(path, tar_file):\n    exists = True\n    try:\n        tarinfo = tarfile.open(tar_file, 'r:gz')\n        tarinfo.getmember(path)\n        tarinfo.close()\n    except KeyError:\n        exists = False\n    finally:\n        return exists\n\n\ndef verify_file_integrity(sig_file_list):\n    \"\"\"Verify a list of signature files.\n\n    The parameter is a list of filenames of gpg signature files which will be\n    verified using gpg. For each of the files it is assumed that there is an\n    sha1 hash file with the same file name minus the '.asc' extension.\n\n    Each of the sha1 files will be checked using sha1sums. All files listed in\n    the sha1 hash file must be found in the same directory as the hash file.\n    \"\"\"\n\n    gpg_sig_ok = True\n    gpg_out = \"\"\n\n    verified_files = []\n    for sig_file in sig_file_list:\n        hash_file = sig_file[0:-len('.asc')]\n        tmp = tempfile.NamedTemporaryFile()\n\n        try:\n            cmd_runner.run(['gpg', '--status-file={0}'.format(tmp.name),\n                            '--verify', sig_file]).wait()\n        except cmd_runner.SubcommandNonZeroReturnValue:\n            gpg_sig_ok = False\n            gpg_out = gpg_out + tmp.read()\n\n        tmp.close()\n\n        if os.path.dirname(hash_file) == '':\n            sha_cwd = None\n        else:\n            sha_cwd = os.path.dirname(hash_file)\n\n        try:\n            sha1sums_out, _ = cmd_runner.Popen(\n                ['sha1sum', '-c', hash_file],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                cwd=sha_cwd\n            ).communicate()\n        except cmd_runner.SubcommandNonZeroReturnValue as inst:\n            sha1sums_out = inst.stdout\n\n        for line in sha1sums_out.splitlines():\n            sha1_check = re.search(r'^(.*):\\s+OK', line)\n            if sha1_check:\n                verified_files.append(sha1_check.group(1))\n\n    return verified_files, gpg_sig_ok, gpg_out\n\n\ndef check_file_integrity_and_log_errors(sig_file_list, binary, hwpacks):\n    \"\"\"\n    Wrapper around verify_file_integrity that prints error messages to stderr\n    if verify_file_integrity finds any problems.\n    \"\"\"\n    verified_files, gpg_sig_pass, _ = verify_file_integrity(sig_file_list)\n\n    # Check the outputs from verify_file_integrity\n    # Abort if anything fails.\n    logger = logging.getLogger(__name__)\n    if len(sig_file_list):\n        if not gpg_sig_pass:\n            logger.error(\"GPG signature verification failed.\")\n            return False, []\n\n        if not os.path.basename(binary) in verified_files:\n            logger.error(\"OS Binary verification failed\")\n            return False, []\n\n        for hwpack in hwpacks:\n            if not os.path.basename(hwpack) in verified_files:\n                logger.error(\"Hwpack {0} verification failed\".format(hwpack))\n                return False, []\n\n        for verified_file in verified_files:\n            logger.info('Hash verification of file {0} OK.'.format(\n                verified_file))\n    return True, verified_files\n\n\ndef install_package_providing(command):\n    \"\"\"Install a package which provides the given command.\n\n    If we can't find any package which provides it, raise\n    UnableToFindPackageProvidingCommand.\n\n    If the user denies installing the package, the program exits.\n    \"\"\"\n\n    if CommandNotFound is None:\n        raise UnableToFindPackageProvidingCommand(\n            \"CommandNotFound python module does not exist.\")\n\n    packages = CommandNotFound().getPackages(command)\n    if len(packages) == 0:\n        raise UnableToFindPackageProvidingCommand(\n            \"Unable to find any package providing %s\" % command)\n\n    # TODO: Ask the user to pick a package when there's more than one that\n    # provides the given command.\n    package, _ = packages[0]\n    output, _ = cmd_runner.run(['apt-get', '-s', 'install', package],\n                               stdout=subprocess.PIPE).communicate()\n    to_install = []\n    for line in output.splitlines():\n        if line.startswith(\"Inst\"):\n            to_install.append(line.split()[1])\n    if not to_install:\n        raise UnableToFindPackageProvidingCommand(\n            \"Unable to find any package to be installed.\")\n\n    try:\n        print (\"In order to use the '%s' command, the following package\/s \"\n               \"have to be installed: %s\" % (command, \" \".join(to_install)))\n        resp = raw_input(\"Install? (Y\/n) \")\n        if resp.lower() != 'y':\n            print \"Package installation is necessary to continue. Exiting.\"\n            sys.exit(1)\n        print (\"Installing required command '%s' from package '%s'...\"\n               % (command, package))\n        cmd_runner.run(['apt-get', '--yes', 'install', package],\n                       as_root=True).wait()\n    except EOFError:\n        raise PackageInstallationRefused(\n            \"Package installation interrupted: input error.\")\n    except KeyboardInterrupt:\n        raise PackageInstallationRefused(\n            \"Package installation interrupted by the user.\")\n\n\ndef has_command(command):\n    \"\"\"Check the given command is available.\"\"\"\n    try:\n        cmd_runner.run(\n            ['which', command], stdout=open('\/dev\/null', 'w')).wait()\n        return True\n    except cmd_runner.SubcommandNonZeroReturnValue:\n        return False\n\n\ndef ensure_command(command):\n    \"\"\"Ensure the given command is available.\n\n    If it's not, look up a package that provides it and install that.\n    \"\"\"\n    if not has_command(command):\n        install_package_providing(command)\n\n\ndef find_command(name, prefer_dir=None):\n    \"\"\"Finds a linaro-image-tools command.\n\n    Prefers specified directory, otherwise searches only the current directory\n    when running from a checkout, or only PATH when running from an installed\n    version.\n    \"\"\"\n    assert name != \"\"\n    assert os.path.dirname(name) == \"\"\n\n    cmd_runner.sanitize_path(os.environ)\n\n    # default to searching in current directory when running from a bzr\n    # checkout\n    dirs = [os.getcwd(), ]\n    if os.path.isabs(__file__):\n        dirs = os.environ[\"PATH\"].split(os.pathsep)\n        # empty dir in PATH means current directory\n        dirs = map(lambda x: x == '' and '.' or x, dirs)\n\n    if prefer_dir is not None:\n        dirs.insert(0, prefer_dir)\n\n    for dir in dirs:\n        path = os.path.join(dir, name)\n        if os.path.exists(path) and os.access(path, os.X_OK):\n            return path\n\n    return None\n\n\ndef is_arm_host():\n    return platform.machine().startswith('arm')\n\n\ndef preferred_tools_dir():\n    prefer_dir = None\n    # running from bzr checkout?\n    if not os.path.isabs(__file__):\n        prefer_dir = os.getcwd()\n    return prefer_dir\n\n\ndef prep_media_path(args):\n    if args.directory is not None:\n        loc = os.path.abspath(args.directory)\n        try:\n            os.makedirs(loc)\n        except OSError:\n            # Directory exists.\n            pass\n\n        path = os.path.join(loc, args.device)\n    else:\n        path = args.device\n\n    return path\n\n\nclass UnableToFindPackageProvidingCommand(Exception):\n    \"\"\"We can't find a package which provides the given command.\"\"\"\n\n\nclass PackageInstallationRefused(Exception):\n    \"\"\"User has chosen not to install a package.\"\"\"\n\n\nclass InvalidHwpackFile(Exception):\n    \"\"\"The hwpack parameter is not a regular file.\"\"\"\n\n\nclass MissingRequiredOption(Exception):\n    \"\"\"A required option from the command line is missing.\"\"\"\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)\n\n\nclass IncompatibleOptions(Exception):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)\n\n\ndef additional_option_checks(args):\n    if args.directory is not None:\n    # If args.device is a path to a device (\/dev\/) then this is an error\n        if \"--mmc\" in sys.argv:\n            raise IncompatibleOptions(\"--directory option incompatible with \"\n                                      \"option --mmc\")\n\n        # If directory is used as well as having a full path (rather than just\n        # a file name or relative path) in args.device, this is an error.\n        if re.search(r\"^\/\", args.device):\n            raise IncompatibleOptions(\"--directory option incompatible with \"\n                                      \"a full path in --image-file\")\n\n    for hwpack in args.hwpacks:\n        if not os.path.isfile(hwpack):\n            raise InvalidHwpackFile(\n                \"--hwpack argument (%s) is not a regular file\" % hwpack)\n\n\ndef additional_android_option_checks(args):\n    \"\"\"Checks that some of the args passed to l-a-m-c are valid.\"\"\"\n    if args.hwpack:\n        if not os.path.isfile(args.hwpack):\n            raise InvalidHwpackFile(\n                \"--hwpack argument (%s) is not a regular file\" % args.hwpack)\n\n\ndef andorid_hwpack_in_boot_tarball(boot_dir):\n    \"\"\"Simple check for existence of a path.\n\n    Needed to make cli command testable in some way.\n    :param boot_dir: The path where the boot tarball has been extracted.\n    :type str\n    :return A tuple with a bool if the path exists, and the path to the config\n            file.\n    \"\"\"\n    conf_file = os.path.join(boot_dir, BOOT_DIR_IN_TARBALL, HWPACK_NAME)\n    return os.path.exists(conf_file), conf_file\n\n\ndef check_required_args(args):\n    \"\"\"Check that the required args are passed.\"\"\"\n    if args.dev is None:\n        raise MissingRequiredOption(\"--dev option is required\")\n    if args.binary is None:\n        raise MissingRequiredOption(\"--binary option is required\")\n\n\ndef get_logger(name=DEFAULT_LOGGER_NAME, debug=False):\n    \"\"\"\n    Retrieves a named logger. Default name is set in the variable\n    DEFAULT_LOG_NAME. Debug is set to False by default.\n\n    :param name: The name of the logger.\n    :param debug: If debug level should be turned on\n    :return: A logger instance.\n    \"\"\"\n    logger = logging.getLogger(name)\n    ch = logging.StreamHandler()\n\n    if debug:\n        ch.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        ch.setFormatter(formatter)\n        logger.setLevel(logging.DEBUG)\n    else:\n        ch.setLevel(logging.INFO)\n        formatter = logging.Formatter(\"%(message)s\")\n        ch.setFormatter(formatter)\n        logger.setLevel(logging.INFO)\n\n    logger.addHandler(ch)\n    return logger\n","license":"gpl-3.0","hash":8347262142669630789,"line_mean":31.3946078431,"line_max":78,"alpha_frac":0.6335023076,"autogenerated":false},
{"repo_name":"cbertinato\/pandas","path":"pandas\/io\/date_converters.py","copies":"1","size":"1865","content":"\"\"\"This module is designed for community supported date conversion functions\"\"\"\nimport numpy as np\n\nfrom pandas._libs.tslibs import parsing\n\n\ndef parse_date_time(date_col, time_col):\n    date_col = _maybe_cast(date_col)\n    time_col = _maybe_cast(time_col)\n    return parsing.try_parse_date_and_time(date_col, time_col)\n\n\ndef parse_date_fields(year_col, month_col, day_col):\n    year_col = _maybe_cast(year_col)\n    month_col = _maybe_cast(month_col)\n    day_col = _maybe_cast(day_col)\n    return parsing.try_parse_year_month_day(year_col, month_col, day_col)\n\n\ndef parse_all_fields(year_col, month_col, day_col, hour_col, minute_col,\n                     second_col):\n    year_col = _maybe_cast(year_col)\n    month_col = _maybe_cast(month_col)\n    day_col = _maybe_cast(day_col)\n    hour_col = _maybe_cast(hour_col)\n    minute_col = _maybe_cast(minute_col)\n    second_col = _maybe_cast(second_col)\n    return parsing.try_parse_datetime_components(year_col, month_col, day_col,\n                                                 hour_col, minute_col,\n                                                 second_col)\n\n\ndef generic_parser(parse_func, *cols):\n    N = _check_columns(cols)\n    results = np.empty(N, dtype=object)\n\n    for i in range(N):\n        args = [c[i] for c in cols]\n        results[i] = parse_func(*args)\n\n    return results\n\n\ndef _maybe_cast(arr):\n    if not arr.dtype.type == np.object_:\n        arr = np.array(arr, dtype=object)\n    return arr\n\n\ndef _check_columns(cols):\n    if not len(cols):\n        raise AssertionError(\"There must be at least 1 column\")\n\n    head, tail = cols[0], cols[1:]\n\n    N = len(head)\n\n    for i, n in enumerate(map(len, tail)):\n        if n != N:\n            raise AssertionError('All columns must have the same length: {0}; '\n                                 'column {1} has length {2}'.format(N, i, n))\n\n    return N\n","license":"bsd-3-clause","hash":8620790840864050110,"line_mean":28.6031746032,"line_max":79,"alpha_frac":0.5962466488,"autogenerated":false},
{"repo_name":"Digilent\/u-boot-digilent","path":"test\/py\/tests\/test_i2c.py","copies":"1","size":"1946","content":"# Copyright (c) 2015 Stephen Warren\n#\n# SPDX-License-Identifier: GPL-2.0\n\nimport pytest\nimport random\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_bus(u_boot_console):\n    expected_response = \"Bus\"\n    response = u_boot_console.run_command(\"i2c bus\")\n    assert(expected_response in response)\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_dev(u_boot_console):\n    expected_response = \"Current bus\"\n    response = u_boot_console.run_command(\"i2c dev\")\n    assert(expected_response in response)\n\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe(u_boot_console):\n    expected_response = \"Valid chip addresses:\"\n    response = u_boot_console.run_command(\"i2c probe\")\n    assert(expected_response in response)\n\n@pytest.mark.boardidentity(\"!qemu\")\n@pytest.mark.boardspec(\"zynq_zc702\")\n@pytest.mark.boardspec(\"zynq_zc706\")\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe_zc70x(u_boot_console):\n    # Enable i2c mux bridge\n    u_boot_console.run_command(\"i2c mw 74 0 4\")\n    u_boot_console.run_command(\"i2c probe\")\n    val = format(random.randint(0,255), '02x')\n    u_boot_console.run_command(\"i2c mw 54 0 \" + val + \" 5\")\n    response = u_boot_console.run_command(\"i2c md 54 0 5\")\n    expected_response = \"0000: \" + val + \" \" + val + \" \" + val + \" \" + val + \" \" + val + \" \"\n    assert(expected_response in response)\n\n@pytest.mark.boardspec(\"xilinx_zynqmp_zcu102\")\n@pytest.mark.buildconfigspec(\"cmd_i2c\")\ndef test_i2c_probe_zcu102(u_boot_console):\n    # This is using i2c mux wiring from config file\n    u_boot_console.run_command(\"i2c dev 5\")\n    u_boot_console.run_command(\"i2c probe\")\n    val = format(random.randint(0,255), '02x')\n    u_boot_console.run_command(\"i2c mw 54 0 \" + val + \" 5\")\n    response = u_boot_console.run_command(\"i2c md 54 0 5\")\n    expected_response = \"0000: \" + val + \" \" + val + \" \" + val + \" \" + val + \" \" + val + \" \"\n    print expected_response\n    assert(expected_response in response)\n","license":"gpl-2.0","hash":-4781732101770132973,"line_mean":37.1568627451,"line_max":92,"alpha_frac":0.674717369,"autogenerated":false},
{"repo_name":"JeffHoogland\/qtdesigner-pyside-tutorial","path":"tut1-mainwindow\/ui_mainWindow.py","copies":"2","size":"1602","content":"# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'mainWindow.ui'\n#\n# Created: Thu Mar 12 15:46:01 2015\n#      by: pyside-uic 0.2.15 running on PySide 1.2.1\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PySide import QtCore, QtGui\n\nclass Ui_mainWindow(object):\n    def setupUi(self, mainWindow):\n        mainWindow.setObjectName(\"mainWindow\")\n        mainWindow.resize(376, 207)\n        self.centralwidget = QtGui.QWidget(mainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.verticalLayout = QtGui.QVBoxLayout(self.centralwidget)\n        self.verticalLayout.setObjectName(\"verticalLayout\")\n        self.goText = QtGui.QTextEdit(self.centralwidget)\n        self.goText.setEnabled(True)\n        self.goText.setTextInteractionFlags(QtCore.Qt.TextSelectableByKeyboard|QtCore.Qt.TextSelectableByMouse)\n        self.goText.setObjectName(\"goText\")\n        self.verticalLayout.addWidget(self.goText)\n        self.goButton = QtGui.QPushButton(self.centralwidget)\n        self.goButton.setObjectName(\"goButton\")\n        self.verticalLayout.addWidget(self.goButton)\n        mainWindow.setCentralWidget(self.centralwidget)\n\n        self.retranslateUi(mainWindow)\n        QtCore.QMetaObject.connectSlotsByName(mainWindow)\n\n    def retranslateUi(self, mainWindow):\n        mainWindow.setWindowTitle(QtGui.QApplication.translate(\"mainWindow\", \"Qt Designer MainWindow Example\", None, QtGui.QApplication.UnicodeUTF8))\n        self.goButton.setText(QtGui.QApplication.translate(\"mainWindow\", \"Go Button\", None, QtGui.QApplication.UnicodeUTF8))\n\n","license":"bsd-3-clause","hash":-2359122869957934119,"line_mean":43.5,"line_max":149,"alpha_frac":0.7322097378,"autogenerated":false},
{"repo_name":"bburan\/psiexperiment","path":"psi\/context\/expression.py","copies":"1","size":"3158","content":"import logging\r\nlog = logging.getLogger(__name__)\r\n\r\nfrom atom.api import Atom, Typed\r\nfrom psi.util import get_dependencies\r\n\r\n\r\nclass Expr(object):\r\n\r\n    def __init__(self, expression):\r\n        if not isinstance(expression, str):\r\n            raise ValueError('Expression must be a string')\r\n        if not expression:\r\n            raise ValueError('No value provided for expression')\r\n        self._expression = expression\r\n        self._code = compile(expression, 'dynamic', 'eval')\r\n        self._dependencies = get_dependencies(expression)\r\n\r\n    def evaluate(self, context):\r\n        return eval(self._expression, context)\r\n\r\n\r\nclass ExpressionNamespace(Atom):\r\n\r\n    _locals = Typed(dict, {})\r\n    _expressions = Typed(dict, {})\r\n    _globals = Typed(dict, {})\r\n\r\n    def __init__(self, expressions=None, globals=None):\r\n        if globals is None:\r\n            globals = {}\r\n        if expressions is None:\r\n            expressions = {}\r\n        self._locals = {}\r\n        self._globals = globals\r\n        self._expressions = {k: Expr(str(v)) for k, v in expressions.items()}\r\n\r\n    def update_expressions(self, expressions):\r\n        self._expressions.update({k: Expr(str(v)) for k, v in expressions.items()})\r\n\r\n    def update_symbols(self, symbols):\r\n        self._globals.update(symbols)\r\n\r\n    def reset(self, context_item_names=None):\r\n        '''\r\n        Clears the computed values (as well as any user-provided values) in\r\n        preparation for the next cycle.\r\n        '''\r\n        self._locals = {}\r\n\r\n    def get_value(self, name, context=None):\r\n        if name not in self._locals:\r\n            self._evaluate_value(name, context)\r\n        return self._locals[name]\r\n\r\n    def get_values(self, names=None, context=None):\r\n        if names is None:\r\n            names = self._expressions.keys()\r\n        for name in names:\r\n            if name not in self._locals:\r\n                self._evaluate_value(name, context)\r\n        return dict(self._locals.copy())\r\n\r\n    def set_value(self, name, value):\r\n        _locals = self._locals.copy()\r\n        _locals[name] = value\r\n        self._locals = _locals\r\n\r\n    def set_values(self, values):\r\n        _locals = self._locals.copy()\r\n        _locals.update(values)\r\n        self._locals = _locals\r\n\r\n    def _evaluate_value(self, name, context=None):\r\n        if context is None:\r\n            context = {}\r\n\r\n        if name in context:\r\n            self._locals[name] = context[name]\r\n            return\r\n\r\n        expr = self._expressions[name]\r\n        c = self._globals.copy()\r\n        c.update(self._locals)\r\n        c.update(context)\r\n\r\n        # Build a context dictionary containing the dependencies required for\r\n        # evaluating the expression.\r\n        for d in expr._dependencies:\r\n            if d not in c and d in self._expressions:\r\n                c[d] = self.get_value(d, c)\r\n        \r\n        # Note that in the past I was forcing a copy of self._locals to ensure\r\n        # that the GUI was updated as needed; however, this proved to be a very\r\n        # slow process since it triggered a cascade of GUI updates. \r\n        self._locals[name] = expr.evaluate(c)\r\n","license":"mit","hash":6478125211385565033,"line_mean":30.8958333333,"line_max":83,"alpha_frac":0.5782140595,"autogenerated":false},
{"repo_name":"HPI-SWA-Lab\/BP2016H1","path":"frt_server\/settings.py","copies":"1","size":"1576","content":"from frt_server.tables import *\nimport frt_server.common\nimport frt_server.config\n\nfrom eve.utils import config\n\nID_FIELD = '_id'\nconfig.ID_FIELD = ID_FIELD\n\nSQLALCHEMY_TRACK_MODIFICATIONS = False\nSQLALCHEMY_RECORD_QUERIES = False\nSQLALCHEMY_ECHO = False\nSQLALCHEMY_DATABASE_URI = frt_server.config.DATABASE_PATH\nRESOURCE_METHODS = ['GET', 'POST']\nITEM_METHODS = ['GET', 'DELETE', 'PATCH']\nIF_MATCH = False\nHATEOAS = False\nPROJECTION = False\nTRANSPARENT_SCHEMA_RULES = False\nBULK_ENABLED = False\nLAST_UPDATED = 'updated_at'\nDATE_CREATED = 'created_at'\nDATE_FORMAT = frt_server.common.DATE_FORMAT\n\nuser_schema = User._eve_schema['user']\nuser_schema['allowed_filters'] = []\nuser_projection = user_schema['datasource']['projection']\nuser_projection['salt'] = 0\nuser_projection['password'] = 0\nDOMAIN = {\n    'user': user_schema,\n    'tag': Tag._eve_schema['tag'],\n    'sample_text': SampleText._eve_schema['sample_text'],\n    'family': Family._eve_schema['family'],\n    'font': Font._eve_schema['font'],\n    'glyph': Glyph._eve_schema['glyph'],\n    'thread': Thread._eve_schema['thread'],\n    'codepoint': Codepoint._eve_schema['codepoint'],\n    'comment': Comment._eve_schema['comment'],\n    'attachment': Attachment._eve_schema['attachment'],\n    'thread_glyph_association': ThreadGlyphAssociation._eve_schema['thread_glyph_association'],\n    'thread_subscription': ThreadSubscription._eve_schema['thread_subscription'],\n    'feedback': Feedback._eve_schema['feedback']\n}\n\n# Include our custom versions list\nDOMAIN['family']['datasource']['projection']['version_messages'] = 1\n","license":"mit","hash":553750893310298538,"line_mean":32.5319148936,"line_max":95,"alpha_frac":0.7087563452,"autogenerated":false},
{"repo_name":"solbirn\/pyActiveSync","path":"pyActiveSync\/client\/FolderDelete.py","copies":"1","size":"2738","content":"########################################################################\n#  Copyright (C) 2013 Sol Birnbaum\n# \n#  This program is free software; you can redistribute it and\/or\n#  modify it under the terms of the GNU General Public License\n#  as published by the Free Software Foundation; either version 2\n#  of the License, or (at your option) any later version.\n# \n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n# \n#  You should have received a copy of the GNU General Public License\n#  along with this program; if not, write to the Free Software\n#  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,\n#  MA  02110-1301, USA.\n########################################################################\n\nfrom utils.wapxml import wapxmltree, wapxmlnode\n\nclass FolderDelete:\n    \"\"\"http:\/\/msdn.microsoft.com\/en-us\/library\/ee201525(v=exchg.80).aspx\"\"\"\n\n    @staticmethod\n    def build(synckey, server_id):\n        folderdelete_xmldoc_req = wapxmltree()\n        xmlrootnode = wapxmlnode(\"FolderDelete\")\n        folderdelete_xmldoc_req.set_root(xmlrootnode, \"folderhierarchy\")\n        xmlsynckeynode = wapxmlnode(\"SyncKey\", xmlrootnode, synckey)\n        xmlserveridnode = wapxmlnode(\"ServerId\", xmlrootnode, server_id)\n        return folderdelete_xmldoc_req\n\n    @staticmethod\n    def parse(wapxml):\n\n        namespace = \"folderhierarchy\"\n        root_tag = \"FolderDelete\"\n\n        root_element = wapxml.get_root()\n        if root_element.get_xmlns() != namespace:\n            raise AttributeError(\"Xmlns '%s' submitted to '%s' parser. Should be '%s'.\" % (root_element.get_xmlns(), root_tag, namespace))\n        if root_element.tag != root_tag:\n            raise AttributeError(\"Root tag '%s' submitted to '%s' parser. Should be '%s'.\" % (root_element.tag, root_tag, root_tag))\n\n        folderhierarchy_folderdelete_children = root_element.get_children()\n\n        folderhierarchy_folderdelete_status = None\n        folderhierarchy_folderdelete_synckey = None\n        folderhierarchy_folderdelete_serverid = None\n\n        for element in folderhierarchy_folderdelete_children:\n            if element.tag is \"Status\":\n                folderhierarchy_folderdelete_status = element.text\n                if folderhierarchy_folderdelete_status != \"1\":\n                     print \"FolderDelete Exception: %s\" % folderhierarchy_folderdelete_status\n            elif element.tag == \"SyncKey\":\n                folderhierarchy_folderdelete_synckey = element.text\n        return (folderhierarchy_folderdelete_status, folderhierarchy_folderdelete_synckey)","license":"gpl-2.0","hash":6184328937033222964,"line_mean":45.4237288136,"line_max":138,"alpha_frac":0.65376187,"autogenerated":false},
{"repo_name":"olivierverdier\/sfepy","path":"sfepy\/fem\/extmods\/setup.py","copies":"1","size":"3036","content":"#!\/usr\/bin\/env python\n\ndef configuration(parent_package='', top_path=None):\n    import distutils.sysconfig as sysc\n    from numpy.distutils.misc_util import Configuration\n    import os.path as op\n\n    import sys;\n    if 'script' not in sys.path:\n        sys.path.append('script')\n    from config import Config\n    system = Config().system()\n    os_flag = {'posix' : 0, 'windows' : 1}\n\n    auto_dir = op.dirname(__file__)\n    auto_name = op.split(auto_dir)[-1]\n    config = Configuration(auto_name, parent_package, top_path)\n    config.add_data_files(('sfepy\/fem\/extmods', ('version.h.in',)))\n\n    defines = [('__SDIR__', \"'\\\"%s\\\"'\" % auto_dir),\n               ('DEBUGFMF', None),\n               ('SFEPY_PLATFORM', os_flag[system])]\n\n    src = ['fmfield.c', 'fem.c', 'fem.i', 'geommech.c', 'sort.c',\n           'common_python.c']\n    config.add_extension('_fem',\n                         sources=src,\n                         depends=['array.i', 'common.i', 'fmfield.i'],\n                         extra_compile_args=['-O2'],\n                         include_dirs=[auto_dir],\n                         define_macros=defines)\n\n    src = ['geomtrans.c', 'meshutils.c', 'meshutils.i', 'common_python.c',\n           'sort.c']\n    config.add_extension('_meshutils',\n                         sources=src,\n                         depends=['array.i', 'common.i'],\n                         extra_compile_args=['-O2'],\n                         include_dirs=[auto_dir],\n                         define_macros=defines)\n\n    src = ['fmfield.c', 'geometry.c', 'geometry.i', 'geommech.c',\n           'common_python.c']\n    config.add_extension('_geometry',\n                         sources=src,\n                         depends=['array.i', 'common.i', 'fmfield.i'],\n                         extra_compile_args=['-O2'],\n                         include_dirs=[auto_dir],\n                         define_macros=defines)\n\n    \n## gcc -g -O2 -fPIC -DPIC -D__SDIR__='\"tests\"' -DDEBUG_FMF -Iexamples -Iinput\n## -Isfepy -Isfepy\/applications -Isfepy\/base -Isfepy\/eldesc -Isfepy\/fem\n## -Isfepy\/fem\/extmods -Isfepy\/geom -Isfepy\/homogenization -Isfepy\/mechanics\n## -Isfepy\/solvers -Isfepy\/terms -Isfepy\/terms\/extmods -Isfepy\/physics\n## -Isfepy\/physics\/extmods -Itests -I\/usr\/include\/python2.5\n## -I\/home\/share\/software\/usr\/lib\/python\/site-packages\/numpy\/core\/include -Wall\n## -c -DISRELEASE sfepy\/fem\/extmods\/fmfield.c -o sfepy\/fem\/extmods\/fmfield.o\n\n## swig -python -Iexamples -Iinput -Isfepy -Isfepy\/applications -Isfepy\/base\n## -Isfepy\/eldesc -Isfepy\/fem -Isfepy\/fem\/extmods -Isfepy\/geom\n## -Isfepy\/homogenization -Isfepy\/mechanics -Isfepy\/solvers -Isfepy\/terms\n## -Isfepy\/terms\/extmods -Isfepy\/physics -Isfepy\/physics\/extmods -Itests\n## -I\/usr\/include\/python2.5\n## -I\/home\/share\/software\/usr\/lib\/python\/site-packages\/numpy\/core\/include\n## -DISRELEASE -o sfepy\/fem\/extmods\/meshutils_wrap.c\n## sfepy\/fem\/extmods\/meshutils.i\n\n    return config\n\nif __name__ == '__main__':\n    from numpy.distutils.core import setup\n    setup(**configuration(top_path='').todict())\n","license":"bsd-3-clause","hash":4851889873265226462,"line_mean":40.5890410959,"line_max":79,"alpha_frac":0.5830039526,"autogenerated":false},
{"repo_name":"SebastiaanPasterkamp\/dnd-machine","path":"app\/views\/campaign.py","copies":"1","size":"6197","content":"# -*- coding: utf-8 -*-\nfrom flask import request, render_template, session, jsonify, url_for, redirect\n\nimport re\n\nfrom views.baseapi import BaseApiBlueprint, BaseApiCallback\nfrom errors import ApiException\nfrom filters import filter_unique\nfrom utils import markdownToToc, indent\nfrom functools import reduce\n\nclass CampaignBlueprint(BaseApiBlueprint):\n\n    def __init__(self, name, *args, **kwargs):\n        super(CampaignBlueprint, self).__init__(name, *args, **kwargs)\n\n        self.add_url_rule(\n            '\/current', 'get_current',\n            self.get_current, methods=['GET'])\n        self.add_url_rule(\n            '\/current', 'set_current',\n            self.set_current, methods=['POST'])\n        self.add_url_rule(\n            '\/current\/<int:obj_id>', 'set_current',\n            self.set_current, methods=['POST'])\n\n    @property\n    def datamapper(self):\n        return self.basemapper.campaign\n\n    @property\n    def encountermapper(self):\n        return self.basemapper.encounter\n\n    @property\n    def monstermapper(self):\n        return self.basemapper.monster\n\n    @property\n    def npcmapper(self):\n        return self.basemapper.npc\n\n    @property\n    def usermapper(self):\n        return self.basemapper.user\n\n    @BaseApiCallback('index')\n    @BaseApiCallback('overview')\n    @BaseApiCallback('show')\n    @BaseApiCallback('new')\n    @BaseApiCallback('edit')\n    @BaseApiCallback('api_list')\n    @BaseApiCallback('api_get')\n    @BaseApiCallback('api_post')\n    @BaseApiCallback('api_patch')\n    @BaseApiCallback('api_delete')\n    @BaseApiCallback('api_recompute')\n    @BaseApiCallback('get_current')\n    @BaseApiCallback('set_current')\n    def adminOrDmOnly(self, *args, **kwargs):\n        if not self.checkRole(['admin', 'dm']):\n            raise ApiException(403, \"Insufficient permissions\")\n\n    @BaseApiCallback('raw')\n    def adminOnly(self):\n        if not self.checkRole(['admin']):\n            raise ApiException(403, \"Insufficient permissions\")\n\n    @BaseApiCallback('api_list.objects')\n    def adminOrOwnedMultiple(self, objs):\n        for obj in objs:\n            del obj.story\n        if not self.checkRole(['admin']):\n            objs[:] = [\n                obj\n                for obj in objs\n                if obj.user_id == request.user.id\n                ]\n\n    @BaseApiCallback('show.object')\n    @BaseApiCallback('edit.object')\n    @BaseApiCallback('api_get.object')\n    @BaseApiCallback('api_patch.original')\n    @BaseApiCallback('api_delete.object')\n    def adminOrOwnedSingle(self, obj):\n        if self.checkRole(['admin']):\n            return\n        if obj.user_id != request.user.id:\n            raise ApiException(403, \"Insufficient permissions\")\n\n    @BaseApiCallback('api_post.object')\n    @BaseApiCallback('api_copy.object')\n    def setOwner(self, obj):\n        obj.user_id = request.user.id\n\n    def show(self, obj_id):\n        self.doCallback('show', obj_id)\n\n        obj = self.datamapper.getById(obj_id)\n        user = self.usermapper.getById(obj.user_id)\n\n        self.doCallback('show.object', obj)\n\n        replace = {}\n        for match in re.finditer(\n                r\"^\/encounter\/(\\d+)\\b\", obj.story, re.M):\n\n            pattern, encounter_id = \\\n                match.group(0), int(match.group(1))\n            if pattern in replace:\n                continue\n\n            encounter = self.encountermapper.getById(encounter_id)\n            if encounter is None:\n                continue\n\n            encounter.monsters = \\\n                self.monstermapper.getByEncounterId(encounter.id)\n\n            replace[pattern] = \"\\n\\n!!! encounter\\n\" + indent(\n                    render_template(\n                        'encounter\/show.md',\n                        encounter=encounter,\n                        indent='##'\n                        )\n                    )\n\n            skip = set()\n            monsters = sorted(\n                encounter.monsters,\n                key=lambda m: m.challenge_rating,\n                reverse=True\n                )\n            for monster in monsters:\n                if monster.id in skip:\n                    continue\n                skip.add(monster.id)\n\n                replace[pattern] += \"\\n\\n!!! monster\\n\" + indent(\n                    render_template(\n                        'monster\/show.md',\n                        monster=monster,\n                        indent='###'\n                        )\n                    )\n            replace[pattern] += \"\\n\"\n\n        for match in re.finditer(\n                r\"^\/npc\/(\\d+)\\b\", obj.story, re.M):\n\n            pattern, npc_id = \\\n                match.group(0), int(match.group(1))\n            if pattern in replace:\n                continue\n\n            npc = self.npcmapper.getById(npc_id)\n            if npc is None:\n                continue\n\n            replace[pattern] = \"\\n!!! npc\\n\" + indent(\n                render_template(\n                    'npc\/show.md',\n                    npc=npc,\n                    indent='##'\n                    )\n                ) + \"\\n\"\n\n        obj.story = reduce(\n            lambda subject, kv: re.sub(\n                r\"%s\\b\" % kv[0], kv[1], subject\n                ),\n            iter(replace.items()),\n            obj.story\n            )\n\n        obj.toc = markdownToToc(obj.story)\n\n        return render_template(\n            'campaign\/show.html',\n            campaign=obj,\n            user=user\n            )\n\n    def get_current(self):\n        self.doCallback('get_current')\n        if session.get('campaign_id') is None:\n            return jsonify(None)\n        return redirect(url_for(\n            '%s.api_get' % self.name,\n            obj_id=session.get('campaign_id'),\n            ))\n\n    def set_current(self, obj_id=None):\n        self.doCallback('set_current', obj_id)\n        session['campaign_id'] = obj_id\n        if not obj_id:\n            return jsonify(None)\n        return redirect(url_for(\n            '%s.api_get' % self.name,\n            obj_id=obj_id,\n            ))\n\ndef get_blueprint(basemapper, config):\n    return '\/campaign', CampaignBlueprint(\n        'campaign',\n        __name__,\n        basemapper,\n        config,\n        template_folder='templates'\n        )\n","license":"gpl-3.0","hash":709065835365650475,"line_mean":28.5095238095,"line_max":79,"alpha_frac":0.5246086816,"autogenerated":false},
{"repo_name":"closeio\/nylas","path":"inbox\/transactions\/search.py","copies":"1","size":"6718","content":"import json\nfrom datetime import datetime\n\nfrom sqlalchemy import asc\nfrom sqlalchemy.sql import func\nfrom sqlalchemy.orm import joinedload\nfrom gevent import Greenlet, sleep\n\nfrom inbox.ignition import engine_manager\nfrom inbox.util.itert import partition\nfrom inbox.models import Transaction, Contact\nfrom inbox.util.stats import statsd_client\nfrom inbox.models.session import session_scope_by_shard_id\nfrom inbox.models.search import ContactSearchIndexCursor\nfrom inbox.contacts.search import (get_doc_service, DOC_UPLOAD_CHUNK_SIZE,\n                                   cloudsearch_contact_repr)\n\nfrom nylas.logging import get_logger\nfrom nylas.logging.sentry import log_uncaught_errors\n\nlog = get_logger()\n\n\nclass ContactSearchIndexService(Greenlet):\n    \"\"\"\n    Poll the transaction log for contact operations\n    (inserts, updates, deletes) for all namespaces and perform the\n    corresponding CloudSearch index operations.\n\n    \"\"\"\n\n    def __init__(self, poll_interval=30, chunk_size=DOC_UPLOAD_CHUNK_SIZE):\n        self.poll_interval = poll_interval\n        self.chunk_size = chunk_size\n        self.transaction_pointers = {}\n\n        self.log = log.new(component='contact-search-index')\n        Greenlet.__init__(self)\n\n    def _report_batch_upload(self):\n        metric_names = [\n            \"contacts_search_index.transactions.batch_upload\",\n        ]\n\n        for metric in metric_names:\n            statsd_client.incr(metric)\n\n    def _report_transactions_latency(self, latency):\n        metric_names = [\n            \"contacts_search_index.transactions.latency\",\n        ]\n\n        for metric in metric_names:\n            statsd_client.timing(metric, latency)\n\n    def _publish_heartbeat(self):\n        metric_names = [\n            \"contacts_search_index.heartbeat\",\n        ]\n\n        for metric in metric_names:\n            statsd_client.incr(metric)\n\n    def _set_transaction_pointers(self):\n        for key in engine_manager.engines:\n            with session_scope_by_shard_id(key) as db_session:\n                pointer = db_session.query(\n                    ContactSearchIndexCursor).first()\n                if pointer:\n                    self.transaction_pointers[key] = pointer.transaction_id\n                else:\n                    # Never start from 0; if the service hasn't run before\n                    # start from the latest transaction, with the expectation\n                    # that a backfill will be run separately.\n                    max_id = db_session.query(\n                        func.max(Transaction.id)).scalar() or 0\n                    latest_transaction = \\\n                        db_session.query(Transaction).get(max_id)\n                    if latest_transaction:\n                        self.transaction_pointers[\n                            key] = latest_transaction.id\n                    else:\n                        self.transaction_pointers[key] = 0\n\n    def _index_transactions(self, namespace_ids=[]):\n        \"\"\" index with filter \"\"\"\n        # index 'em\n        for key in engine_manager.engines:\n            shard_should_sleep = []\n            with session_scope_by_shard_id(key) as db_session:\n                txn_query = db_session.query(Transaction).filter(\n                    Transaction.id > self.transaction_pointers[key],\n                    Transaction.object_type == 'contact')\n                if namespace_ids:\n                    txn_query = txn_query.filter(\n                        Transaction.namespace_id.in_(\n                            namespace_ids))\n                transactions = txn_query\\\n                    .order_by(asc(Transaction.id)) \\\n                    .limit(self.chunk_size).all()\n\n                # index up to chunk_size transactions\n                should_sleep = False\n                if transactions:\n                    self.index(transactions, db_session)\n                    oldest_transaction = min(\n                        transactions, key=lambda t: t.created_at)\n                    current_timestamp = datetime.utcnow()\n                    latency = (current_timestamp -\n                               oldest_transaction.created_at).total_seconds()\n                    self._report_transactions_latency(latency)\n                    new_pointer = transactions[-1].id\n                    self.update_pointer(new_pointer, key, db_session)\n                    db_session.commit()\n                else:\n                    should_sleep = True\n            shard_should_sleep.append(should_sleep)\n        if all(shard_should_sleep):\n            log.info('sleeping')\n            sleep(self.poll_interval)\n\n    def _run(self):\n        \"\"\"\n        Index into CloudSearch the contacts of all namespaces.\n\n        \"\"\"\n        try:\n            self._set_transaction_pointers()\n\n            self.log.info('Starting contact-search-index service',\n                          transaction_pointers=self.transaction_pointers)\n\n            while True:\n                self._publish_heartbeat()\n                self._index_transactions()\n\n        except Exception:\n            log_uncaught_errors(log)\n\n    def index(self, transactions, db_session):\n        \"\"\"\n        Translate database operations to CloudSearch index operations\n        and perform them.\n\n        \"\"\"\n        docs = []\n        doc_service = get_doc_service()\n        add_txns, delete_txns = partition(\n            lambda trx: trx.command == 'delete', transactions)\n        delete_docs = [{'type': 'delete', 'id': txn.record_id}\n                       for txn in delete_txns]\n        add_record_ids = [txn.record_id for txn in add_txns]\n        add_records = db_session.query(Contact).options(\n            joinedload(\"phone_numbers\")).filter(\n                Contact.id.in_(add_record_ids))\n        add_docs = [{'type': 'add', 'id': obj.id,\n                     'fields': cloudsearch_contact_repr(obj)}\n                    for obj in add_records]\n        docs = delete_docs + add_docs\n\n        if docs:\n            doc_service.upload_documents(\n                documents=json.dumps(docs),\n                contentType='application\/json')\n            self._report_batch_upload()\n\n        self.log.info('docs indexed', adds=len(add_docs),\n                      deletes=len(delete_docs))\n\n    def update_pointer(self, new_pointer, shard_key, db_session):\n        \"\"\"\n        Persist transaction pointer to support restarts, update\n        self.transaction_pointer.\n\n        \"\"\"\n        pointer = db_session.query(ContactSearchIndexCursor).first()\n        if pointer is None:\n            pointer = ContactSearchIndexCursor()\n            db_session.add(pointer)\n        pointer.transaction_id = new_pointer\n        self.transaction_pointers[shard_key] = new_pointer\n","license":"agpl-3.0","hash":6984933507339685103,"line_mean":36.1160220994,"line_max":77,"alpha_frac":0.5675796368,"autogenerated":false},
{"repo_name":"robomq\/robomq.io","path":"sdk\/SSL\/certificate-not-verified\/producer.py","copies":"1","size":"1174","content":"# File: producer.py\n# Description: This is the AMQP SSL producer publishes outgoing AMQP\n#     communication to  clients consuming messages from a broker server.\n#     This example applies routing-key pattern out of 5 patterns.\n#     It doesn't verify the certificate of robomq.io.\n#\n# Author: Eamin Zhang\n# robomq.io (http:\/\/www.robomq.io)\n\nimport pika\nimport ssl\n\nserver = \"hostname\"\nport = 5671\nvhost = \"yourvhost\" \nusername = \"username\"\npassword = \"password\"\nexchangeName = \"testEx\"\nroutingKey = \"test\"\n\ntry:\n\t#connect\n\tcredentials = pika.PlainCredentials(username, password)\n\tsslOptions = {\"cert_reqs\": ssl.CERT_NONE}\n\tparameters = pika.ConnectionParameters(host = server, port = port, virtual_host = vhost, credentials = credentials, heartbeat_interval = 60, ssl = True, ssl_options = sslOptions)\n\tconnection = pika.BlockingConnection(parameters)\n\tchannel = connection.channel()\n\n\t#send message\n\tproperties = pika.spec.BasicProperties(content_type = \"text\/plain\", delivery_mode = 1)\n\tchannel.basic_publish(exchange = exchangeName, routing_key = routingKey, body = \"Hello World!\", properties = properties)\n\n\t#disconnect\n\tconnection.close()\nexcept Exception, e:\n\tprint e","license":"apache-2.0","hash":795165451424089893,"line_mean":31.6388888889,"line_max":179,"alpha_frac":0.7461669506,"autogenerated":false},
{"repo_name":"munhyunsu\/Hobby","path":"Pygame\/tetris\/tetris.py","copies":"1","size":"7057","content":"#!\/usr\/bin\/env python\n#-*- coding: utf-8 -*-\n\n# Very simple tetris implementation\n# \n# Control keys:\n# Down - Drop stone faster\n# Left\/Right - Move stone\n# Up - Rotate Stone clockwise\n# Escape - Quit game\n# P - Pause game\n#\n# Have fun!\n\n# Copyright (c) 2010 \"Kevin Chabowski\"<kevin@kch42.de>\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\nfrom random import randrange as rand\nimport pygame, sys\n\n# The configuration\nconfig = {\n\t'cell_size':\t20,\n\t'cols':\t\t8,\n\t'rows':\t\t16,\n\t'delay':\t750,\n\t'maxfps':\t30\n}\n\ncolors = [\n(0,   0,   0  ),\n(255, 0,   0  ),\n(0,   150, 0  ),\n(0,   0,   255),\n(255, 120, 0  ),\n(255, 255, 0  ),\n(180, 0,   255),\n(0,   220, 220)\n]\n\n# Define the shapes of the single parts\ntetris_shapes = [\n\t[[1, 1, 1],\n\t [0, 1, 0]],\n\t\n\t[[0, 2, 2],\n\t [2, 2, 0]],\n\t\n\t[[3, 3, 0],\n\t [0, 3, 3]],\n\t\n\t[[4, 0, 0],\n\t [4, 4, 4]],\n\t\n\t[[0, 0, 5],\n\t [5, 5, 5]],\n\t\n\t[[6, 6, 6, 6]],\n\t\n\t[[7, 7],\n\t [7, 7]]\n]\n\ndef rotate_clockwise(shape):\n\treturn [ [ shape[y][x]\n\t\t\tfor y in xrange(len(shape)) ]\n\t\tfor x in xrange(len(shape[0]) - 1, -1, -1) ]\n\ndef check_collision(board, shape, offset):\n\toff_x, off_y = offset\n\tfor cy, row in enumerate(shape):\n\t\tfor cx, cell in enumerate(row):\n\t\t\ttry:\n\t\t\t\tif cell and board[ cy + off_y ][ cx + off_x ]:\n\t\t\t\t\treturn True\n\t\t\texcept IndexError:\n\t\t\t\treturn True\n\treturn False\n\ndef remove_row(board, row):\n\tdel board[row]\n\treturn [[0 for i in xrange(config['cols'])]] + board\n\t\ndef join_matrixes(mat1, mat2, mat2_off):\n\toff_x, off_y = mat2_off\n\tfor cy, row in enumerate(mat2):\n\t\tfor cx, val in enumerate(row):\n\t\t\tmat1[cy+off_y-1\t][cx+off_x] += val\n\treturn mat1\n\ndef new_board():\n\tboard = [ [ 0 for x in xrange(config['cols']) ]\n\t\t\tfor y in xrange(config['rows']) ]\n\tboard += [[ 1 for x in xrange(config['cols'])]]\n\treturn board\n\nclass TetrisApp(object):\n\tdef __init__(self):\n\t\tpygame.init()\n\t\tpygame.key.set_repeat(250,25)\n\t\tself.width = config['cell_size']*config['cols']\n\t\tself.height = config['cell_size']*config['rows']\n\t\t\n\t\tself.screen = pygame.display.set_mode((self.width, self.height))\n\t\tpygame.event.set_blocked(pygame.MOUSEMOTION) # We do not need\n\t\t                                             # mouse movement\n\t\t                                             # events, so we\n\t\t                                             # block them.\n\t\tself.init_game()\n\t\n\tdef new_stone(self):\n\t\tself.stone = tetris_shapes[rand(len(tetris_shapes))]\n\t\tself.stone_x = int(config['cols'] \/ 2 - len(self.stone[0])\/2)\n\t\tself.stone_y = 0\n\t\t\n\t\tif check_collision(self.board,\n\t\t                   self.stone,\n\t\t                   (self.stone_x, self.stone_y)):\n\t\t\tself.gameover = True\n\t\n\tdef init_game(self):\n\t\tself.board = new_board()\n\t\tself.new_stone()\n\t\n\tdef center_msg(self, msg):\n\t\tfor i, line in enumerate(msg.splitlines()):\n\t\t\tmsg_image =  pygame.font.Font(\n\t\t\t\tpygame.font.get_default_font(), 12).render(\n\t\t\t\t\tline, False, (255,255,255), (0,0,0))\n\t\t\n\t\t\tmsgim_center_x, msgim_center_y = msg_image.get_size()\n\t\t\tmsgim_center_x \/\/= 2\n\t\t\tmsgim_center_y \/\/= 2\n\t\t\n\t\t\tself.screen.blit(msg_image, (\n\t\t\t  self.width \/\/ 2-msgim_center_x,\n\t\t\t  self.height \/\/ 2-msgim_center_y+i*22))\n\t\n\tdef draw_matrix(self, matrix, offset):\n\t\toff_x, off_y  = offset\n\t\tfor y, row in enumerate(matrix):\n\t\t\tfor x, val in enumerate(row):\n\t\t\t\tif val:\n\t\t\t\t\tpygame.draw.rect(\n\t\t\t\t\t\tself.screen,\n\t\t\t\t\t\tcolors[val],\n\t\t\t\t\t\tpygame.Rect(\n\t\t\t\t\t\t\t(off_x+x) *\n\t\t\t\t\t\t\t  config['cell_size'],\n\t\t\t\t\t\t\t(off_y+y) *\n\t\t\t\t\t\t\t  config['cell_size'], \n\t\t\t\t\t\t\tconfig['cell_size'],\n\t\t\t\t\t\t\tconfig['cell_size']),0)\n\t\n\tdef move(self, delta_x):\n\t\tif not self.gameover and not self.paused:\n\t\t\tnew_x = self.stone_x + delta_x\n\t\t\tif new_x < 0:\n\t\t\t\tnew_x = 0\n\t\t\tif new_x > config['cols'] - len(self.stone[0]):\n\t\t\t\tnew_x = config['cols'] - len(self.stone[0])\n\t\t\tif not check_collision(self.board,\n\t\t\t                       self.stone,\n\t\t\t                       (new_x, self.stone_y)):\n\t\t\t\tself.stone_x = new_x\n\tdef quit(self):\n\t\tself.center_msg(\"Exiting...\")\n\t\tpygame.display.update()\n\t\tsys.exit()\n\t\n\tdef drop(self):\n\t\tif not self.gameover and not self.paused:\n\t\t\tself.stone_y += 1\n\t\t\tif check_collision(self.board,\n\t\t\t                   self.stone,\n\t\t\t                   (self.stone_x, self.stone_y)):\n\t\t\t\tself.board = join_matrixes(\n\t\t\t\t  self.board,\n\t\t\t\t  self.stone,\n\t\t\t\t  (self.stone_x, self.stone_y))\n\t\t\t\tself.new_stone()\n\t\t\t\twhile True:\n\t\t\t\t\tfor i, row in enumerate(self.board[:-1]):\n\t\t\t\t\t\tif 0 not in row:\n\t\t\t\t\t\t\tself.board = remove_row(\n\t\t\t\t\t\t\t  self.board, i)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tbreak\n\t\n\tdef rotate_stone(self):\n\t\tif not self.gameover and not self.paused:\n\t\t\tnew_stone = rotate_clockwise(self.stone)\n\t\t\tif not check_collision(self.board,\n\t\t\t                       new_stone,\n\t\t\t                       (self.stone_x, self.stone_y)):\n\t\t\t\tself.stone = new_stone\n\t\n\tdef toggle_pause(self):\n\t\tself.paused = not self.paused\n\t\n\tdef start_game(self):\n\t\tif self.gameover:\n\t\t\tself.init_game()\n\t\t\tself.gameover = False\n\t\n\tdef run(self):\n\t\tkey_actions = {\n\t\t\t'ESCAPE':\tself.quit,\n\t\t\t'LEFT':\t\tlambda:self.move(-1),\n\t\t\t'RIGHT':\tlambda:self.move(+1),\n\t\t\t'DOWN':\t\tself.drop,\n\t\t\t'UP':\t\tself.rotate_stone,\n\t\t\t'p':\t\tself.toggle_pause,\n\t\t\t'SPACE':\tself.start_game\n\t\t}\n\t\t\n\t\tself.gameover = False\n\t\tself.paused = False\n\t\t\n\t\tpygame.time.set_timer(pygame.USEREVENT+1, config['delay'])\n\t\tdont_burn_my_cpu = pygame.time.Clock()\n\t\twhile 1:\n\t\t\tself.screen.fill((0,0,0))\n\t\t\tif self.gameover:\n\t\t\t\tself.center_msg(\"\"\"Game Over!\nPress space to continue\"\"\")\n\t\t\telse:\n\t\t\t\tif self.paused:\n\t\t\t\t\tself.center_msg(\"Paused\")\n\t\t\t\telse:\n\t\t\t\t\tself.draw_matrix(self.board, (0,0))\n\t\t\t\t\tself.draw_matrix(self.stone,\n\t\t\t\t\t                 (self.stone_x,\n\t\t\t\t\t                  self.stone_y))\n\t\t\tpygame.display.update()\n\t\t\t\n\t\t\tfor event in pygame.event.get():\n\t\t\t\tif event.type == pygame.USEREVENT+1:\n\t\t\t\t\tself.drop()\n\t\t\t\telif event.type == pygame.QUIT:\n\t\t\t\t\tself.quit()\n\t\t\t\telif event.type == pygame.KEYDOWN:\n\t\t\t\t\tfor key in key_actions:\n\t\t\t\t\t\tif event.key == eval(\"pygame.K_\"\n\t\t\t\t\t\t+key):\n\t\t\t\t\t\t\tkey_actions[key]()\n\t\t\t\t\t\n\t\t\tdont_burn_my_cpu.tick(config['maxfps'])\n\nif __name__ == '__main__':\n\tApp = TetrisApp()\n\tApp.run()\n","license":"gpl-3.0","hash":-8732550770174458158,"line_mean":25.137037037,"line_max":79,"alpha_frac":0.5964290775,"autogenerated":false},
{"repo_name":"DougBurke\/astropy","path":"astropy\/io\/fits\/hdu\/base.py","copies":"1","size":"58256","content":"# Licensed under a 3-clause BSD style license - see PYFITS.rst\n\n\n\nimport datetime\nimport os\nimport sys\nimport warnings\nfrom contextlib import suppress\nfrom inspect import signature, Parameter\n\nimport numpy as np\n\nfrom .. import conf\nfrom ..file import _File\nfrom ..header import Header, _pad_length\nfrom ..util import (_is_int, _is_pseudo_unsigned, _unsigned_zero,\n                    itersubclasses, decode_ascii, _get_array_mmap, first,\n                    _free_space_check, _extract_number)\nfrom ..verify import _Verify, _ErrList\n\nfrom ....utils import lazyproperty\nfrom ....utils.exceptions import AstropyUserWarning\nfrom ....utils.decorators import deprecated_renamed_argument\n\n\nclass _Delayed:\n    pass\n\n\nDELAYED = _Delayed()\n\n\nBITPIX2DTYPE = {8: 'uint8', 16: 'int16', 32: 'int32', 64: 'int64',\n                -32: 'float32', -64: 'float64'}\n\"\"\"Maps FITS BITPIX values to Numpy dtype names.\"\"\"\n\nDTYPE2BITPIX = {'uint8': 8, 'int16': 16, 'uint16': 16, 'int32': 32,\n                'uint32': 32, 'int64': 64, 'uint64': 64, 'float32': -32,\n                'float64': -64}\n\"\"\"\nMaps Numpy dtype names to FITS BITPIX values (this includes unsigned\nintegers, with the assumption that the pseudo-unsigned integer convention\nwill be used in this case.\n\"\"\"\n\n\nclass InvalidHDUException(Exception):\n    \"\"\"\n    A custom exception class used mainly to signal to _BaseHDU.__new__ that\n    an HDU cannot possibly be considered valid, and must be assumed to be\n    corrupted.\n    \"\"\"\n\n\ndef _hdu_class_from_header(cls, header):\n    \"\"\"\n    Used primarily by _BaseHDU.__new__ to find an appropriate HDU class to use\n    based on values in the header.  See the _BaseHDU.__new__ docstring.\n    \"\"\"\n\n    klass = cls  # By default, if no subclasses are defined\n    if header:\n        for c in reversed(list(itersubclasses(cls))):\n            try:\n                # HDU classes built into astropy.io.fits are always considered,\n                # but extension HDUs must be explicitly registered\n                if not (c.__module__.startswith('astropy.io.fits.') or\n                        c in cls._hdu_registry):\n                    continue\n                if c.match_header(header):\n                    klass = c\n                    break\n            except NotImplementedError:\n                continue\n            except Exception as exc:\n                warnings.warn(\n                    'An exception occurred matching an HDU header to the '\n                    'appropriate HDU type: {0}'.format(exc),\n                    AstropyUserWarning)\n                warnings.warn('The HDU will be treated as corrupted.',\n                              AstropyUserWarning)\n                klass = _CorruptedHDU\n                del exc\n                break\n\n    return klass\n\n\nclass _BaseHDUMeta(type):\n    def __init__(cls, name, bases, members):\n        # The sole purpose of this metaclass right now is to add the same\n        # data.deleter to all HDUs with a data property.\n        # It's unfortunate, but there's otherwise no straightforward way\n        # that a property can inherit setters\/deleters of the property of the\n        # same name on base classes\n        if 'data' in members:\n            data_prop = members['data']\n            if (isinstance(data_prop, (lazyproperty, property)) and\n                    data_prop.fdel is None):\n                # Don't do anything if the class has already explicitly\n                # set the deleter for its data property\n                def data(self):\n                    # The deleter\n                    if self._file is not None and self._data_loaded:\n                        data_refcount = sys.getrefcount(self.data)\n                        # Manually delete *now* so that FITS_rec.__del__\n                        # cleanup can happen if applicable\n                        del self.__dict__['data']\n                        # Don't even do this unless the *only* reference to the\n                        # .data array was the one we're deleting by deleting\n                        # this attribute; if any other references to the array\n                        # are hanging around (perhaps the user ran ``data =\n                        # hdu.data``) don't even consider this:\n                        if data_refcount == 2:\n                            self._file._maybe_close_mmap()\n\n                setattr(cls, 'data', data_prop.deleter(data))\n\n\n# TODO: Come up with a better __repr__ for HDUs (and for HDULists, for that\n# matter)\nclass _BaseHDU(metaclass=_BaseHDUMeta):\n    \"\"\"Base class for all HDU (header data unit) classes.\"\"\"\n\n    _hdu_registry = set()\n\n    # This HDU type is part of the FITS standard\n    _standard = True\n\n    # Byte to use for padding out blocks\n    _padding_byte = '\\x00'\n\n    _default_name = ''\n\n    def __new__(cls, data=None, header=None, *args, **kwargs):\n        \"\"\"\n        Iterates through the subclasses of _BaseHDU and uses that class's\n        match_header() method to determine which subclass to instantiate.\n\n        It's important to be aware that the class hierarchy is traversed in a\n        depth-last order.  Each match_header() should identify an HDU type as\n        uniquely as possible.  Abstract types may choose to simply return False\n        or raise NotImplementedError to be skipped.\n\n        If any unexpected exceptions are raised while evaluating\n        match_header(), the type is taken to be _CorruptedHDU.\n        \"\"\"\n\n        klass = _hdu_class_from_header(cls, header)\n        return super().__new__(klass)\n\n    def __init__(self, data=None, header=None, *args, **kwargs):\n        if header is None:\n            header = Header()\n        self._header = header\n        self._file = None\n        self._buffer = None\n        self._header_offset = None\n        self._data_offset = None\n        self._data_size = None\n\n        # This internal variable is used to track whether the data attribute\n        # still points to the same data array as when the HDU was originally\n        # created (this does not track whether the data is actually the same\n        # content-wise)\n        self._data_replaced = False\n        self._data_needs_rescale = False\n        self._new = True\n        self._output_checksum = False\n\n        if 'DATASUM' in self._header and 'CHECKSUM' not in self._header:\n            self._output_checksum = 'datasum'\n        elif 'CHECKSUM' in self._header:\n            self._output_checksum = True\n\n    @property\n    def header(self):\n        return self._header\n\n    @header.setter\n    def header(self, value):\n        self._header = value\n\n    @property\n    def name(self):\n        # Convert the value to a string to be flexible in some pathological\n        # cases (see ticket #96)\n        return str(self._header.get('EXTNAME', self._default_name))\n\n    @name.setter\n    def name(self, value):\n        if not isinstance(value, str):\n            raise TypeError(\"'name' attribute must be a string\")\n        if not conf.extension_name_case_sensitive:\n            value = value.upper()\n        if 'EXTNAME' in self._header:\n            self._header['EXTNAME'] = value\n        else:\n            self._header['EXTNAME'] = (value, 'extension name')\n\n    @property\n    def ver(self):\n        return self._header.get('EXTVER', 1)\n\n    @ver.setter\n    def ver(self, value):\n        if not _is_int(value):\n            raise TypeError(\"'ver' attribute must be an integer\")\n        if 'EXTVER' in self._header:\n            self._header['EXTVER'] = value\n        else:\n            self._header['EXTVER'] = (value, 'extension value')\n\n    @property\n    def level(self):\n        return self._header.get('EXTLEVEL', 1)\n\n    @level.setter\n    def level(self, value):\n        if not _is_int(value):\n            raise TypeError(\"'level' attribute must be an integer\")\n        if 'EXTLEVEL' in self._header:\n            self._header['EXTLEVEL'] = value\n        else:\n            self._header['EXTLEVEL'] = (value, 'extension level')\n\n    @property\n    def is_image(self):\n        return (\n            self.name == 'PRIMARY' or\n            ('XTENSION' in self._header and\n             (self._header['XTENSION'] == 'IMAGE' or\n              (self._header['XTENSION'] == 'BINTABLE' and\n               'ZIMAGE' in self._header and self._header['ZIMAGE'] is True))))\n\n    @property\n    def _data_loaded(self):\n        return ('data' in self.__dict__ and self.data is not DELAYED)\n\n    @property\n    def _has_data(self):\n        return self._data_loaded and self.data is not None\n\n    @classmethod\n    def register_hdu(cls, hducls):\n        cls._hdu_registry.add(hducls)\n\n    @classmethod\n    def unregister_hdu(cls, hducls):\n        if hducls in cls._hdu_registry:\n            cls._hdu_registry.remove(hducls)\n\n    @classmethod\n    def match_header(cls, header):\n        raise NotImplementedError\n\n    @classmethod\n    def fromstring(cls, data, checksum=False, ignore_missing_end=False,\n                   **kwargs):\n        \"\"\"\n        Creates a new HDU object of the appropriate type from a string\n        containing the HDU's entire header and, optionally, its data.\n\n        Note: When creating a new HDU from a string without a backing file\n        object, the data of that HDU may be read-only.  It depends on whether\n        the underlying string was an immutable Python str\/bytes object, or some\n        kind of read-write memory buffer such as a `memoryview`.\n\n        Parameters\n        ----------\n        data : str, bytearray, memoryview, ndarray\n           A byte string containing the HDU's header and data.\n\n        checksum : bool, optional\n           Check the HDU's checksum and\/or datasum.\n\n        ignore_missing_end : bool, optional\n           Ignore a missing end card in the header data.  Note that without the\n           end card the end of the header may be ambiguous and resulted in a\n           corrupt HDU.  In this case the assumption is that the first 2880\n           block that does not begin with valid FITS header data is the\n           beginning of the data.\n\n        kwargs : optional\n           May consist of additional keyword arguments specific to an HDU\n           type--these correspond to keywords recognized by the constructors of\n           different HDU classes such as `PrimaryHDU`, `ImageHDU`, or\n           `BinTableHDU`.  Any unrecognized keyword arguments are simply\n           ignored.\n        \"\"\"\n\n        return cls._readfrom_internal(data, checksum=checksum,\n                                      ignore_missing_end=ignore_missing_end,\n                                      **kwargs)\n\n    @classmethod\n    def readfrom(cls, fileobj, checksum=False, ignore_missing_end=False,\n                 **kwargs):\n        \"\"\"\n        Read the HDU from a file.  Normally an HDU should be opened with\n        :func:`open` which reads the entire HDU list in a FITS file.  But this\n        method is still provided for symmetry with :func:`writeto`.\n\n        Parameters\n        ----------\n        fileobj : file object or file-like object\n            Input FITS file.  The file's seek pointer is assumed to be at the\n            beginning of the HDU.\n\n        checksum : bool\n            If `True`, verifies that both ``DATASUM`` and ``CHECKSUM`` card\n            values (when present in the HDU header) match the header and data\n            of all HDU's in the file.\n\n        ignore_missing_end : bool\n            Do not issue an exception when opening a file that is missing an\n            ``END`` card in the last header.\n        \"\"\"\n\n        # TODO: Figure out a way to make it possible for the _File\n        # constructor to be a noop if the argument is already a _File\n        if not isinstance(fileobj, _File):\n            fileobj = _File(fileobj)\n\n        hdu = cls._readfrom_internal(fileobj, checksum=checksum,\n                                     ignore_missing_end=ignore_missing_end,\n                                     **kwargs)\n\n        # If the checksum had to be checked the data may have already been read\n        # from the file, in which case we don't want to seek relative\n        fileobj.seek(hdu._data_offset + hdu._data_size, os.SEEK_SET)\n        return hdu\n\n    @deprecated_renamed_argument('clobber', 'overwrite', '2.0')\n    def writeto(self, name, output_verify='exception', overwrite=False,\n                checksum=False):\n        \"\"\"\n        Write the HDU to a new file. This is a convenience method to\n        provide a user easier output interface if only one HDU needs\n        to be written to a file.\n\n        Parameters\n        ----------\n        name : file path, file object or file-like object\n            Output FITS file.  If the file object is already opened, it must\n            be opened in a writeable mode.\n\n        output_verify : str\n            Output verification option.  Must be one of ``\"fix\"``,\n            ``\"silentfix\"``, ``\"ignore\"``, ``\"warn\"``, or\n            ``\"exception\"``.  May also be any combination of ``\"fix\"`` or\n            ``\"silentfix\"`` with ``\"+ignore\"``, ``+warn``, or ``+exception\"\n            (e.g. ``\"fix+warn\"``).  See :ref:`verify` for more info.\n\n        overwrite : bool, optional\n            If ``True``, overwrite the output file if it exists. Raises an\n            ``OSError`` if ``False`` and the output file exists. Default is\n            ``False``.\n\n            .. versionchanged:: 1.3\n               ``overwrite`` replaces the deprecated ``clobber`` argument.\n\n        checksum : bool\n            When `True` adds both ``DATASUM`` and ``CHECKSUM`` cards\n            to the header of the HDU when written to the file.\n        \"\"\"\n\n        from .hdulist import HDUList\n\n        hdulist = HDUList([self])\n        hdulist.writeto(name, output_verify, overwrite=overwrite,\n                        checksum=checksum)\n\n    @classmethod\n    def _readfrom_internal(cls, data, header=None, checksum=False,\n                           ignore_missing_end=False, **kwargs):\n        \"\"\"\n        Provides the bulk of the internal implementation for readfrom and\n        fromstring.\n\n        For some special cases, supports using a header that was already\n        created, and just using the input data for the actual array data.\n        \"\"\"\n\n        hdu_buffer = None\n        hdu_fileobj = None\n        header_offset = 0\n\n        if isinstance(data, _File):\n            if header is None:\n                header_offset = data.tell()\n                header = Header.fromfile(data, endcard=not ignore_missing_end)\n            hdu_fileobj = data\n            data_offset = data.tell()  # *after* reading the header\n        else:\n            try:\n                # Test that the given object supports the buffer interface by\n                # ensuring an ndarray can be created from it\n                np.ndarray((), dtype='ubyte', buffer=data)\n            except TypeError:\n                raise TypeError(\n                    'The provided object {!r} does not contain an underlying '\n                    'memory buffer.  fromstring() requires an object that '\n                    'supports the buffer interface such as bytes, buffer, '\n                    'memoryview, ndarray, etc.  This restriction is to ensure '\n                    'that efficient access to the array\/table data is possible.'\n                    .format(data))\n\n            if header is None:\n                def block_iter(nbytes):\n                    idx = 0\n                    while idx < len(data):\n                        yield data[idx:idx + nbytes]\n                        idx += nbytes\n\n                header_str, header = Header._from_blocks(\n                    block_iter, True, '', not ignore_missing_end, True)\n\n                if len(data) > len(header_str):\n                    hdu_buffer = data\n            elif data:\n                hdu_buffer = data\n\n            header_offset = 0\n            data_offset = len(header_str)\n\n        # Determine the appropriate arguments to pass to the constructor from\n        # self._kwargs.  self._kwargs contains any number of optional arguments\n        # that may or may not be valid depending on the HDU type\n        cls = _hdu_class_from_header(cls, header)\n        sig = signature(cls.__init__)\n        new_kwargs = kwargs.copy()\n        if Parameter.VAR_KEYWORD not in (x.kind for x in sig.parameters.values()):\n            # If __init__ accepts arbitrary keyword arguments, then we can go\n            # ahead and pass all keyword arguments; otherwise we need to delete\n            # any that are invalid\n            for key in kwargs:\n                if key not in sig.parameters:\n                    del new_kwargs[key]\n\n        hdu = cls(data=DELAYED, header=header, **new_kwargs)\n\n        # One of these may be None, depending on whether the data came from a\n        # file or a string buffer--later this will be further abstracted\n        hdu._file = hdu_fileobj\n        hdu._buffer = hdu_buffer\n\n        hdu._header_offset = header_offset     # beginning of the header area\n        hdu._data_offset = data_offset         # beginning of the data area\n\n        # data area size, including padding\n        size = hdu.size\n        hdu._data_size = size + _pad_length(size)\n\n        # Checksums are not checked on invalid HDU types\n        if checksum and checksum != 'remove' and isinstance(hdu, _ValidHDU):\n            hdu._verify_checksum_datasum()\n\n        return hdu\n\n    def _get_raw_data(self, shape, code, offset):\n        \"\"\"\n        Return raw array from either the HDU's memory buffer or underlying\n        file.\n        \"\"\"\n\n        if isinstance(shape, int):\n            shape = (shape,)\n\n        if self._buffer:\n            return np.ndarray(shape, dtype=code, buffer=self._buffer,\n                              offset=offset)\n        elif self._file:\n            return self._file.readarray(offset=offset, dtype=code, shape=shape)\n        else:\n            return None\n\n    # TODO: Rework checksum handling so that it's not necessary to add a\n    # checksum argument here\n    # TODO: The BaseHDU class shouldn't even handle checksums since they're\n    # only implemented on _ValidHDU...\n    def _prewriteto(self, checksum=False, inplace=False):\n        self._update_uint_scale_keywords()\n\n        # Handle checksum\n        self._update_checksum(checksum)\n\n    def _update_uint_scale_keywords(self):\n        \"\"\"\n        If the data is unsigned int 16, 32, or 64 add BSCALE\/BZERO cards to\n        header.\n        \"\"\"\n\n        if (self._has_data and self._standard and\n                _is_pseudo_unsigned(self.data.dtype)):\n            # CompImageHDUs need TFIELDS immediately after GCOUNT,\n            # so BSCALE has to go after TFIELDS if it exists.\n            if 'TFIELDS' in self._header:\n                self._header.set('BSCALE', 1, after='TFIELDS')\n            elif 'GCOUNT' in self._header:\n                self._header.set('BSCALE', 1, after='GCOUNT')\n            else:\n                self._header.set('BSCALE', 1)\n            self._header.set('BZERO', _unsigned_zero(self.data.dtype),\n                             after='BSCALE')\n\n    def _update_checksum(self, checksum, checksum_keyword='CHECKSUM',\n                         datasum_keyword='DATASUM'):\n        \"\"\"Update the 'CHECKSUM' and 'DATASUM' keywords in the header (or\n        keywords with equivalent semantics given by the ``checksum_keyword``\n        and ``datasum_keyword`` arguments--see for example ``CompImageHDU``\n        for an example of why this might need to be overridden).\n        \"\"\"\n\n        # If the data is loaded it isn't necessarily 'modified', but we have no\n        # way of knowing for sure\n        modified = self._header._modified or self._data_loaded\n\n        if checksum == 'remove':\n            if checksum_keyword in self._header:\n                del self._header[checksum_keyword]\n\n            if datasum_keyword in self._header:\n                del self._header[datasum_keyword]\n        elif (modified or self._new or\n                (checksum and ('CHECKSUM' not in self._header or\n                               'DATASUM' not in self._header or\n                               not self._checksum_valid or\n                               not self._datasum_valid))):\n            if checksum == 'datasum':\n                self.add_datasum(datasum_keyword=datasum_keyword)\n            elif checksum:\n                self.add_checksum(checksum_keyword=checksum_keyword,\n                                  datasum_keyword=datasum_keyword)\n\n    def _postwriteto(self):\n        # If data is unsigned integer 16, 32 or 64, remove the\n        # BSCALE\/BZERO cards\n        if (self._has_data and self._standard and\n                _is_pseudo_unsigned(self.data.dtype)):\n            for keyword in ('BSCALE', 'BZERO'):\n                with suppress(KeyError):\n                    del self._header[keyword]\n\n    def _writeheader(self, fileobj):\n        offset = 0\n        if not fileobj.simulateonly:\n            with suppress(AttributeError, OSError):\n                offset = fileobj.tell()\n\n            self._header.tofile(fileobj)\n\n            try:\n                size = fileobj.tell() - offset\n            except (AttributeError, OSError):\n                size = len(str(self._header))\n        else:\n            size = len(str(self._header))\n\n        return offset, size\n\n    def _writedata(self, fileobj):\n        # TODO: A lot of the simulateonly stuff should be moved back into the\n        # _File class--basically it should turn write and flush into a noop\n        offset = 0\n        size = 0\n\n        if not fileobj.simulateonly:\n            fileobj.flush()\n            try:\n                offset = fileobj.tell()\n            except OSError:\n                offset = 0\n\n        if self._data_loaded or self._data_needs_rescale:\n            if self.data is not None:\n                size += self._writedata_internal(fileobj)\n            # pad the FITS data block\n            if size > 0:\n                padding = _pad_length(size) * self._padding_byte\n                # TODO: Not that this is ever likely, but if for some odd\n                # reason _padding_byte is > 0x80 this will fail; but really if\n                # somebody's custom fits format is doing that, they're doing it\n                # wrong and should be reprimanded harshly.\n                fileobj.write(padding.encode('ascii'))\n                size += len(padding)\n        else:\n            # The data has not been modified or does not need need to be\n            # rescaled, so it can be copied, unmodified, directly from an\n            # existing file or buffer\n            size += self._writedata_direct_copy(fileobj)\n\n        # flush, to make sure the content is written\n        if not fileobj.simulateonly:\n            fileobj.flush()\n\n        # return both the location and the size of the data area\n        return offset, size\n\n    def _writedata_internal(self, fileobj):\n        \"\"\"\n        The beginning and end of most _writedata() implementations are the\n        same, but the details of writing the data array itself can vary between\n        HDU types, so that should be implemented in this method.\n\n        Should return the size in bytes of the data written.\n        \"\"\"\n\n        if not fileobj.simulateonly:\n            fileobj.writearray(self.data)\n        return self.data.size * self.data.itemsize\n\n    def _writedata_direct_copy(self, fileobj):\n        \"\"\"Copies the data directly from one file\/buffer to the new file.\n\n        For now this is handled by loading the raw data from the existing data\n        (including any padding) via a memory map or from an already in-memory\n        buffer and using Numpy's existing file-writing facilities to write to\n        the new file.\n\n        If this proves too slow a more direct approach may be used.\n        \"\"\"\n        raw = self._get_raw_data(self._data_size, 'ubyte', self._data_offset)\n        if raw is not None:\n            fileobj.writearray(raw)\n            return raw.nbytes\n        else:\n            return 0\n\n    # TODO: This is the start of moving HDU writing out of the _File class;\n    # Though right now this is an internal private method (though still used by\n    # HDUList, eventually the plan is to have this be moved into writeto()\n    # somehow...\n    def _writeto(self, fileobj, inplace=False, copy=False):\n        try:\n            dirname = os.path.dirname(fileobj._file.name)\n        except AttributeError:\n            dirname = None\n\n        with _free_space_check(self, dirname):\n            self._writeto_internal(fileobj, inplace, copy)\n\n    def _writeto_internal(self, fileobj, inplace, copy):\n        # For now fileobj is assumed to be a _File object\n        if not inplace or self._new:\n            header_offset, _ = self._writeheader(fileobj)\n            data_offset, data_size = self._writedata(fileobj)\n\n            # Set the various data location attributes on newly-written HDUs\n            if self._new:\n                self._header_offset = header_offset\n                self._data_offset = data_offset\n                self._data_size = data_size\n            return\n\n        hdrloc = self._header_offset\n        hdrsize = self._data_offset - self._header_offset\n        datloc = self._data_offset\n        datsize = self._data_size\n\n        if self._header._modified:\n            # Seek to the original header location in the file\n            self._file.seek(hdrloc)\n            # This should update hdrloc with he header location in the new file\n            hdrloc, hdrsize = self._writeheader(fileobj)\n\n            # If the data is to be written below with self._writedata, that\n            # will also properly update the data location; but it should be\n            # updated here too\n            datloc = hdrloc + hdrsize\n        elif copy:\n            # Seek to the original header location in the file\n            self._file.seek(hdrloc)\n            # Before writing, update the hdrloc with the current file position,\n            # which is the hdrloc for the new file\n            hdrloc = fileobj.tell()\n            fileobj.write(self._file.read(hdrsize))\n            # The header size is unchanged, but the data location may be\n            # different from before depending on if previous HDUs were resized\n            datloc = fileobj.tell()\n\n        if self._data_loaded:\n            if self.data is not None:\n                # Seek through the array's bases for an memmap'd array; we\n                # can't rely on the _File object to give us this info since\n                # the user may have replaced the previous mmap'd array\n                if copy or self._data_replaced:\n                    # Of course, if we're copying the data to a new file\n                    # we don't care about flushing the original mmap;\n                    # instead just read it into the new file\n                    array_mmap = None\n                else:\n                    array_mmap = _get_array_mmap(self.data)\n\n                if array_mmap is not None:\n                    array_mmap.flush()\n                else:\n                    self._file.seek(self._data_offset)\n                    datloc, datsize = self._writedata(fileobj)\n        elif copy:\n            datsize = self._writedata_direct_copy(fileobj)\n\n        self._header_offset = hdrloc\n        self._data_offset = datloc\n        self._data_size = datsize\n        self._data_replaced = False\n\n    def _close(self, closed=True):\n        # If the data was mmap'd, close the underlying mmap (this will\n        # prevent any future access to the .data attribute if there are\n        # not other references to it; if there are other references then\n        # it is up to the user to clean those up\n        if (closed and self._data_loaded and\n                _get_array_mmap(self.data) is not None):\n            del self.data\n\n\n# For backwards-compatibility, though nobody should have\n# been using this directly:\n_AllHDU = _BaseHDU\n\n# For convenience...\n# TODO: register_hdu could be made into a class decorator which would be pretty\n# cool, but only once 2.6 support is dropped.\nregister_hdu = _BaseHDU.register_hdu\nunregister_hdu = _BaseHDU.unregister_hdu\n\n\nclass _CorruptedHDU(_BaseHDU):\n    \"\"\"\n    A Corrupted HDU class.\n\n    This class is used when one or more mandatory `Card`s are\n    corrupted (unparsable), such as the ``BITPIX``, ``NAXIS``, or\n    ``END`` cards.  A corrupted HDU usually means that the data size\n    cannot be calculated or the ``END`` card is not found.  In the case\n    of a missing ``END`` card, the `Header` may also contain the binary\n    data\n\n    .. note::\n       In future, it may be possible to decipher where the last block\n       of the `Header` ends, but this task may be difficult when the\n       extension is a `TableHDU` containing ASCII data.\n    \"\"\"\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the size (in bytes) of the HDU's data part.\n        \"\"\"\n\n        # Note: On compressed files this might report a negative size; but the\n        # file is corrupt anyways so I'm not too worried about it.\n        if self._buffer is not None:\n            return len(self._buffer) - self._data_offset\n\n        return self._file.size - self._data_offset\n\n    def _summary(self):\n        return (self.name, self.ver, 'CorruptedHDU')\n\n    def verify(self):\n        pass\n\n\nclass _NonstandardHDU(_BaseHDU, _Verify):\n    \"\"\"\n    A Non-standard HDU class.\n\n    This class is used for a Primary HDU when the ``SIMPLE`` Card has\n    a value of `False`.  A non-standard HDU comes from a file that\n    resembles a FITS file but departs from the standards in some\n    significant way.  One example would be files where the numbers are\n    in the DEC VAX internal storage format rather than the standard\n    FITS most significant byte first.  The header for this HDU should\n    be valid.  The data for this HDU is read from the file as a byte\n    stream that begins at the first byte after the header ``END`` card\n    and continues until the end of the file.\n    \"\"\"\n\n    _standard = False\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        Matches any HDU that has the 'SIMPLE' keyword but is not a standard\n        Primary or Groups HDU.\n        \"\"\"\n\n        # The SIMPLE keyword must be in the first card\n        card = header.cards[0]\n\n        # The check that 'GROUPS' is missing is a bit redundant, since the\n        # match_header for GroupsHDU will always be called before this one.\n        if card.keyword == 'SIMPLE':\n            if 'GROUPS' not in header and card.value is False:\n                return True\n            else:\n                raise InvalidHDUException\n        else:\n            return False\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the size (in bytes) of the HDU's data part.\n        \"\"\"\n\n        if self._buffer is not None:\n            return len(self._buffer) - self._data_offset\n\n        return self._file.size - self._data_offset\n\n    def _writedata(self, fileobj):\n        \"\"\"\n        Differs from the base class :class:`_writedata` in that it doesn't\n        automatically add padding, and treats the data as a string of raw bytes\n        instead of an array.\n        \"\"\"\n\n        offset = 0\n        size = 0\n\n        if not fileobj.simulateonly:\n            fileobj.flush()\n            try:\n                offset = fileobj.tell()\n            except OSError:\n                offset = 0\n\n        if self.data is not None:\n            if not fileobj.simulateonly:\n                fileobj.write(self.data)\n                # flush, to make sure the content is written\n                fileobj.flush()\n                size = len(self.data)\n\n        # return both the location and the size of the data area\n        return offset, size\n\n    def _summary(self):\n        return (self.name, self.ver, 'NonstandardHDU', len(self._header))\n\n    @lazyproperty\n    def data(self):\n        \"\"\"\n        Return the file data.\n        \"\"\"\n\n        return self._get_raw_data(self.size, 'ubyte', self._data_offset)\n\n    def _verify(self, option='warn'):\n        errs = _ErrList([], unit='Card')\n\n        # verify each card\n        for card in self._header.cards:\n            errs.append(card._verify(option))\n\n        return errs\n\n\nclass _ValidHDU(_BaseHDU, _Verify):\n    \"\"\"\n    Base class for all HDUs which are not corrupted.\n    \"\"\"\n\n    def __init__(self, data=None, header=None, name=None, ver=None, **kwargs):\n        super().__init__(data=data, header=header)\n\n        # NOTE:  private data members _checksum and _datasum are used by the\n        # utility script \"fitscheck\" to detect missing checksums.\n        self._checksum = None\n        self._checksum_valid = None\n        self._datasum = None\n        self._datasum_valid = None\n\n        if name is not None:\n            self.name = name\n        if ver is not None:\n            self.ver = ver\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        Matches any HDU that is not recognized as having either the SIMPLE or\n        XTENSION keyword in its header's first card, but is nonetheless not\n        corrupted.\n\n        TODO: Maybe it would make more sense to use _NonstandardHDU in this\n        case?  Not sure...\n        \"\"\"\n\n        return first(header.keys()) not in ('SIMPLE', 'XTENSION')\n\n    @property\n    def size(self):\n        \"\"\"\n        Size (in bytes) of the data portion of the HDU.\n        \"\"\"\n\n        size = 0\n        naxis = self._header.get('NAXIS', 0)\n        if naxis > 0:\n            size = 1\n            for idx in range(naxis):\n                size = size * self._header['NAXIS' + str(idx + 1)]\n            bitpix = self._header['BITPIX']\n            gcount = self._header.get('GCOUNT', 1)\n            pcount = self._header.get('PCOUNT', 0)\n            size = abs(bitpix) * gcount * (pcount + size) \/\/ 8\n        return size\n\n    def filebytes(self):\n        \"\"\"\n        Calculates and returns the number of bytes that this HDU will write to\n        a file.\n        \"\"\"\n\n        f = _File()\n        # TODO: Fix this once new HDU writing API is settled on\n        return self._writeheader(f)[1] + self._writedata(f)[1]\n\n    def fileinfo(self):\n        \"\"\"\n        Returns a dictionary detailing information about the locations\n        of this HDU within any associated file.  The values are only\n        valid after a read or write of the associated file with no\n        intervening changes to the `HDUList`.\n\n        Returns\n        -------\n        dict or None\n\n           The dictionary details information about the locations of\n           this HDU within an associated file.  Returns `None` when\n           the HDU is not associated with a file.\n\n           Dictionary contents:\n\n           ========== ================================================\n           Key        Value\n           ========== ================================================\n           file       File object associated with the HDU\n           filemode   Mode in which the file was opened (readonly, copyonwrite,\n                      update, append, ostream)\n           hdrLoc     Starting byte location of header in file\n           datLoc     Starting byte location of data block in file\n           datSpan    Data size including padding\n           ========== ================================================\n        \"\"\"\n\n        if hasattr(self, '_file') and self._file:\n            return {'file': self._file, 'filemode': self._file.mode,\n                    'hdrLoc': self._header_offset, 'datLoc': self._data_offset,\n                    'datSpan': self._data_size}\n        else:\n            return None\n\n    def copy(self):\n        \"\"\"\n        Make a copy of the HDU, both header and data are copied.\n        \"\"\"\n\n        if self.data is not None:\n            data = self.data.copy()\n        else:\n            data = None\n        return self.__class__(data=data, header=self._header.copy())\n\n    def _verify(self, option='warn'):\n        errs = _ErrList([], unit='Card')\n\n        is_valid = BITPIX2DTYPE.__contains__\n\n        # Verify location and value of mandatory keywords.\n        # Do the first card here, instead of in the respective HDU classes, so\n        # the checking is in order, in case of required cards in wrong order.\n        if isinstance(self, ExtensionHDU):\n            firstkey = 'XTENSION'\n            firstval = self._extension\n        else:\n            firstkey = 'SIMPLE'\n            firstval = True\n\n        self.req_cards(firstkey, 0, None, firstval, option, errs)\n        self.req_cards('BITPIX', 1, lambda v: (_is_int(v) and is_valid(v)), 8,\n                       option, errs)\n        self.req_cards('NAXIS', 2,\n                       lambda v: (_is_int(v) and 0 <= v <= 999), 0,\n                       option, errs)\n\n        naxis = self._header.get('NAXIS', 0)\n        if naxis < 1000:\n            for ax in range(3, naxis + 3):\n                key = 'NAXIS' + str(ax - 2)\n                self.req_cards(key, ax,\n                               lambda v: (_is_int(v) and v >= 0),\n                               _extract_number(self._header[key], default=1),\n                               option, errs)\n\n            # Remove NAXISj cards where j is not in range 1, naxis inclusive.\n            for keyword in self._header:\n                if keyword.startswith('NAXIS') and len(keyword) > 5:\n                    try:\n                        number = int(keyword[5:])\n                        if number <= 0 or number > naxis:\n                            raise ValueError\n                    except ValueError:\n                        err_text = (\"NAXISj keyword out of range ('{}' when \"\n                                    \"NAXIS == {})\".format(keyword, naxis))\n\n                        def fix(self=self, keyword=keyword):\n                            del self._header[keyword]\n\n                        errs.append(\n                            self.run_option(option=option, err_text=err_text,\n                                            fix=fix, fix_text=\"Deleted.\"))\n\n        # Verify that the EXTNAME keyword exists and is a string\n        if 'EXTNAME' in self._header:\n            if not isinstance(self._header['EXTNAME'], str):\n                err_text = 'The EXTNAME keyword must have a string value.'\n                fix_text = 'Converted the EXTNAME keyword to a string value.'\n\n                def fix(header=self._header):\n                    header['EXTNAME'] = str(header['EXTNAME'])\n\n                errs.append(self.run_option(option, err_text=err_text,\n                                            fix_text=fix_text, fix=fix))\n\n        # verify each card\n        for card in self._header.cards:\n            errs.append(card._verify(option))\n\n        return errs\n\n    # TODO: Improve this API a little bit--for one, most of these arguments\n    # could be optional\n    def req_cards(self, keyword, pos, test, fix_value, option, errlist):\n        \"\"\"\n        Check the existence, location, and value of a required `Card`.\n\n        Parameters\n        ----------\n        keyword : str\n            The keyword to validate\n\n        pos : int, callable\n            If an ``int``, this specifies the exact location this card should\n            have in the header.  Remember that Python is zero-indexed, so this\n            means ``pos=0`` requires the card to be the first card in the\n            header.  If given a callable, it should take one argument--the\n            actual position of the keyword--and return `True` or `False`.  This\n            can be used for custom evaluation.  For example if\n            ``pos=lambda idx: idx > 10`` this will check that the keyword's\n            index is greater than 10.\n\n        test : callable\n            This should be a callable (generally a function) that is passed the\n            value of the given keyword and returns `True` or `False`.  This can\n            be used to validate the value associated with the given keyword.\n\n        fix_value : str, int, float, complex, bool, None\n            A valid value for a FITS keyword to to use if the given ``test``\n            fails to replace an invalid value.  In other words, this provides\n            a default value to use as a replacement if the keyword's current\n            value is invalid.  If `None`, there is no replacement value and the\n            keyword is unfixable.\n\n        option : str\n            Output verification option.  Must be one of ``\"fix\"``,\n            ``\"silentfix\"``, ``\"ignore\"``, ``\"warn\"``, or\n            ``\"exception\"``.  May also be any combination of ``\"fix\"`` or\n            ``\"silentfix\"`` with ``\"+ignore\"``, ``+warn``, or ``+exception\"\n            (e.g. ``\"fix+warn\"``).  See :ref:`verify` for more info.\n\n        errlist : list\n            A list of validation errors already found in the FITS file; this is\n            used primarily for the validation system to collect errors across\n            multiple HDUs and multiple calls to `req_cards`.\n\n        Notes\n        -----\n        If ``pos=None``, the card can be anywhere in the header.  If the card\n        does not exist, the new card will have the ``fix_value`` as its value\n        when created.  Also check the card's value by using the ``test``\n        argument.\n        \"\"\"\n\n        errs = errlist\n        fix = None\n\n        try:\n            index = self._header.index(keyword)\n        except ValueError:\n            index = None\n\n        fixable = fix_value is not None\n\n        insert_pos = len(self._header) + 1\n\n        # If pos is an int, insert at the given position (and convert it to a\n        # lambda)\n        if _is_int(pos):\n            insert_pos = pos\n            pos = lambda x: x == insert_pos\n\n        # if the card does not exist\n        if index is None:\n            err_text = \"'{}' card does not exist.\".format(keyword)\n            fix_text = \"Fixed by inserting a new '{}' card.\".format(keyword)\n            if fixable:\n                # use repr to accommodate both string and non-string types\n                # Boolean is also OK in this constructor\n                card = (keyword, fix_value)\n\n                def fix(self=self, insert_pos=insert_pos, card=card):\n                    self._header.insert(insert_pos, card)\n\n            errs.append(self.run_option(option, err_text=err_text,\n                        fix_text=fix_text, fix=fix, fixable=fixable))\n        else:\n            # if the supposed location is specified\n            if pos is not None:\n                if not pos(index):\n                    err_text = (\"'{}' card at the wrong place \"\n                                \"(card {}).\".format(keyword, index))\n                    fix_text = (\"Fixed by moving it to the right place \"\n                                \"(card {}).\".format(insert_pos))\n\n                    def fix(self=self, index=index, insert_pos=insert_pos):\n                        card = self._header.cards[index]\n                        del self._header[index]\n                        self._header.insert(insert_pos, card)\n\n                    errs.append(self.run_option(option, err_text=err_text,\n                                fix_text=fix_text, fix=fix))\n\n            # if value checking is specified\n            if test:\n                val = self._header[keyword]\n                if not test(val):\n                    err_text = (\"'{}' card has invalid value '{}'.\".format(\n                            keyword, val))\n                    fix_text = (\"Fixed by setting a new value '{}'.\".format(\n                            fix_value))\n\n                    if fixable:\n                        def fix(self=self, keyword=keyword, val=fix_value):\n                            self._header[keyword] = fix_value\n\n                    errs.append(self.run_option(option, err_text=err_text,\n                                fix_text=fix_text, fix=fix, fixable=fixable))\n\n        return errs\n\n    def add_datasum(self, when=None, datasum_keyword='DATASUM'):\n        \"\"\"\n        Add the ``DATASUM`` card to this HDU with the value set to the\n        checksum calculated for the data.\n\n        Parameters\n        ----------\n        when : str, optional\n            Comment string for the card that by default represents the\n            time when the checksum was calculated\n\n        datasum_keyword : str, optional\n            The name of the header keyword to store the datasum value in;\n            this is typically 'DATASUM' per convention, but there exist\n            use cases in which a different keyword should be used\n\n        Returns\n        -------\n        checksum : int\n            The calculated datasum\n\n        Notes\n        -----\n        For testing purposes, provide a ``when`` argument to enable the comment\n        value in the card to remain consistent.  This will enable the\n        generation of a ``CHECKSUM`` card with a consistent value.\n        \"\"\"\n\n        cs = self._calculate_datasum()\n\n        if when is None:\n            when = 'data unit checksum updated {}'.format(self._get_timestamp())\n\n        self._header[datasum_keyword] = (str(cs), when)\n        return cs\n\n    def add_checksum(self, when=None, override_datasum=False,\n                     checksum_keyword='CHECKSUM', datasum_keyword='DATASUM'):\n        \"\"\"\n        Add the ``CHECKSUM`` and ``DATASUM`` cards to this HDU with\n        the values set to the checksum calculated for the HDU and the\n        data respectively.  The addition of the ``DATASUM`` card may\n        be overridden.\n\n        Parameters\n        ----------\n        when : str, optional\n           comment string for the cards; by default the comments\n           will represent the time when the checksum was calculated\n\n        override_datasum : bool, optional\n           add the ``CHECKSUM`` card only\n\n        checksum_keyword : str, optional\n            The name of the header keyword to store the checksum value in; this\n            is typically 'CHECKSUM' per convention, but there exist use cases\n            in which a different keyword should be used\n\n        datasum_keyword : str, optional\n            See ``checksum_keyword``\n\n        Notes\n        -----\n        For testing purposes, first call `add_datasum` with a ``when``\n        argument, then call `add_checksum` with a ``when`` argument and\n        ``override_datasum`` set to `True`.  This will provide consistent\n        comments for both cards and enable the generation of a ``CHECKSUM``\n        card with a consistent value.\n        \"\"\"\n\n        if not override_datasum:\n            # Calculate and add the data checksum to the header.\n            data_cs = self.add_datasum(when, datasum_keyword=datasum_keyword)\n        else:\n            # Just calculate the data checksum\n            data_cs = self._calculate_datasum()\n\n        if when is None:\n            when = 'HDU checksum updated {}'.format(self._get_timestamp())\n\n        # Add the CHECKSUM card to the header with a value of all zeros.\n        if datasum_keyword in self._header:\n            self._header.set(checksum_keyword, '0' * 16, when,\n                             before=datasum_keyword)\n        else:\n            self._header.set(checksum_keyword, '0' * 16, when)\n\n        csum = self._calculate_checksum(data_cs,\n                                        checksum_keyword=checksum_keyword)\n        self._header[checksum_keyword] = csum\n\n    def verify_datasum(self):\n        \"\"\"\n        Verify that the value in the ``DATASUM`` keyword matches the value\n        calculated for the ``DATASUM`` of the current HDU data.\n\n        Returns\n        -------\n        valid : int\n           - 0 - failure\n           - 1 - success\n           - 2 - no ``DATASUM`` keyword present\n        \"\"\"\n\n        if 'DATASUM' in self._header:\n            datasum = self._calculate_datasum()\n            if datasum == int(self._header['DATASUM']):\n                return 1\n            else:\n                # Failed\n                return 0\n        else:\n            return 2\n\n    def verify_checksum(self):\n        \"\"\"\n        Verify that the value in the ``CHECKSUM`` keyword matches the\n        value calculated for the current HDU CHECKSUM.\n\n        Returns\n        -------\n        valid : int\n           - 0 - failure\n           - 1 - success\n           - 2 - no ``CHECKSUM`` keyword present\n        \"\"\"\n\n        if 'CHECKSUM' in self._header:\n            if 'DATASUM' in self._header:\n                datasum = self._calculate_datasum()\n            else:\n                datasum = 0\n            checksum = self._calculate_checksum(datasum)\n            if checksum == self._header['CHECKSUM']:\n                return 1\n            else:\n                # Failed\n                return 0\n        else:\n            return 2\n\n    def _verify_checksum_datasum(self):\n        \"\"\"\n        Verify the checksum\/datasum values if the cards exist in the header.\n        Simply displays warnings if either the checksum or datasum don't match.\n        \"\"\"\n\n        if 'CHECKSUM' in self._header:\n            self._checksum = self._header['CHECKSUM']\n            self._checksum_valid = self.verify_checksum()\n            if not self._checksum_valid:\n                warnings.warn(\n                    'Checksum verification failed for HDU {0}.\\n'.format(\n                        (self.name, self.ver)), AstropyUserWarning)\n\n        if 'DATASUM' in self._header:\n            self._datasum = self._header['DATASUM']\n            self._datasum_valid = self.verify_datasum()\n            if not self._datasum_valid:\n                warnings.warn(\n                    'Datasum verification failed for HDU {0}.\\n'.format(\n                        (self.name, self.ver)), AstropyUserWarning)\n\n    def _get_timestamp(self):\n        \"\"\"\n        Return the current timestamp in ISO 8601 format, with microseconds\n        stripped off.\n\n        Ex.: 2007-05-30T19:05:11\n        \"\"\"\n\n        return datetime.datetime.now().isoformat()[:19]\n\n    def _calculate_datasum(self):\n        \"\"\"\n        Calculate the value for the ``DATASUM`` card in the HDU.\n        \"\"\"\n\n        if not self._data_loaded:\n            # This is the case where the data has not been read from the file\n            # yet.  We find the data in the file, read it, and calculate the\n            # datasum.\n            if self.size > 0:\n                raw_data = self._get_raw_data(self._data_size, 'ubyte',\n                                              self._data_offset)\n                return self._compute_checksum(raw_data)\n            else:\n                return 0\n        elif self.data is not None:\n            return self._compute_checksum(self.data.view('ubyte'))\n        else:\n            return 0\n\n    def _calculate_checksum(self, datasum, checksum_keyword='CHECKSUM'):\n        \"\"\"\n        Calculate the value of the ``CHECKSUM`` card in the HDU.\n        \"\"\"\n\n        old_checksum = self._header[checksum_keyword]\n        self._header[checksum_keyword] = '0' * 16\n\n        # Convert the header to bytes.\n        s = self._header.tostring().encode('utf8')\n\n        # Calculate the checksum of the Header and data.\n        cs = self._compute_checksum(np.frombuffer(s, dtype='ubyte'), datasum)\n\n        # Encode the checksum into a string.\n        s = self._char_encode(~cs)\n\n        # Return the header card value.\n        self._header[checksum_keyword] = old_checksum\n\n        return s\n\n    def _compute_checksum(self, data, sum32=0):\n        \"\"\"\n        Compute the ones-complement checksum of a sequence of bytes.\n\n        Parameters\n        ----------\n        data\n            a memory region to checksum\n\n        sum32\n            incremental checksum value from another region\n\n        Returns\n        -------\n        ones complement checksum\n        \"\"\"\n\n        blocklen = 2880\n        sum32 = np.uint32(sum32)\n        for i in range(0, len(data), blocklen):\n            length = min(blocklen, len(data) - i)   # ????\n            sum32 = self._compute_hdu_checksum(data[i:i + length], sum32)\n        return sum32\n\n    def _compute_hdu_checksum(self, data, sum32=0):\n        \"\"\"\n        Translated from FITS Checksum Proposal by Seaman, Pence, and Rots.\n        Use uint32 literals as a hedge against type promotion to int64.\n\n        This code should only be called with blocks of 2880 bytes\n        Longer blocks result in non-standard checksums with carry overflow\n        Historically,  this code *was* called with larger blocks and for that\n        reason still needs to be for backward compatibility.\n        \"\"\"\n\n        u8 = np.uint32(8)\n        u16 = np.uint32(16)\n        uFFFF = np.uint32(0xFFFF)\n\n        if data.nbytes % 2:\n            last = data[-1]\n            data = data[:-1]\n        else:\n            last = np.uint32(0)\n\n        data = data.view('>u2')\n\n        hi = sum32 >> u16\n        lo = sum32 & uFFFF\n        hi += np.add.reduce(data[0::2], dtype=np.uint64)\n        lo += np.add.reduce(data[1::2], dtype=np.uint64)\n\n        if (data.nbytes \/\/ 2) % 2:\n            lo += last << u8\n        else:\n            hi += last << u8\n\n        hicarry = hi >> u16\n        locarry = lo >> u16\n\n        while hicarry or locarry:\n            hi = (hi & uFFFF) + locarry\n            lo = (lo & uFFFF) + hicarry\n            hicarry = hi >> u16\n            locarry = lo >> u16\n\n        return (hi << u16) + lo\n\n    # _MASK and _EXCLUDE used for encoding the checksum value into a character\n    # string.\n    _MASK = [0xFF000000,\n             0x00FF0000,\n             0x0000FF00,\n             0x000000FF]\n\n    _EXCLUDE = [0x3a, 0x3b, 0x3c, 0x3d, 0x3e, 0x3f, 0x40,\n                0x5b, 0x5c, 0x5d, 0x5e, 0x5f, 0x60]\n\n    def _encode_byte(self, byte):\n        \"\"\"\n        Encode a single byte.\n        \"\"\"\n\n        quotient = byte \/\/ 4 + ord('0')\n        remainder = byte % 4\n\n        ch = np.array(\n            [(quotient + remainder), quotient, quotient, quotient],\n            dtype='int32')\n\n        check = True\n        while check:\n            check = False\n            for x in self._EXCLUDE:\n                for j in [0, 2]:\n                    if ch[j] == x or ch[j + 1] == x:\n                        ch[j] += 1\n                        ch[j + 1] -= 1\n                        check = True\n        return ch\n\n    def _char_encode(self, value):\n        \"\"\"\n        Encodes the checksum ``value`` using the algorithm described\n        in SPR section A.7.2 and returns it as a 16 character string.\n\n        Parameters\n        ----------\n        value\n            a checksum\n\n        Returns\n        -------\n        ascii encoded checksum\n        \"\"\"\n\n        value = np.uint32(value)\n\n        asc = np.zeros((16,), dtype='byte')\n        ascii = np.zeros((16,), dtype='byte')\n\n        for i in range(4):\n            byte = (value & self._MASK[i]) >> ((3 - i) * 8)\n            ch = self._encode_byte(byte)\n            for j in range(4):\n                asc[4 * j + i] = ch[j]\n\n        for i in range(16):\n            ascii[i] = asc[(i + 15) % 16]\n\n        return decode_ascii(ascii.tostring())\n\n\nclass ExtensionHDU(_ValidHDU):\n    \"\"\"\n    An extension HDU class.\n\n    This class is the base class for the `TableHDU`, `ImageHDU`, and\n    `BinTableHDU` classes.\n    \"\"\"\n\n    _extension = ''\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        This class should never be instantiated directly.  Either a standard\n        extension HDU type should be used for a specific extension, or\n        NonstandardExtHDU should be used.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @deprecated_renamed_argument('clobber', 'overwrite', '2.0')\n    def writeto(self, name, output_verify='exception', overwrite=False,\n                checksum=False):\n        \"\"\"\n        Works similarly to the normal writeto(), but prepends a default\n        `PrimaryHDU` are required by extension HDUs (which cannot stand on\n        their own).\n\n        .. versionchanged:: 1.3\n           ``overwrite`` replaces the deprecated ``clobber`` argument.\n        \"\"\"\n\n        from .hdulist import HDUList\n        from .image import PrimaryHDU\n\n        hdulist = HDUList([PrimaryHDU(), self])\n        hdulist.writeto(name, output_verify, overwrite=overwrite,\n                        checksum=checksum)\n\n    def _verify(self, option='warn'):\n\n        errs = super()._verify(option=option)\n\n        # Verify location and value of mandatory keywords.\n        naxis = self._header.get('NAXIS', 0)\n        self.req_cards('PCOUNT', naxis + 3, lambda v: (_is_int(v) and v >= 0),\n                       0, option, errs)\n        self.req_cards('GCOUNT', naxis + 4, lambda v: (_is_int(v) and v == 1),\n                       1, option, errs)\n\n        return errs\n\n\n# For backwards compatibility, though this needs to be deprecated\n# TODO: Mark this as deprecated\n_ExtensionHDU = ExtensionHDU\n\n\nclass NonstandardExtHDU(ExtensionHDU):\n    \"\"\"\n    A Non-standard Extension HDU class.\n\n    This class is used for an Extension HDU when the ``XTENSION``\n    `Card` has a non-standard value.  In this case, Astropy can figure\n    out how big the data is but not what it is.  The data for this HDU\n    is read from the file as a byte stream that begins at the first\n    byte after the header ``END`` card and continues until the\n    beginning of the next header or the end of the file.\n    \"\"\"\n\n    _standard = False\n\n    @classmethod\n    def match_header(cls, header):\n        \"\"\"\n        Matches any extension HDU that is not one of the standard extension HDU\n        types.\n        \"\"\"\n\n        card = header.cards[0]\n        xtension = card.value\n        if isinstance(xtension, str):\n            xtension = xtension.rstrip()\n        # A3DTABLE is not really considered a 'standard' extension, as it was\n        # sort of the prototype for BINTABLE; however, since our BINTABLE\n        # implementation handles A3DTABLE HDUs it is listed here.\n        standard_xtensions = ('IMAGE', 'TABLE', 'BINTABLE', 'A3DTABLE')\n        # The check that xtension is not one of the standard types should be\n        # redundant.\n        return (card.keyword == 'XTENSION' and\n                xtension not in standard_xtensions)\n\n    def _summary(self):\n        axes = tuple(self.data.shape)\n        return (self.name, self.ver, 'NonstandardExtHDU', len(self._header), axes)\n\n    @lazyproperty\n    def data(self):\n        \"\"\"\n        Return the file data.\n        \"\"\"\n\n        return self._get_raw_data(self.size, 'ubyte', self._data_offset)\n\n\n# TODO: Mark this as deprecated\n_NonstandardExtHDU = NonstandardExtHDU\n","license":"bsd-3-clause","hash":-846878474168458060,"line_mean":35.1389578164,"line_max":82,"alpha_frac":0.561212579,"autogenerated":false},
{"repo_name":"psci2195\/espresso-ffans","path":"samples\/espresso_logo.py","copies":"1","size":"6271","content":"# Copyright (C) 2010-2019 The ESPResSo project\n#\n# This file is part of ESPResSo.\n#\n# ESPResSo is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# ESPResSo is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\"\"\"\nBuild the ESPResSo logo with particles.\n\"\"\"\nimport math\nimport numpy as np\n\nimport espressomd\nespressomd.assert_features([\"WCA\", \"MASS\"])\nimport espressomd.shapes\nfrom espressomd.visualization_opengl import openGLLive\n\nbox_l = 50\nsystem = espressomd.System(box_l=[box_l, 15, box_l])\nsystem.set_random_state_PRNG()\n#system.seed = system.cell_system.get_state()['n_nodes'] * [1234]\nnp.random.seed(seed=system.seed)\n\nyoff = 3\n\n# cup\ncup_top_circ = 21\ncup_bot_circ = 15\ncup_height = 6\nfor i in range(cup_height):\n    circ = cup_bot_circ + i * \\\n        (cup_top_circ - cup_bot_circ) \/ float(cup_height - 1)\n    rad = circ \/ (2.0 * np.pi)\n    alpha = 2.0 * np.pi \/ int(circ)\n    posy = yoff + i\n    for j in range(int(circ)):\n        posx = box_l \/ 2.0 + rad * math.sin(j * alpha + (np.pi \/ 2.0))\n        posz = box_l \/ 2.0 + rad * math.cos(j * alpha + (np.pi \/ 2.0))\n        system.part.add(pos=[posx, posy, posz], type=0)\n\n# cup bottom\nrad = cup_bot_circ \/ (2.0 * np.pi)\nposy = yoff\nwhile rad > 1.0:\n    rad -= 0.9\n    circ = 2.0 * np.pi * rad\n    alpha = 2.0 * np.pi \/ int(circ)\n    for j in range(int(circ)):\n        posx = box_l \/ 2.0 + rad * math.sin(j * alpha + (np.pi \/ 2.0))\n        posz = box_l \/ 2.0 + rad * math.cos(j * alpha + (np.pi \/ 2.0))\n        system.part.add(pos=[posx, posy, posz], type=0)\n\n\n# cup handle\nhand_rad = (cup_height - 4.0) \/ math.sqrt(2.0)\nhand_circ = (1.5 * np.pi * hand_rad)\nhand_xoff = (cup_bot_circ + cup_top_circ) \/ (4.0 * np.pi) + 1.2\nhand_yoff = yoff + cup_height \/ 2.0 - 0.2\nalpha = 2.0 * np.pi \/ int(4.0 * hand_circ \/ 3.0)\nbeta = math.sin((cup_top_circ - cup_bot_circ) \/ (2.0 * np.pi * cup_height - 1))\nbeta = beta - np.pi \/ 8.0\nposz = (box_l \/ 2.0) + 0.5\nfor i in range(int(hand_circ)):\n    posx = hand_xoff + box_l \/ 2.0 + hand_rad * math.sin(i * alpha + beta)\n    posy = hand_yoff + hand_rad * math.cos(i * alpha + beta)\n    system.part.add(pos=[posx, posy, posz], type=0)\n\n# saucer\nsaucer_circ = 30\ns_rad_o = saucer_circ \/ (2.0 * np.pi)\ns_rad_i = cup_bot_circ \/ (2.0 * np.pi)\nn_saucer = int(s_rad_o - s_rad_i) + 1\nn_ci = 0\nfor i in range(n_saucer):\n    n_ci += int(saucer_circ - (i * 2.0 * np.pi))\n\nci_val = -len(system.part) \/ float(n_ci)\nfor i in range(n_saucer):\n    rad = s_rad_o - i\n    alpha = 2.0 * np.pi \/ int(saucer_circ - (i * 2.0 * np.pi))\n    posy = yoff + 0.3 - 0.5 * i\n    for j in range(int(saucer_circ - (i * 2.0 * np.pi))):\n        posx = box_l \/ 2.0 + rad * math.sin(j * alpha)\n        posz = box_l \/ 2.0 + rad * math.cos(j * alpha)\n        system.part.add(pos=[posx, posy, posz], type=1)\n\n# python\nn_pbody = 12\nposy = 3.5\nposz = box_l \/ 2.0\ndiam = 0.8\nmass = 0.01\nfl = -1\nharm = espressomd.interactions.HarmonicBond(k=400.0, r_0=diam, r_cut=5.0)\nsystem.bonded_inter.add(harm)\n\nfor i in range(n_pbody):\n    posx = i * diam\n    system.part.add(pos=[posx, posy, posz], type=2, mass=mass)\n    pid = len(system.part) - 1\n    if i > 0:\n        system.part[pid].bonds = (harm, pid - 1)\n    if i % 3 == 0:\n        fl *= -1\n        system.part[pid].ext_force = [0, fl * 40 * mass, 0]\n    if i >= n_pbody - 3:\n        system.part[pid].ext_force = [50.0 * mass, 0, 0]\n    elif i == 0:\n        system.part[pid].ext_force = [-20 * mass, 0, 0]\n\n\n# steam\nfene = espressomd.interactions.FeneBond(k=15.1, d_r_max=2.0, r_0=0.1)\nsystem.bonded_inter.add(fene)\n\nn_steam = 6\nl_steam = 12\nrad = (cup_top_circ - 12.5) \/ (2.0 * np.pi)\nalpha = 2.0 * np.pi \/ int(n_steam)\nfor i in range(n_steam):\n    for j in range(l_steam):\n        posx = box_l \/ 2.0 + rad * math.sin(i * alpha + j * 0.6)\n        posz = box_l \/ 2.0 + rad * math.cos(i * alpha + j * 0.6)\n        posy = yoff + 2 + j * 0.1 * rad\n        system.part.add(pos=[posx, posy, posz], type=3)\n        pid = len(system.part) - 1\n\n        if j == 0:\n            system.part[pid].fix = [1, 1, 1]\n        else:\n            system.part[pid].bonds = (fene, pid - 1)\n\n        if j == l_steam - 1:\n            system.part[pid].ext_force = [0, 7.0, 0]\n\n\n# stand\nsystem.constraints.add(\n    shape=espressomd.shapes.Cylinder(\n        center=[box_l \/ 2.0, 1.0, box_l \/ 2.0],\n        axis=[0, 1, 0],\n        direction=1,\n        radius=7.5,\n        length=1),\n    particle_type=0,\n    penetrable=True)\n\n\nsystem.time_step = 0.00022\nsystem.cell_system.skin = 0.4\n\nsystem.thermostat.set_langevin(kT=0.0, gamma=0.02, seed=42)\n\nwca_eps = 1.0\nwca_sig = 0.7\nfor i in range(2):\n    for j in range(i, 2):\n        system.non_bonded_inter[i, j].wca.set_params(\n            epsilon=wca_eps, sigma=wca_sig)\n\nwca_eps = 1.0\nwca_sig = 1.0\nfor i in range(3):\n    system.non_bonded_inter[i, 2].wca.set_params(\n        epsilon=wca_eps, sigma=wca_sig)\n\nvisualizer = openGLLive(\n    system,\n    background_color=[0.2, 0.2, 0.3],\n    camera_position=[box_l \/ 2.0, box_l \/ 4.0, 20 * 3],\n    particle_sizes=[0.6, 0.75, 0.9, 0.2],\n    particle_type_materials=['bright', 'bright', 'plastic', 'chrome'],\n    particle_type_colors=[[0.2, 0.2, 0.8, 1],\n                          [0.8, 0.2, 0.2, 1],\n                          [1, 1, 1, 1],\n                          [0.8, 0.8, 0.8, 1]],\n    bond_type_materials=['chrome'],\n    bond_type_colors=[[0.2, 0.2, 0.2, 0.5]],\n    bond_type_radius=[0.1],\n    constraint_type_colors=[[1, 1, 1, 0.5]],\n    constraint_type_materials=['chrome'],\n    spotlight_brightness=5.0,\n    spotlight_focus=100,\n    spotlight_angle=60,\n    light_brightness=1.0,\n    ext_force_arrows=False,\n    draw_axis=False,\n    draw_box=False,\n    drag_enabled=True)\n\n\ndef rotate():\n    visualizer.camera.rotateSystemXL()\n\n# visualizer.registerCallback(rotate, interval = 16)\n\n\nvisualizer.run(1)\n","license":"gpl-3.0","hash":-5592752257253676098,"line_mean":29.1490384615,"line_max":79,"alpha_frac":0.5853930793,"autogenerated":false},
{"repo_name":"zentralopensource\/zentral","path":"zentral\/contrib\/osquery\/migrations\/0008_auto_20210323_1844.py","copies":"1","size":"4330","content":"# Generated by Django 2.2.18 on 2021-03-23 18:44\n\nimport django.contrib.postgres.fields\nimport django.core.validators\nfrom django.db import migrations, models\nfrom django.db.models import F\n\n\ndef update_osquery_queries(apps, schema_editor):\n    Pack = apps.get_model(\"osquery\", \"Pack\")\n\n    for pack in Pack.objects.all():\n        for packquery in pack.packquery_set.all():\n            platforms = packquery.platforms or pack.platforms\n            minimum_osquery_version = packquery.minimum_osquery_version or pack.minimum_osquery_version\n            if platforms or minimum_osquery_version:\n                packquery.query.platforms = platforms\n                packquery.query.minimum_osquery_version = minimum_osquery_version\n                packquery.query.version = F(\"version\") + 1\n                packquery.query.save()\n\n\ndef update_osquery_enrolled_machine_platform_mask(apps, schema_editor):\n    try:\n        from zentral.contrib.inventory.models import MetaMachine\n        from zentral.contrib.inventory.conf import LINUX, MACOS, WINDOWS\n    except ImportError:\n        pass\n    EnrolledMachine = apps.get_model(\"osquery\", \"EnrolledMachine\")\n    for enrolled_machine in EnrolledMachine.objects.all():\n        mm = MetaMachine(enrolled_machine.serial_number)\n        if mm.platform:\n            if mm.platform == LINUX:\n                enrolled_machine.platform_mask = 0x01 | 0x08\n            elif mm.platform == MACOS:\n                enrolled_machine.platform_mask = 0x01 | 0x04 | 0x10\n            elif mm.platform == WINDOWS:\n                enrolled_machine.platform_mask = 0x02\n            else:\n                print(\"Unsupported osquery enrolled machine platform\", mm.platform)\n                continue\n            enrolled_machine.save()\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('osquery', '0007_auto_20210215_2159'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='distributedquery',\n            name='minimum_osquery_version',\n            field=models.CharField(\n                editable=False, max_length=14, null=True,\n                validators=[django.core.validators.RegexValidator('^[0-9]{1,4}\\\\.[0-9]{1,4}\\\\.[0-9]{1,4}(\\\\.[0-9]{1,4})?$')]\n            ),\n        ),\n        migrations.AddField(\n            model_name='distributedquery',\n            name='platforms',\n            field=django.contrib.postgres.fields.ArrayField(base_field=models.CharField(choices=[('darwin', 'darwin'), ('freebsd', 'freebsd'), ('linux', 'linux'), ('posix', 'posix'), ('windows', 'windows')], max_length=32), default=list, editable=False, size=None),\n        ),\n        migrations.AddField(\n            model_name='query',\n            name='minimum_osquery_version',\n            field=models.CharField(\n                blank=True,\n                help_text='This query will only execute on osquery versions greater than or equal-to this version string',\n                max_length=14, null=True,\n                validators=[django.core.validators.RegexValidator('^[0-9]{1,4}\\\\.[0-9]{1,4}\\\\.[0-9]{1,4}(\\\\.[0-9]{1,4})?$')]\n            ),\n        ),\n        migrations.AddField(\n            model_name='query',\n            name='platforms',\n            field=django.contrib.postgres.fields.ArrayField(base_field=models.CharField(choices=[('darwin', 'darwin'), ('freebsd', 'freebsd'), ('linux', 'linux'), ('posix', 'posix'), ('windows', 'windows')], max_length=32), blank=True, default=list, help_text=\"Restrict this query to some platforms, default is 'all' platforms\", size=None),\n        ),\n        migrations.RunPython(update_osquery_queries),\n        migrations.RemoveField(\n            model_name='pack',\n            name='minimum_osquery_version',\n        ),\n        migrations.RemoveField(\n            model_name='pack',\n            name='platforms',\n        ),\n        migrations.RemoveField(\n            model_name='packquery',\n            name='minimum_osquery_version',\n        ),\n        migrations.RemoveField(\n            model_name='packquery',\n            name='platforms',\n        ),\n        migrations.AddField(\n            model_name='enrolledmachine',\n            name='platform_mask',\n            field=models.PositiveSmallIntegerField(default=0),\n        ),\n        migrations.RunPython(update_osquery_enrolled_machine_platform_mask),\n    ]\n","license":"apache-2.0","hash":3073662493980166797,"line_mean":41.0388349515,"line_max":340,"alpha_frac":0.5986143187,"autogenerated":false},
{"repo_name":"Soldia1138\/ACE3","path":"tools\/sqf_linter.py","copies":"7","size":"1950","content":"\ufeff#!\/usr\/bin\/env python3\n\n# Requires: https:\/\/github.com\/LordGolias\/sqf\n\nimport fnmatch\nimport os\nimport sys\nimport argparse\nfrom sqf.parser import parse\nimport sqf.analyzer\nfrom sqf.exceptions import SQFParserError\n\n\ndef analyze(filename, writer=sys.stdout):\n    warnings = 0\n    errors = 0\n    with open(filename, 'r') as file:\n        code = file.read()\n        try:\n            result = parse(code)\n        except SQFParserError as e:\n            print(\"{}:\".format(filename))\n            writer.write('    [%d,%d]:%s\\n' % (e.position[0], e.position[1] - 1, e.message))\n            return 0, 1\n\n        exceptions = sqf.analyzer.analyze(result).exceptions\n        if (exceptions): \n            print(\"{}:\".format(filename))\n            for e in exceptions:\n                if (e.message.startswith(\"error\")):\n                    errors += 1\n                else:\n                    warnings += 1\n                writer.write('    [%d,%d]:%s\\n' % (e.position[0], e.position[1] - 1, e.message))\n    \n    return warnings, errors\n         \ndef main():\n    print(\"#########################\")\n    print(\"# Lint Check  #\")\n    print(\"#########################\")\n\n    sqf_list = []\n    all_warnings = 0\n    all_errors = 0\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('-m','--module', help='only search specified module addon folder', required=False, default=\".\")\n    args = parser.parse_args()\n \n    for root, dirnames, filenames in os.walk('..\/addons' + '\/' + args.module):\n        for filename in fnmatch.filter(filenames, '*.sqf'):\n            sqf_list.append(os.path.join(root, filename))\n        \n    for filename in sqf_list:\n        warnings, errors = analyze(filename)\n        all_warnings += warnings\n        all_errors += errors\n    \n    print (\"Parse Errors {0} - Warnings {1}\".format(all_errors,all_warnings))\n\n    # return (all_errors + all_warnings)\n    return all_errors\n    \nif __name__ == \"__main__\":\n    main()\n","license":"gpl-2.0","hash":-2038532092595454950,"line_mean":28.5151515152,"line_max":119,"alpha_frac":0.5497946612,"autogenerated":false},
{"repo_name":"proxama\/zorp","path":"zorp\/registry.py","copies":"1","size":"2078","content":"\"\"\"\nRemote method registry\n\"\"\"\n\nfrom copy import deepcopy\nfrom inspect import getargspec, ismethod\n\nBASE_SCHEMA = {\n    \"additionalProperties\": False,\n    \"required\": [\"args\", \"kwargs\"],\n    \"properties\": {\n        \"args\": {\n            \"type\": \"array\",\n        },\n        \"kwargs\": {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n        },\n    },\n}\n\ndef schema_from_function(func):\n    \"\"\"\n    Return a json schema derived from\n    the provided function's signature\n    \"\"\"\n\n    (args, _, _, defaults) = getargspec(func)\n\n    # `getargspec` returns the `self` arg for bound methods, even though it's\n    # implicitly provided when calling the function. Let's remove it.\n    if ismethod(func):\n        args = args[1:]\n\n    args = args or []\n    defaults = defaults or []\n\n    if defaults:\n        defaults = args[-len(defaults):]\n        args = args[:-len(defaults)]\n    else:\n        defaults = []\n\n    schema = deepcopy(BASE_SCHEMA)\n\n    schema[\"properties\"][\"args\"][\"minItems\"] = len(args)\n    schema[\"properties\"][\"args\"][\"maxItems\"] = len(args)\n\n    schema[\"properties\"][\"kwargs\"][\"properties\"] = {\n        key: {}\n        for key\n        in defaults\n    }\n\n    return schema\n\nclass Registry(object):\n    \"\"\"\n    Generate json schema for each registered function\n    Store functions by a name string\n    Retrieve a schema and function by a name string\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialise the store\n        \"\"\"\n\n        self.methods = {}\n        self.schemas = {}\n\n    def put(self, name, func):\n        \"\"\"\n        Generate a JSON schema for `func`\n        Store the function and the schema\n        using `name` as the key\n        \"\"\"\n\n        schema = schema_from_function(func)\n\n        self.methods[name] = func\n        self.schemas[name] = schema\n\n    def get(self, name):\n        \"\"\"\n        Return a tuple containing the schema and function\n        using `name` as the key\n\n        Throw a KeyError if it doesn't exist\n        \"\"\"\n\n        return (self.schemas[name], self.methods[name])\n\nregistry = Registry()\n","license":"mit","hash":7159959980971317420,"line_mean":21.1063829787,"line_max":77,"alpha_frac":0.5635226179,"autogenerated":false},
{"repo_name":"Edraak\/circleci-edx-platform","path":"common\/djangoapps\/course_modes\/views.py","copies":"2","size":"11647","content":"\"\"\"\nViews for the course_mode module\n\"\"\"\n\nimport decimal\nfrom ipware.ip import get_ip\n\nfrom django.core.urlresolvers import reverse\nfrom django.db import transaction\nfrom django.http import HttpResponse, HttpResponseBadRequest\nfrom django.shortcuts import redirect\nfrom django.views.generic.base import View\nfrom django.utils.translation import ugettext as _\nfrom django.contrib.auth.decorators import login_required\nfrom django.utils.decorators import method_decorator\n\nfrom edxmako.shortcuts import render_to_response\n\nfrom course_modes.models import CourseMode\nfrom courseware.access import has_access\nfrom student.models import CourseEnrollment\nfrom opaque_keys.edx.locations import SlashSeparatedCourseKey\nfrom opaque_keys.edx.keys import CourseKey\nfrom util.db import outer_atomic\nfrom xmodule.modulestore.django import modulestore\n\nfrom embargo import api as embargo_api\n\nfrom helpers import SUCCESS_ENROLL_PAGE, get_mktg_for_course\n\n\nclass ChooseModeView(View):\n    \"\"\"View used when the user is asked to pick a mode.\n\n    When a get request is used, shows the selection page.\n\n    When a post request is used, assumes that it is a form submission\n    from the selection page, parses the response, and then sends user\n    to the next step in the flow.\n\n    \"\"\"\n\n    @method_decorator(transaction.non_atomic_requests)\n    def dispatch(self, *args, **kwargs):        # pylint: disable=missing-docstring\n        return super(ChooseModeView, self).dispatch(*args, **kwargs)\n\n    @method_decorator(login_required)\n    @method_decorator(transaction.atomic)\n    def get(self, request, course_id, error=None):\n        \"\"\"Displays the course mode choice page.\n\n        Args:\n            request (`Request`): The Django Request object.\n            course_id (unicode): The slash-separated course key.\n\n        Keyword Args:\n            error (unicode): If provided, display this error message\n                on the page.\n\n        Returns:\n            Response\n\n        \"\"\"\n        course_key = CourseKey.from_string(course_id)\n\n        # Check whether the user has access to this course\n        # based on country access rules.\n        embargo_redirect = embargo_api.redirect_if_blocked(\n            course_key,\n            user=request.user,\n            ip_address=get_ip(request),\n            url=request.path\n        )\n        if embargo_redirect:\n            return redirect(embargo_redirect)\n\n        enrollment_mode, is_active = CourseEnrollment.enrollment_mode_for_user(request.user, course_key)\n        modes = CourseMode.modes_for_course_dict(course_key)\n\n        # We assume that, if 'professional' is one of the modes, it is the *only* mode.\n        # If we offer more modes alongside 'professional' in the future, this will need to route\n        # to the usual \"choose your track\" page same is true for no-id-professional mode.\n        has_enrolled_professional = (CourseMode.is_professional_slug(enrollment_mode) and is_active)\n        if CourseMode.has_professional_mode(modes) and not has_enrolled_professional:\n            return redirect(\n                reverse(\n                    'verify_student_start_flow',\n                    kwargs={'course_id': unicode(course_key)}\n                )\n            )\n\n        # If there isn't a verified mode available, then there's nothing\n        # to do on this page.  The user has almost certainly been auto-registered\n        # in the \"honor\" track by this point, so we send the user\n        # to the dashboard.\n        if not CourseMode.has_verified_mode(modes):\n            return redirect(get_mktg_for_course(SUCCESS_ENROLL_PAGE, unicode(course_id)))\n\n        # If a user has already paid, redirect them to the dashboard.\n        if is_active and (enrollment_mode in CourseMode.VERIFIED_MODES + [CourseMode.NO_ID_PROFESSIONAL_MODE]):\n            return redirect(reverse('dashboard'))\n\n        donation_for_course = request.session.get(\"donation_for_course\", {})\n        chosen_price = donation_for_course.get(unicode(course_key), None)\n\n        course = modulestore().get_course(course_key)\n\n        # When a credit mode is available, students will be given the option\n        # to upgrade from a verified mode to a credit mode at the end of the course.\n        # This allows students who have completed photo verification to be eligible\n        # for univerity credit.\n        # Since credit isn't one of the selectable options on the track selection page,\n        # we need to check *all* available course modes in order to determine whether\n        # a credit mode is available.  If so, then we show slightly different messaging\n        # for the verified track.\n        has_credit_upsell = any(\n            CourseMode.is_credit_mode(mode) for mode\n            in CourseMode.modes_for_course(course_key, only_selectable=False)\n        )\n\n        context = {\n            \"course_modes_choose_url\": reverse(\"course_modes_choose\", kwargs={'course_id': course_key.to_deprecated_string()}),\n            \"modes\": modes,\n            \"has_credit_upsell\": has_credit_upsell,\n            \"course_name\": course.display_name_with_default_escaped,\n            \"course_org\": course.display_org_with_default,\n            \"course_num\": course.display_number_with_default,\n            \"chosen_price\": chosen_price,\n            \"error\": error,\n            \"responsive\": True,\n            \"nav_hidden\": True,\n        }\n        if \"verified\" in modes:\n            context[\"suggested_prices\"] = [\n                decimal.Decimal(x.strip())\n                for x in modes[\"verified\"].suggested_prices.split(\",\")\n                if x.strip()\n            ]\n            context[\"currency\"] = modes[\"verified\"].currency.upper()\n            context[\"min_price\"] = modes[\"verified\"].min_price\n            context[\"verified_name\"] = modes[\"verified\"].name\n            context[\"verified_description\"] = modes[\"verified\"].description\n\n        return render_to_response(\"course_modes\/choose.html\", context)\n\n    @method_decorator(transaction.non_atomic_requests)\n    @method_decorator(login_required)\n    @method_decorator(outer_atomic(read_committed=True))\n    def post(self, request, course_id):\n        \"\"\"Takes the form submission from the page and parses it.\n\n        Args:\n            request (`Request`): The Django Request object.\n            course_id (unicode): The slash-separated course key.\n\n        Returns:\n            Status code 400 when the requested mode is unsupported. When the honor mode\n            is selected, redirects to the dashboard. When the verified mode is selected,\n            returns error messages if the indicated contribution amount is invalid or\n            below the minimum, otherwise redirects to the verification flow.\n\n        \"\"\"\n        course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id)\n        user = request.user\n\n        # This is a bit redundant with logic in student.views.change_enrollment,\n        # but I don't really have the time to refactor it more nicely and test.\n        course = modulestore().get_course(course_key)\n        if not has_access(user, 'enroll', course):\n            error_msg = _(\"Enrollment is closed\")\n            return self.get(request, course_id, error=error_msg)\n\n        requested_mode = self._get_requested_mode(request.POST)\n\n        allowed_modes = CourseMode.modes_for_course_dict(course_key)\n        if requested_mode not in allowed_modes:\n            return HttpResponseBadRequest(_(\"Enrollment mode not supported\"))\n\n        if requested_mode == 'audit':\n            # The user will have already been enrolled in the audit mode at this\n            # point, so we just redirect them to the dashboard, thereby avoiding\n            # hitting the database a second time attempting to enroll them.\n            return redirect(reverse('dashboard'))\n\n        if requested_mode == 'honor':\n            CourseEnrollment.enroll(user, course_key, mode=requested_mode)\n            return redirect(reverse('dashboard'))\n\n        mode_info = allowed_modes[requested_mode]\n\n        if requested_mode == 'verified':\n            amount = request.POST.get(\"contribution\") or \\\n                request.POST.get(\"contribution-other-amt\") or 0\n\n            try:\n                # Validate the amount passed in and force it into two digits\n                amount_value = decimal.Decimal(amount).quantize(decimal.Decimal('.01'), rounding=decimal.ROUND_DOWN)\n            except decimal.InvalidOperation:\n                error_msg = _(\"Invalid amount selected.\")\n                return self.get(request, course_id, error=error_msg)\n\n            # Check for minimum pricing\n            if amount_value < mode_info.min_price:\n                error_msg = _(\"No selected price or selected price is too low.\")\n                return self.get(request, course_id, error=error_msg)\n\n            donation_for_course = request.session.get(\"donation_for_course\", {})\n            donation_for_course[unicode(course_key)] = amount_value\n            request.session[\"donation_for_course\"] = donation_for_course\n\n            return redirect(\n                reverse(\n                    'verify_student_start_flow',\n                    kwargs={'course_id': unicode(course_key)}\n                )\n            )\n\n    def _get_requested_mode(self, request_dict):\n        \"\"\"Get the user's requested mode\n\n        Args:\n            request_dict (`QueryDict`): A dictionary-like object containing all given HTTP POST parameters.\n\n        Returns:\n            The course mode slug corresponding to the choice in the POST parameters,\n            None if the choice in the POST parameters is missing or is an unsupported mode.\n\n        \"\"\"\n        if 'verified_mode' in request_dict:\n            return 'verified'\n        if 'honor_mode' in request_dict:\n            return 'honor'\n        if 'audit_mode' in request_dict:\n            return 'audit'\n        else:\n            return None\n\n\ndef create_mode(request, course_id):\n    \"\"\"Add a mode to the course corresponding to the given course ID.\n\n    Only available when settings.FEATURES['MODE_CREATION_FOR_TESTING'] is True.\n\n    Attempts to use the following querystring parameters from the request:\n        `mode_slug` (str): The mode to add, either 'honor', 'verified', or 'professional'\n        `mode_display_name` (str): Describes the new course mode\n        `min_price` (int): The minimum price a user must pay to enroll in the new course mode\n        `suggested_prices` (str): Comma-separated prices to suggest to the user.\n        `currency` (str): The currency in which to list prices.\n\n    By default, this endpoint will create an 'honor' mode for the given course with display name\n    'Honor Code', a minimum price of 0, no suggested prices, and using USD as the currency.\n\n    Args:\n        request (`Request`): The Django Request object.\n        course_id (unicode): A course ID.\n\n    Returns:\n        Response\n    \"\"\"\n    PARAMETERS = {\n        'mode_slug': u'honor',\n        'mode_display_name': u'Honor Code Certificate',\n        'min_price': 0,\n        'suggested_prices': u'',\n        'currency': u'usd',\n    }\n\n    # Try pulling querystring parameters out of the request\n    for parameter, default in PARAMETERS.iteritems():\n        PARAMETERS[parameter] = request.GET.get(parameter, default)\n\n    # Attempt to create the new mode for the given course\n    course_key = CourseKey.from_string(course_id)\n    CourseMode.objects.get_or_create(course_id=course_key, **PARAMETERS)\n\n    # Return a success message and a 200 response\n    return HttpResponse(\"Mode '{mode_slug}' created for '{course}'.\".format(\n        mode_slug=PARAMETERS['mode_slug'],\n        course=course_id\n    ))\n","license":"agpl-3.0","hash":5502033912637248874,"line_mean":40.4483985765,"line_max":127,"alpha_frac":0.6442860823,"autogenerated":false},
{"repo_name":"ESSolutions\/ESSArch_Core","path":"ESSArch_Core\/mixins.py","copies":"1","size":"2258","content":"from rest_framework.generics import get_object_or_404\nfrom rest_framework.settings import api_settings\n\n\nclass PaginatedViewMixin:\n    pagination_class = api_settings.DEFAULT_PAGINATION_CLASS\n\n    @property\n    def paginator(self):\n        \"\"\"\n        The paginator instance associated with the view, or `None`.\n        \"\"\"\n        if not hasattr(self, '_paginator'):\n            if self.pagination_class is None:\n                self._paginator = None\n            else:\n                self._paginator = self.pagination_class()\n        return self._paginator\n\n    def paginate_queryset(self, queryset):\n        \"\"\"\n        Return a single page of results, or `None` if pagination is disabled.\n        \"\"\"\n        if self.paginator is None:\n            return None\n        return self.paginator.paginate_queryset(queryset, self.request, view=self)\n\n    def get_paginated_response(self, data):\n        \"\"\"\n        Return a paginated style `Response` object for the given output data.\n        \"\"\"\n        assert self.paginator is not None\n        return self.paginator.get_paginated_response(data)\n\n\nclass GetObjectForUpdateViewMixin:\n    def get_object_for_update(self):\n        \"\"\"\n        Returns the object the view is displaying using a queryset with\n        `select_for_update`.\n\n        You may want to override this if you need to provide non-standard\n        queryset lookups.  Eg if objects are referenced using multiple\n        keyword arguments in the url conf.\n        \"\"\"\n        queryset = self.filter_queryset(self.get_queryset()).select_related().select_for_update()\n\n        # Perform the lookup filtering.\n        lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field\n\n        assert lookup_url_kwarg in self.kwargs, (\n            'Expected view %s to be called with a URL keyword argument '\n            'named \"%s\". Fix your URL conf, or set the `.lookup_field` '\n            'attribute on the view correctly.' %\n            (self.__class__.__name__, lookup_url_kwarg)\n        )\n\n        filter_kwargs = {self.lookup_field: self.kwargs[lookup_url_kwarg]}\n        obj = get_object_or_404(queryset, **filter_kwargs)\n\n        # May raise a permission denied\n        self.check_object_permissions(self.request, obj)\n\n        return obj\n","license":"gpl-3.0","hash":2814356610682958476,"line_mean":34.28125,"line_max":97,"alpha_frac":0.6302037201,"autogenerated":false},
{"repo_name":"aonotas\/chainer","path":"chainer\/testing\/parameterized.py","copies":"1","size":"3594","content":"import functools\nimport inspect\nimport itertools\nimport sys\nimport types\nimport unittest\n\nimport six\n\n\ndef _gen_case(base, module, i, param):\n    cls_name = '%s_param_%d' % (base.__name__, i)\n\n    # Add parameters as members\n\n    def __str__(self):\n        name = base.__str__(self)\n        return '%s  parameter: %s' % (name, param)\n\n    mb = {'__str__': __str__}\n    for k, v in six.iteritems(param):\n        if isinstance(v, types.FunctionType):\n\n            def create_new_v():\n                f = v\n\n                def new_v(self, *args, **kwargs):\n                    return f(*args, **kwargs)\n                return new_v\n\n            mb[k] = create_new_v()\n        else:\n            mb[k] = v\n\n    cls = type(cls_name, (base,), mb)\n\n    # Wrap test methods to generate useful error message\n\n    def wrap_test_method(method):\n        @functools.wraps(method)\n        def wrap(*args, **kwargs):\n            try:\n                return method(*args, **kwargs)\n            except Exception as e:\n                s = six.StringIO()\n                s.write('Parameterized test failed.\\n\\n')\n                s.write('Base test method: {}.{}\\n'.format(\n                    base.__name__, method.__name__))\n                s.write('Test parameters:\\n')\n                for k, v in six.iteritems(param):\n                    s.write('  {}: {}\\n'.format(k, v))\n                s.write('\\n')\n                s.write('{}: {}\\n'.format(type(e).__name__, e))\n                e_new = AssertionError(s.getvalue())\n                if sys.version_info < (3,):\n                    six.reraise(AssertionError, e_new, sys.exc_info()[2])\n                else:\n                    six.raise_from(e_new.with_traceback(e.__traceback__), None)\n        return wrap\n\n    # ismethod for Python 2 and isfunction for Python 3\n    members = inspect.getmembers(\n        cls, predicate=lambda _: inspect.ismethod(_) or inspect.isfunction(_))\n    for name, method in members:\n        if name.startswith('test_'):\n            setattr(cls, name, wrap_test_method(method))\n\n    # Add new test class to module\n    setattr(module, cls_name, cls)\n\n\ndef _gen_cases(name, base, params):\n    module = sys.modules[name]\n    for i, param in enumerate(params):\n        _gen_case(base, module, i, param)\n\n\ndef parameterize(*params):\n    def f(klass):\n        assert issubclass(klass, unittest.TestCase)\n        _gen_cases(klass.__module__, klass, params)\n        # Remove original base class\n        return None\n    return f\n\n\ndef product(parameter):\n    if isinstance(parameter, dict):\n        keys = sorted(parameter)\n        values = [parameter[key] for key in keys]\n        values_product = itertools.product(*values)\n        return [dict(zip(keys, vals)) for vals in values_product]\n\n    elif isinstance(parameter, list):\n        # list of lists of dicts\n        if not all(isinstance(_, list) for _ in parameter):\n            raise TypeError('parameter must be list of lists of dicts')\n        if not all(isinstance(_, dict) for l in parameter for _ in l):\n            raise TypeError('parameter must be list of lists of dicts')\n\n        lst = []\n        for dict_lst in itertools.product(*parameter):\n            a = {}\n            for d in dict_lst:\n                a.update(d)\n            lst.append(a)\n        return lst\n\n    else:\n        raise TypeError(\n            'parameter must be either dict or list. Actual: {}'.format(\n                type(parameter)))\n\n\ndef product_dict(*parameters):\n    return [\n        {k: v for dic in dicts for k, v in six.iteritems(dic)}\n        for dicts in itertools.product(*parameters)]\n","license":"mit","hash":3015021605456955529,"line_mean":29.4576271186,"line_max":79,"alpha_frac":0.5439621592,"autogenerated":false},
{"repo_name":"markgw\/jazzparser","path":"src\/jazzparser\/taggers\/loader.py","copies":"1","size":"1572","content":"\"\"\"Dynamic loader for supertagger modules.\n\n\"\"\"\n\"\"\"\n============================== License ========================================\n Copyright (C) 2008, 2010-12 University of Edinburgh, Mark Granroth-Wilding\n \n This file is part of The Jazz Parser.\n \n The Jazz Parser is free software: you can redistribute it and\/or modify\n it under the terms of the GNU General Public License as published by\n the Free Software Foundation, either version 3 of the License, or\n (at your option) any later version.\n \n The Jazz Parser is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU General Public License for more details.\n \n You should have received a copy of the GNU General Public License\n along with The Jazz Parser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n============================ End license ======================================\n\n\"\"\"\n__author__ = \"Mark Granroth-Wilding <mark.granroth-wilding@ed.ac.uk>\" \n\nfrom jazzparser.utils.base import load_class\nfrom jazzparser.settings import DEFAULT_SUPERTAGGER\n\ndef get_tagger(name):\n\t\"\"\"\n\tReturns the tagger class for the tagger with the given name.\n\t\"\"\"\n\tfrom . import TAGGERS\n\tif name not in TAGGERS:\n\t\tfrom .tagger import TaggerLoadError\n\t\traise TaggerLoadError, \"The tagger '%s' does not exist\" % name\n\tpath = 'jazzparser.taggers.%s.%s' % TAGGERS[name]\n\treturn load_class(path)\n\ndef get_default_tagger():\n\treturn get_tagger(DEFAULT_SUPERTAGGER)\n\nclass TaggerLoadError(Exception):\n    pass\n","license":"gpl-3.0","hash":-5118854278064580285,"line_mean":33.1739130435,"line_max":79,"alpha_frac":0.6876590331,"autogenerated":false},
{"repo_name":"QuinDiesel\/CommitSudoku-Project-Game","path":"Definitief\/Question.py","copies":"2","size":"4163","content":"import pygame\r\n\r\ndef question():\r\n    #dit zoekt waar de speler zich bevind\r\n    player_position = int(input(\"What catagorie are you in?: 0=Red, 1=Yellow, 2=Blue, 3=Green \\n Select the number of catagorie? \"))\r\n    catagorie = 0\r\n\r\n    #score per catagorie zodat je geen zelfde vragen krijgt\r\n    red_score = 0\r\n    yellow_score = 0\r\n    blue_score = 0\r\n    green_score = 0\r\n\r\n    #catagorie indeling\r\n    if player_position == 0:\r\n        print(\"Catagorie Red: Entertainment\")\r\n        catagorie += 0\r\n\r\n    elif player_position == 1:\r\n        print(\"Catagorie Yellow: History\")\r\n        catagorie += 1\r\n\r\n    elif player_position == 2:\r\n        print(\"Catagorie Blue: Sport\")\r\n        catagorie += 2\r\n\r\n    elif player_position == 3:\r\n        print(\"Catagorie Green: Geography\")\r\n        catagorie += 3\r\n\r\n    if player_position == 0 and red_score == 0:\r\n        print(\"Question R1: Which artwork is called the Dutch version of the Sistine Chapel? \\n 1. Chapelprison \\n 2. De Markthal \\n 3. Ark the Rotterdam\")\r\n        answer = int(input(\"\"))\r\n        if answer == 2:\r\n            print(\"That is correct\")\r\n            red_score = red_score + 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 0 and red_score == 1:\r\n        print(\"Question R2: What is the name of the cultural and culinary exploration of Rotterdam? \\n 1. Drive & Eat \\n 2. Bicycle Diner \\n 3. Bike & Bite\")\r\n        answer = int(input(\"\"))\r\n        if answer == 3:\r\n            print(\"That is correct\")\r\n            red_score = red_score - 1\r\n        else:\r\n            print(\"That is wrong\")\r\n            \r\n    if player_position == 1 and yellow_score == 0:\r\n        print(\"Question Y1: What does the abbreviation of the NRC newspaper stands for? \\n 1. Nieuw Rotterdam Chronicle \\n 2. Nieuwe Rotterdamsche Courant \\n 3. Nieuwe Rotterdamse Co\u00f6peratie\")\r\n        answer = int(input(\"\"))\r\n        if answer == 2:\r\n            print(\"That is correct\")\r\n            yellow_score = yellow_score + 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 1 and yellow_score == 1:\r\n        print(\"Question Y2: During the Second World War, what was the only road for the Germans to reach the center? \\n 1. De nieuwe Binnenweg \\n 2. Maasbrug \\n 3. Koninginnenbrug\")\r\n        answer = int(input(\"\"))\r\n        if answer == 2:\r\n            print(\"That is correct\")\r\n            yellow_score = yellow_score - 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 2 and blue_score == 0:\r\n        print(\"Question B1: What is the name of the sportcentre next to De Kuip? \\n 1. Fit for Free Rotterdam \\n 2. Basic Fit Rotterdam \\n 3. Topsportcentrum Rotterdam\")\r\n        answer = int(input(\"\"))\r\n        if answer == 3:\r\n            print(\"That is correct\")\r\n            blue_score = blue_score + 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 2 and blue_score == 1:\r\n        print(\"Question B2: What kind of sport is most practiced in Rotterdam? \\n 1. Fitness \\n 2. Football \\n 3. Basketball\")\r\n        answer = int(input(\"\"))\r\n        if answer == 1:\r\n            print(\"That is correct\")\r\n            blue_score = blue_score - 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 3 and green_score == 0:\r\n        print(\"Question G1: What is the name of the most famous houses in Rotterdam? \\n 1. The kubuswoningen \\n 2. The havenhuizen \\n 3. The bijenkorf \")\r\n        answer = int(input(\"\"))\r\n        if answer == 1:\r\n            print(\"That is correct\")\r\n            green_score = green_score + 1\r\n        else:\r\n            print(\"That is wrong\")\r\n\r\n    if player_position == 3 and green_score == 1:\r\n        print(\"Question G2: What is the oldest bridge in Rotterdam? \\n 1. The Willemsbrug\\n 2. The Koninginnebrug\\n 3. The van Briennenoordbrug\")\r\n        answer = int(input(\"\"))\r\n        if answer == 2:\r\n            print(\"That is correct\")\r\n            green_score = green_score - 1\r\n        else:\r\n            print(\"That is wrong\")\r\n            \r\n    else:\r\n        print(\"Please reselect a correct color\")\r\n        return question()\r\n\r\nquestion()\r\n\r\n","license":"mit","hash":-2219572439652225400,"line_mean":36.537037037,"line_max":192,"alpha_frac":0.5648726574,"autogenerated":false},
{"repo_name":"ContextLogic\/redis-py","path":"redis\/async_connection.py","copies":"1","size":"15324","content":"from __future__ import with_statement\nfrom select import select\nfrom redis._compat import iteritems\nimport datetime\nimport socket\nimport sys\n\nfrom redis.connection import (\n    Connection,\n    HiredisParser,\n    HIREDIS_SUPPORTS_CALLABLE_ERRORS,\n    SERVER_CLOSED_CONNECTION_ERROR,\n)\n\nfrom redis.exceptions import (\n    RedisError,\n    ConnectionError,\n    TimeoutError,\n    ResponseError,\n)\nfrom redis.utils import (\n    SSL_AVAILABLE,\n    HIREDIS_AVAILABLE,\n    TORNADO_AVAILABLE,\n    GREENLET_AVAILABLE,\n)\n\nif TORNADO_AVAILABLE:\n    from tornado.iostream import IOStream\n    from tornado.iostream import SSLIOStream\n    from tornado.ioloop import IOLoop\nif GREENLET_AVAILABLE:\n    import greenlet\nif SSL_AVAILABLE:\n    import ssl\n\n\nclass AsyncHiredisParser(HiredisParser):\n    def __init__(self, socket_read_size):\n        self._ioloop = None\n        self._iostream = None\n        self._timeout_handle = None\n        self._disconnecting = False\n\n        if not HIREDIS_AVAILABLE:\n            raise(\"Async parser requires Hiredis\")\n        if not GREENLET_AVAILABLE:\n            raise(\"Async parser requires Greenlet\")\n        if not TORNADO_AVAILABLE:\n            raise(\"Async parser requires Tornado\")\n\n        super(AsyncHiredisParser, self).__init__(socket_read_size)\n\n    def on_disconnect(self):\n        if self._iostream is None:\n            return\n\n        self._ioloop = None\n        self._iostream = None\n        self._timeout_handle = None\n        self._disconnecting = False\n\n        super(AsyncHiredisParser, self).on_disconnect()\n\n    def on_connect(self, connection):\n        self._ioloop = connection._ioloop\n        self._iostream = connection._iostream\n        self._read_timeout = connection.socket_timeout\n\n        super(AsyncHiredisParser, self).on_connect(connection)\n\n    def read_response(self):\n        if not self._reader:\n            raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n\n        # _next_response might be cached from a can_read() call\n        if self._next_response is not False:\n            response = self._next_response\n            self._next_response = False\n            return response\n\n        current_greenlet = greenlet.getcurrent()\n\n        def handle_read_timeout():\n            self._iostream.set_close_callback(None)\n            self._iostream._read_callback = None\n            self._disconnecting = True\n            self._timeout_handle = None\n            current_greenlet.switch('timeout', None)\n\n        def handle_read_error():\n            \"\"\" Connection error, stream is closed \"\"\"\n            self._iostream._read_callback = None\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n\n            if self._disconnecting:\n                self._disconnecting = False\n            else:\n                current_greenlet.switch('error', None)\n\n        def handle_read_complete(data):\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n\n            if self._disconnecting:\n                # Read timed-out while a read callback was pending in the\n                # ioloop\n                self._disconnecting = False\n            else:\n                self._iostream.set_close_callback(None)\n                current_greenlet.switch('success', data)\n\n        response = self._reader.gets()\n        while response is False:\n            if self._read_timeout:\n                timedelta = datetime.timedelta(seconds=self._read_timeout)\n                self._timeout_handle = self._ioloop.add_timeout(timedelta,\n                    handle_read_timeout)\n            self._iostream.set_close_callback(handle_read_error)\n            self._iostream.read_bytes(self.socket_read_size,\n                                      handle_read_complete,\n                                      partial=True)\n            status, data = current_greenlet.parent.switch()\n\n            if status is 'timeout':\n                raise TimeoutError(\"Timeout reading from socket\")\n            if status is 'error':\n                raise ConnectionError(\"Timeout reading from socket\")\n            # an empty string indicates the server shutdown the socket\n            if not isinstance(data, bytes) or len(data) == 0:\n                raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n\n            self._reader.feed(data)\n            response = self._reader.gets()\n        # if an older version of hiredis is installed, we need to attempt\n        # to convert ResponseErrors to their appropriate types.\n        if not HIREDIS_SUPPORTS_CALLABLE_ERRORS:\n            if isinstance(response, ResponseError):\n                response = self.parse_error(response.args[0])\n            elif isinstance(response, list) and response and \\\n                    isinstance(response[0], ResponseError):\n                response[0] = self.parse_error(response[0].args[0])\n        # if the response is a ConnectionError or the response is a list and\n        # the first item is a ConnectionError, raise it as something bad\n        # happened\n        if isinstance(response, ConnectionError):\n            raise response\n        elif isinstance(response, list) and response and \\\n                isinstance(response[0], ConnectionError):\n            raise response[0]\n        return response\n\n\nclass ConnectionInvalidContext(RedisError):\n    pass\n\n\nclass AsyncConnection(Connection):\n    \"Manages TCP communication to and from a Redis server\"\n    description_format = (\"AsyncConnection\"\n                          \"<host=%(host)s,port=%(port)s,db=%(db)s>\")\n\n    def __init__(self, parser_class=AsyncHiredisParser,\n                 ioloop = None, *args, **kwargs):\n        super(AsyncConnection, self).__init__(parser_class=parser_class,\n                                              *args, **kwargs)\n        self._ioloop = ioloop or IOLoop.instance()\n        self._iostream = None\n        self._timeout_handle = None\n        self._disconnecting = False\n\n    def _wrap_socket(self, sock):\n        return IOStream(sock)\n\n    def _maybe_raise_no_greenlet_parent(self):\n        if greenlet.getcurrent().parent is None:\n            raise ConnectionInvalidContext(\"Greenlet parent not found, \"\n                                           \"cannot perform async operations\")\n\n    def _connect(self):\n        \"Create a TCP socket connection\"\n        # we want to mimic what socket.create_connection does to support\n        # ipv4\/ipv6, but we want to set options prior to calling\n        # socket.connect()\n        self._maybe_raise_no_greenlet_parent()\n\n        if self._iostream:\n            return\n\n        err = None\n        current_greenlet = greenlet.getcurrent()\n\n        def handle_timeout():\n            self._iostream.set_close_callback(None)\n            self._iostream._connect_callback = None\n            self._timeout_handle = None\n\n            if self._iostream._pending_callbacks > 0:\n                # There's a close or connect callback pending so we'll let\n                # it either cleanup or connect.  This is not honoring the\n                # timeout. It could be honored by remembering that it timed-out\n                # and handling it in the connect, but that seems\n                # counterproductive.\n                return\n            else:\n                current_greenlet.switch('timeout')\n\n        def handle_error():\n            \"\"\" Connection error, stream is closed \"\"\"\n            self._iostream._connect_callback = None\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n            current_greenlet.switch('error')\n\n        def handle_connected():\n            self._iostream.set_close_callback(None)\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n            current_greenlet.switch('success')\n\n        for res in socket.getaddrinfo(self.host, self.port, 0,\n                                      socket.SOCK_STREAM):\n            family, socktype, proto, canonname, socket_address = res\n            sock = None\n            try:\n                sock = socket.socket(family, socktype, proto)\n                # TCP_NODELAY\n                sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n                # TCP_KEEPALIVE\n                if self.socket_keepalive:\n                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n                    for k, v in iteritems(self.socket_keepalive_options):\n                        sock.setsockopt(socket.SOL_TCP, k, v)\n\n                self._iostream = self._wrap_socket(sock)\n\n                timeout = self.socket_connect_timeout\n                if timeout:\n                    timedelta = datetime.timedelta(seconds=timeout)\n                    self._timeout_handle = self._ioloop.add_timeout(timedelta,\n                                                        handle_timeout)\n                self._iostream.set_close_callback(handle_error)\n                self._iostream.connect(socket_address, callback=handle_connected)\n\n                # yield back to parent, wait for connect, error or timeout\n                status = current_greenlet.parent.switch()\n                if status == 'error':\n                    raise ConnectionError('Error connecting to host')\n                if status == 'timeout':\n                    raise ConnectionError('Connection timed out')\n\n                return sock\n            except ConnectionError as _:\n                err = _\n                if sock is not None:\n                    sock.close()\n                if self._iostream is not None:\n                    self._iostream.close()\n                    self._iostream = None\n\n        if err is not None:\n            raise err\n        raise socket.error(\"socket.getaddrinfo returned an empty list\")\n\n    def disconnect(self):\n        \"Disconnects from the Redis server\"\n        if self._iostream is None:\n            return\n\n        self._iostream.set_close_callback(None)\n        self._iostream.close()\n\n        if self._iostream._pending_callbacks > 0:\n            current_greenlet = greenlet.getcurrent()\n\n            def handle_disconnect():\n                current_greenlet.switch()\n\n            # There's a pending read\/write callback pending in the ioloop. Wait\n            # until it has been called before releasing this connection back to\n            # the connection pool.\n            self._ioloop.add_callback(handle_disconnect)\n            # wait for handle_disconnect callback\n            current_greenlet.parent.switch()\n\n        # This will call into the AsynchiredisParser on_disconnect.\n        super(AsyncConnection, self).disconnect()\n\n        self._iostream = None\n        self._timeout_handle = None\n        self._disconnecting = False\n\n    def send_packed_command(self, command):\n        \"Send an already packed command to the Redis server\"\n\n        self._maybe_raise_no_greenlet_parent()\n\n        if not self._iostream:\n            self.connect()\n\n        current_greenlet = greenlet.getcurrent()\n\n        def handle_write_timeout():\n            self._timeout_handle = None\n            self._iostream.set_close_callback(None)\n            self._iostream._write_callback = None\n            self._disconnecting = True\n            current_greenlet.switch('timeout')\n\n        def handle_write_error():\n            \"\"\" Connection error, stream is closed \"\"\"\n            self._iostream._write_callback = None\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n\n            if self._disconnecting:\n                self._disconnecting = False\n            else:\n                current_greenlet.switch('error')\n\n        def handle_write_complete():\n            if self._timeout_handle:\n                self._ioloop.remove_timeout(self._timeout_handle)\n                self._timeout_handle = None\n\n            if self._disconnecting:\n                # write timeout must have fired while the write callback was\n                # pending\n                self._disconnecting = False\n            else:\n                self._iostream.set_close_callback(None)\n                current_greenlet.switch('success')\n\n        try:\n            if isinstance(command, str):\n                command = [command]\n            ncmds = len(command)\n            for i, item in enumerate(command):\n                if i == (ncmds-1):\n                    cb = handle_write_complete\n                    self._iostream.set_close_callback(handle_write_error)\n                    timedelta = datetime.timedelta(seconds=self.socket_timeout)\n                    self._timeout_handle = self._ioloop.add_timeout(timedelta,\n                                                      handle_write_timeout)\n                else:\n                    cb = None\n\n                self._iostream.write(item, callback=cb)\n\n            status = current_greenlet.parent.switch()\n            if status == 'timeout':\n                raise TimeoutError(\"Timeout writing to socket\")\n            if status == 'error':\n                raise ConnectionError(\"Socket error during write\")\n        except:\n            self.disconnect()\n            raise\n\n    def can_read(self, timeout=0):\n        \"Check if there's any data that can be read\"\n        if not self._iostream:\n            self.connect()\n        self._maybe_raise_no_greenlet_noparent()\n\n        def check_for_data():\n            if (self._parser.can_read() or\n                    self._iostream._read_buffer_size):\n                return True\n            return bool(select([self._iostream.sock], [], [], 0)[0])\n\n        if timeout is 0:\n            return check_for_data()\n        else:\n            self._ioloop.call_later(timeout, greenlet.getcurrent().switch)\n            greenlet.getcurrent().parent.switch()\n            return check_for_data()\n\n\nclass AsyncSSLConnection(AsyncConnection):\n    description_format = (\"AsyncSSLConnection\"\n                          \"<host=%(host)s,port=%(port)s,db=%(db)s>\")\n\n    def __init__(self, ssl_keyfile=None, ssl_certfile=None, ssl_cert_reqs=None,\n                 ssl_ca_certs=None, **kwargs):\n        if not SSL_AVAILABLE:\n            raise RedisError(\"Python wasn't built with SSL support\")\n\n        if ssl_cert_reqs is None:\n            ssl_cert_reqs = ssl.CERT_NONE\n        elif isinstance(ssl_cert_reqs, basestring):\n            CERT_REQS = {\n                'none': ssl.CERT_NONE,\n                'optional': ssl.CERT_OPTIONAL,\n                'required': ssl.CERT_REQUIRED\n            }\n            if ssl_cert_reqs not in CERT_REQS:\n                raise RedisError(\n                    \"Invalid SSL Certificate Requirements Flag: %s\" %\n                    ssl_cert_reqs)\n            ssl_cert_reqs = CERT_REQS[ssl_cert_reqs]\n\n        self.ssl_options = {\n            'keyfile': ssl_keyfile,\n            'certfile': ssl_certfile,\n            'ca_certs': ssl_ca_certs,\n            'cert_reqs': ssl_cert_reqs,\n        }\n\n        super(AsyncSSLConnection, self).__init__(**kwargs)\n\n    def _wrap_socket(self, sock):\n        return SSLIOStream(sock, ssl_options=self.ssl_options)\n","license":"mit","hash":9107227227048309079,"line_mean":35.5727923628,"line_max":81,"alpha_frac":0.5664969982,"autogenerated":false},
{"repo_name":"WesleyPeng\/uiXautomation","path":"src\/main\/python\/taf\/foundation\/plugins\/web\/selenium\/controls\/radiogroup.py","copies":"1","size":"3429","content":"# Copyright (c) 2017-2018 {Flair Inc.} WESLEY PENG\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import namedtuple\n\nfrom taf.foundation.api.ui.controls import RadioGroup as IRadioGroup\nfrom taf.foundation.plugins.web.selenium.controls.edit import Edit\nfrom taf.foundation.plugins.web.selenium.controls.listitem import ListItem\nfrom taf.foundation.plugins.web.selenium.support.elementfinder import \\\n    ElementFinder\nfrom taf.foundation.plugins.web.selenium.support.locator import Locator\nfrom taf.foundation.plugins.web.selenium.webelement import WebElement\n\nRadioButton = namedtuple(\n    'RadioButton', ['option', 'label']\n)\n\n\nclass RadioGroup(WebElement, IRadioGroup):\n    def __init__(\n            self, *elements, **conditions\n    ):\n        conditions.setdefault('tag', 'form')\n\n        _options_kwarg = 'option'\n        _opt_label_kwarg = 'label'\n\n        self._options_tag = conditions.pop(\n            _options_kwarg\n        ) if _options_kwarg in conditions else 'input'\n\n        self._label_tag = conditions.pop(\n            _opt_label_kwarg\n        ) if _options_kwarg in conditions else _opt_label_kwarg\n\n        WebElement.__init__(self, *elements, **conditions)\n\n    def set(self, value):\n        if str(value).isdigit() and (\n                int(value) < len(list(self.items))\n        ):\n            list(self.items)[int(value)].option.select()\n        else:\n            for item in self.items:\n                if (\n                        value == item.option.current.get_attribute('value')\n                ) or (value == item.label.text):\n                    item.option.select()\n                    break\n            else:\n                raise ValueError(\n                    'Could not locate element with value: {}'.format(\n                        value\n                    )\n                )\n\n    @property\n    def value(self):\n        if self.exists():\n            for item in self.items:\n                if item.option.is_selected:\n                    return item.label.text\n\n        return r''\n\n    @property\n    def items(self):\n        if not self._children:\n            if self.exists():\n                _finder = ElementFinder(self.object).find_elements\n                self._children = [\n                    RadioButton(\n                        ListItem(element=option, parent=self),\n                        Edit(element=label, parent=self)\n                    ) for option, label in zip(\n                        _finder(\n                            Locator.XPATH,\n                            '.\/\/{}[@type=radio]'.format(\n                                self._options_tag\n                            )\n                        ),\n                        _finder(\n                            Locator.XPATH,\n                            '.\/\/{}'.format(self._label_tag)\n                        )\n                    )\n                ]\n\n        return (child for child in self._children)\n","license":"apache-2.0","hash":-6344076500846791389,"line_mean":33.29,"line_max":75,"alpha_frac":0.5488480607,"autogenerated":false},
{"repo_name":"marcusmartins\/django-beagle","path":"tests\/test_middleware.py","copies":"1","size":"3132","content":"\"\"\"Tests for the middlewares of the influxdb_metrics app.\"\"\"\nfrom django.test import TestCase, RequestFactory\nfrom django.contrib.auth.models import AnonymousUser, User\n\nfrom beagle.middleware import MetricsRequestMiddleware\n\n\nclass MetricsRequestMiddlewareTestCase(TestCase):\n    \"\"\"Tests for the ``MetricsRequestMiddleware`` middleware.\"\"\"\n\n    def setUp(self):\n        super(MetricsRequestMiddlewareTestCase, self).setUp()\n        self.staff = User.objects.create(username='staff', is_staff=True)\n        self.superuser = User.objects.create(\n            username='superuser', is_superuser=True)\n\n    def tearDown(self):\n        super(MetricsRequestMiddlewareTestCase, self).tearDown()\n\n    def test_get(self):\n        req = RequestFactory().get('\/')\n        req.user = AnonymousUser()\n        mware = MetricsRequestMiddleware()\n        mware.process_view(req, 'view_funx', 'view_args', 'view_kwargs')\n        resp = mware.record_time(req)\n        self.assertIsNotNone(resp)\n        timing, tags = resp\n        self.assertGreater(timing, 0.000)\n        self.assertIn('is_authenticated:False', tags)\n        self.assertIn('is_superuser:False', tags)\n        self.assertIn('is_staff:False', tags)\n        self.assertIn('is_ajax:False', tags)\n        self.assertIn('path:\/', tags)\n        self.assertIn('method:GET', tags)\n\n    def test_get_long_path(self):\n        req = RequestFactory().get('\/mysuper\/long\/path')\n        req.user = AnonymousUser()\n        mware = MetricsRequestMiddleware()\n        mware.process_view(req, 'view_funx', 'view_args', 'view_kwargs')\n        resp = mware.record_time(req)\n        self.assertIsNotNone(resp)\n        timing, tags = resp\n        self.assertGreater(timing, 0.000)\n        self.assertIn('path:\/mysuper\/long\/path', tags)\n\n    def test_post(self):\n        req = RequestFactory().post('\/')\n        req.user = AnonymousUser()\n        mware = MetricsRequestMiddleware()\n        mware.process_view(req, 'view_funx', 'view_args', 'view_kwargs')\n        resp = mware.record_time(req)\n        self.assertIsNotNone(resp)\n        timing, tags = resp\n        self.assertGreater(timing, 0.000)\n        self.assertIn('method:POST', tags)\n\n    def test_as_staff(self):\n        req = RequestFactory().get('\/')\n        req.user = self.staff\n        mware = MetricsRequestMiddleware()\n        mware.process_view(req, 'view_funx', 'view_args', 'view_kwargs')\n        resp = mware.record_time(req)\n        self.assertIsNotNone(resp)\n        timing, tags = resp\n        self.assertGreater(timing, 0.000)\n        self.assertIn('is_authenticated:True', tags)\n        self.assertIn('is_staff:True', tags)\n\n    def test_without_user(self):\n        req = RequestFactory().get('\/')\n        req.user = None\n        mware = MetricsRequestMiddleware()\n        mware.process_view(req, 'view_funx', 'view_args', 'view_kwargs')\n        resp = mware.record_time(req)\n        self.assertIsNotNone(resp)\n        timing, tags = resp\n        self.assertGreater(timing, 0.000)\n        self.assertIn('is_authenticated:False', tags)\n        self.assertIn('is_staff:False', tags)\n        self.assertIn('is_superuser:False', tags)\n","license":"apache-2.0","hash":-8372463236172879288,"line_mean":37.6666666667,"line_max":73,"alpha_frac":0.6360153257,"autogenerated":false},
{"repo_name":"kz26\/dottorrent","path":"dottorrent\/__init__.py","copies":"1","size":"13655","content":"# MIT License\n\n# Copyright (c) 2016 Kevin Zhang\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nfrom base64 import b32encode\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom hashlib import sha1, md5\nimport fnmatch\nimport math\nimport os\nimport sys\nfrom urllib.parse import urlparse\n\nfrom bencoder import bencode\n\nfrom .version import __version__\nfrom . import exceptions\n\nDEFAULT_CREATOR = \"dottorrent\/{} (https:\/\/github.com\/kz26\/dottorrent)\".format(\n    __version__)\n\n\nMIN_PIECE_SIZE = 2 ** 14\nMAX_PIECE_SIZE = 2 ** 26\n\n\nif sys.version_info >= (3, 5) and os.name == 'nt':\n    import stat\n\n    def is_hidden_file(path):\n        fn = path.split(os.sep)[-1]\n        return fn.startswith('.') or \\\n            bool(os.stat(path).st_file_attributes &\n                 stat.FILE_ATTRIBUTE_HIDDEN)\nelse:\n    def is_hidden_file(path):\n        fn = path.split(os.sep)[-1]\n        return fn.startswith('.')\n\n\ndef print_err(v):\n    print(v, file=sys.stderr)\n\n\nclass Torrent(object):\n\n    def __init__(self, path, trackers=None, web_seeds=None,\n                 piece_size=None, private=False, source=None,\n                 creation_date=None, comment=None, created_by=None,\n                 include_md5=False, exclude=None):\n        \"\"\"\n        :param path: path to a file or directory from which to create the torrent\n        :param trackers: list\/iterable of tracker URLs\n        :param web_seeds: list\/iterable of HTTP\/FTP seed URLs\n        :param piece_size: Piece size in bytes. Must be >= 16 KB and a power of 2.\n            If None, ``get_info()`` will be used to automatically select a piece size.\n        :param private: The private flag. If True, DHT\/PEX will be disabled.\n        :param source: An optional source string for the torrent.\n        :param exclude: A list of filename patterns that should be excluded from the torrent.\n        :param creation_date: An optional datetime object representing the torrent creation date.\n        :param comment: An optional comment string for the torrent.\n        :param created_by: name\/version of the program used to create the .torrent.\n            If None, defaults to the value of ``DEFAULT_CREATOR``.\n        :param include_md5: If True, also computes and stores MD5 hashes for each file.\n        \"\"\"\n\n        self.path = os.path.normpath(path)\n        self.trackers = trackers\n        self.web_seeds = web_seeds\n        self.piece_size = piece_size\n        self.private = private\n        self.source = source\n        self.exclude = [] if exclude is None else exclude\n        self.creation_date = creation_date\n        self.comment = comment\n        self.created_by = created_by\n        self.include_md5 = include_md5\n\n        self._data = None\n\n    @property\n    def trackers(self):\n        return self._trackers\n\n    @trackers.setter\n    def trackers(self, value):\n        tl = []\n        if value:\n            for t in value:\n                pr = urlparse(t)\n                if pr.scheme and pr.netloc:\n                    tl.append(t)\n                else:\n                    raise exceptions.InvalidURLException(t)\n        self._trackers = tl\n\n    @property\n    def web_seeds(self):\n        return self._web_seeds\n\n    @web_seeds.setter\n    def web_seeds(self, value):\n        tl = []\n        if value:\n            for t in value:\n                pr = urlparse(t)\n                if pr.scheme and pr.netloc:\n                    tl.append(t)\n                else:\n                    raise exceptions.InvalidURLException(t)\n        self._web_seeds = tl\n\n    @property\n    def piece_size(self):\n        return self._piece_size\n\n    @piece_size.setter\n    def piece_size(self, value):\n        if value:\n            value = int(value)\n            if value > 0 and (value & (value-1) == 0):\n                if value < MIN_PIECE_SIZE:\n                    raise exceptions.InvalidPieceSizeException(\n                        \"Piece size should be at least 16 KiB\")\n                if value > MAX_PIECE_SIZE:\n                    print_err(\"Warning: piece size is greater than 64 MiB\")\n                self._piece_size = value\n            else:\n                raise exceptions.InvalidPieceSizeException(\n                    \"Piece size must be a power of 2 bytes\")\n        else:\n            self._piece_size = None\n\n    def get_info(self):\n        \"\"\"\n        Scans the input path and automatically determines the optimal\n        piece size based on ~1500 pieces (up to MAX_PIECE_SIZE) along\n        with other basic info, including total size (in bytes), the\n        total number of files, piece size (in bytes), and resulting\n        number of pieces. If ``piece_size`` has already been set, the\n        custom value will be used instead.\n\n        :return: ``(total_size, total_files, piece_size, num_pieces)``\n        \"\"\"\n        if os.path.isfile(self.path):\n            total_size = os.path.getsize(self.path)\n            total_files = 1\n        elif os.path.exists(self.path):\n            total_size = 0\n            total_files = 0\n            for x in os.walk(self.path):\n                for fn in x[2]:\n                    if any(fnmatch.fnmatch(fn, ext) for ext in self.exclude):\n                        continue\n                    fpath = os.path.normpath(os.path.join(x[0], fn))\n                    fsize = os.path.getsize(fpath)\n                    if fsize and not is_hidden_file(fpath):\n                        total_size += fsize\n                        total_files += 1\n        else:\n            raise exceptions.InvalidInputException\n        if not (total_files and total_size):\n            raise exceptions.EmptyInputException\n        if self.piece_size:\n            ps = self.piece_size\n        else:\n            ps = 1 << max(0, math.ceil(math.log(total_size \/ 1500, 2)))\n            if ps < MIN_PIECE_SIZE:\n                ps = MIN_PIECE_SIZE\n            if ps > MAX_PIECE_SIZE:\n                ps = MAX_PIECE_SIZE\n        return (total_size, total_files, ps, math.ceil(total_size \/ ps))\n\n    def generate(self, callback=None):\n        \"\"\"\n        Computes and stores piece data. Returns ``True`` on success, ``False``\n        otherwise.\n\n        :param callback: progress\/cancellation callable with method\n            signature ``(filename, pieces_completed, pieces_total)``.\n            Useful for reporting progress if dottorrent is used in a\n            GUI\/threaded context, and if torrent generation needs to be cancelled.\n            The callable's return value should evaluate to ``True`` to trigger\n            cancellation.\n        \"\"\"\n        files = []\n        single_file = os.path.isfile(self.path)\n        if single_file:\n            files.append((self.path, os.path.getsize(self.path), {}))\n        elif os.path.exists(self.path):\n            for x in os.walk(self.path):\n                for fn in x[2]:\n                    if any(fnmatch.fnmatch(fn, ext) for ext in self.exclude):\n                        continue\n                    fpath = os.path.normpath(os.path.join(x[0], fn))\n                    fsize = os.path.getsize(fpath)\n                    if fsize and not is_hidden_file(fpath):\n                        files.append((fpath, fsize, {}))\n        else:\n            raise exceptions.InvalidInputException\n        total_size = sum([x[1] for x in files])\n        if not (len(files) and total_size):\n            raise exceptions.EmptyInputException\n        # set piece size if not already set\n        if self.piece_size is None:\n            self.piece_size = self.get_info()[2]\n        if files:\n            self._pieces = bytearray()\n            i = 0\n            num_pieces = math.ceil(total_size \/ self.piece_size)\n            pc = 0\n            buf = bytearray()\n            while i < len(files):\n                fe = files[i]\n                f = open(fe[0], 'rb')\n                if self.include_md5:\n                    md5_hasher = md5()\n                else:\n                    md5_hasher = None\n                for chunk in iter(lambda: f.read(self.piece_size), b''):\n                    buf += chunk\n                    if len(buf) >= self.piece_size \\\n                            or i == len(files)-1:\n                        piece = buf[:self.piece_size]\n                        self._pieces += sha1(piece).digest()\n                        del buf[:self.piece_size]\n                        pc += 1\n                        if callback:\n                            cancel = callback(fe[0], pc, num_pieces)\n                            if cancel:\n                                f.close()\n                                return False\n                    if self.include_md5:\n                        md5_hasher.update(chunk)\n                if self.include_md5:\n                    fe[2]['md5sum'] = md5_hasher.hexdigest()\n                f.close()\n                i += 1\n            # Add pieces from any remaining data\n            while len(buf):\n                piece = buf[:self.piece_size]\n                self._pieces += sha1(piece).digest()\n                del buf[:self.piece_size]\n                pc += 1\n                if callback:\n                    cancel = callback(fe[0], pc, num_pieces)\n                    if cancel:\n                        return False\n\n        # Create the torrent data structure\n        data = OrderedDict()\n        if len(self.trackers) > 0:\n            data['announce'] = self.trackers[0].encode()\n            if len(self.trackers) > 1:\n                data['announce-list'] = [[x.encode()] for x in self.trackers]\n        if self.comment:\n            data['comment'] = self.comment.encode()\n        if self.created_by:\n            data['created by'] = self.created_by.encode()\n        else:\n            data['created by'] = DEFAULT_CREATOR.encode()\n        if self.creation_date:\n            data['creation date'] = int(self.creation_date.timestamp())\n        if self.web_seeds:\n            data['url-list'] = [x.encode() for x in self.web_seeds]\n        data['info'] = OrderedDict()\n        if single_file:\n            data['info']['length'] = files[0][1]\n            if self.include_md5:\n                data['info']['md5sum'] = files[0][2]['md5sum']\n            data['info']['name'] = files[0][0].split(os.sep)[-1].encode()\n        else:\n            data['info']['files'] = []\n            path_sp = self.path.split(os.sep)\n            for x in files:\n                fx = OrderedDict()\n                fx['length'] = x[1]\n                if self.include_md5:\n                    fx['md5sum'] = x[2]['md5sum']\n                fx['path'] = [y.encode()\n                              for y in x[0].split(os.sep)[len(path_sp):]]\n                data['info']['files'].append(fx)\n            data['info']['name'] = path_sp[-1].encode()\n        data['info']['pieces'] = bytes(self._pieces)\n        data['info']['piece length'] = self.piece_size\n        data['info']['private'] = int(self.private)\n        if self.source:\n            data['info']['source'] = self.source.encode()\n\n        self._data = data\n        return True\n\n    @property\n    def data(self):\n        \"\"\"\n        Returns the data dictionary for the torrent.\n\n        .. note:: ``generate()`` must be called first.\n        \"\"\"\n    \n        if self._data:\n            return self._data\n        else:\n            raise exceptions.TorrentNotGeneratedException\n\n    @property\n    def info_hash_base32(self):\n        \"\"\"\n        Returns the base32 info hash of the torrent. Useful for generating\n        magnet links.\n\n        .. note:: ``generate()`` must be called first.\n        \"\"\"\n        if getattr(self, '_data', None):\n            return b32encode(sha1(bencode(self._data['info'])).digest())\n        else:\n            raise exceptions.TorrentNotGeneratedException\n\n    @property\n    def info_hash(self):\n        \"\"\"\n        :return: The SHA-1 info hash of the torrent. Useful for generating\n            magnet links.\n\n        .. note:: ``generate()`` must be called first.\n        \"\"\"\n        if getattr(self, '_data', None):\n            return sha1(bencode(self._data['info'])).hexdigest()\n        else:\n            raise exceptions.TorrentNotGeneratedException\n\n    def dump(self):\n        \"\"\"\n        :return: The bencoded torrent data as a byte string.\n\n        .. note:: ``generate()`` must be called first.\n        \"\"\"\n        if getattr(self, '_data', None):\n\n            return bencode(self._data)\n        else:\n            raise exceptions.TorrentNotGeneratedException\n\n    def save(self, fp):\n        \"\"\"\n        Saves the torrent to ``fp``, a file(-like) object\n        opened in binary writing (``wb``) mode.\n\n        .. note:: ``generate()`` must be called first.\n        \"\"\"\n        fp.write(self.dump())\n","license":"mit","hash":7388105224268803603,"line_mean":35.8059299191,"line_max":97,"alpha_frac":0.5453679971,"autogenerated":false},
{"repo_name":"matt-gardner\/deep_qa","path":"tests\/data\/instances\/text_classification\/text_classification_instance_test.py","copies":"1","size":"3715","content":"# pylint: disable=no-self-use,invalid-name\nimport numpy\n\nfrom deep_qa.data.instances.text_classification import IndexedTextClassificationInstance\nfrom deep_qa.data.instances.text_classification import TextClassificationInstance\nfrom ....common.test_case import DeepQaTestCase\n\n\nclass TestTextClassificationInstance:\n    @staticmethod\n    def instance_to_line(text, label=None, index=None):\n        line = ''\n        if index is not None:\n            line += str(index) + '\\t'\n        line += text\n        if label is not None:\n            label_str = '1' if label else '0'\n            line += '\\t' + label_str\n        return line\n\n    def test_read_from_line_handles_one_column(self):\n        text = \"this is a sentence\"\n        instance = TextClassificationInstance.read_from_line(text)\n        assert instance.text == text\n        assert instance.label is None\n        assert instance.index is None\n\n    def test_read_from_line_handles_three_column(self):\n        index = 23\n        text = \"this is a sentence\"\n        label = True\n        line = self.instance_to_line(text, label, index)\n\n        instance = TextClassificationInstance.read_from_line(line)\n        assert instance.text == text\n        assert instance.label is label\n        assert instance.index == index\n\n    def test_read_from_line_handles_two_column_with_label(self):\n        index = None\n        text = \"this is a sentence\"\n        label = True\n        line = self.instance_to_line(text, label, index)\n\n        instance = TextClassificationInstance.read_from_line(line)\n        assert instance.text == text\n        assert instance.label is label\n        assert instance.index == index\n\n    def test_read_from_line_handles_two_column_with_index(self):\n        index = 23\n        text = \"this is a sentence\"\n        label = None\n        line = self.instance_to_line(text, label, index)\n\n        instance = TextClassificationInstance.read_from_line(line)\n        assert instance.text == text\n        assert instance.label is label\n        assert instance.index == index\n\n    def test_words_tokenizes_the_sentence_correctly(self):\n        t = TextClassificationInstance(\"This is a sentence.\", None)\n        assert t.words() == {'words': ['this', 'is', 'a', 'sentence', '.']}\n        t = TextClassificationInstance(\"This isn't a sentence.\", None)\n        assert t.words() == {'words': ['this', 'is', \"n't\", 'a', 'sentence', '.']}\n        t = TextClassificationInstance(\"And, I have commas.\", None)\n        assert t.words() == {'words': ['and', ',', 'i', 'have', 'commas', '.']}\n\n\nclass TestIndexedTextClassificationInstance(DeepQaTestCase):\n    def test_get_padding_lengths_returns_length_of_word_indices(self):\n        instance = IndexedTextClassificationInstance([1, 2, 3, 4], True)\n        assert instance.get_padding_lengths() == {'num_sentence_words': 4}\n\n    def test_pad_adds_zeros_on_left(self):\n        instance = IndexedTextClassificationInstance([1, 2, 3, 4], True)\n        instance.pad({'num_sentence_words': 5})\n        assert instance.word_indices == [0, 1, 2, 3, 4]\n\n    def test_pad_truncates_from_right(self):\n        instance = IndexedTextClassificationInstance([1, 2, 3, 4], True)\n        instance.pad({'num_sentence_words': 3})\n        assert instance.word_indices == [2, 3, 4]\n\n    def test_as_training_data_produces_correct_numpy_arrays(self):\n        instance = IndexedTextClassificationInstance([1, 2, 3, 4], True)\n        inputs, label = instance.as_training_data()\n        assert numpy.all(label == numpy.asarray([0, 1]))\n        assert numpy.all(inputs == numpy.asarray([1, 2, 3, 4]))\n        instance.label = False\n        _, label = instance.as_training_data()\n        assert numpy.all(label == numpy.asarray([1, 0]))\n","license":"apache-2.0","hash":-2023674013320152435,"line_mean":39.3804347826,"line_max":88,"alpha_frac":0.6355316285,"autogenerated":false},
{"repo_name":"MaximeKjaer\/such-tip-stats","path":"suchDelay3.3.1.py","copies":"1","size":"6446","content":"# -*- coding: cp1252 -*-\nimport praw\nimport time\nimport json\nfrom collections import Counter\nimport os\nimport zipfile\n\ndata = []\nlimit = 5\n\n########################\n####### CRAWLERS #######\n########################\n\ndef crawl(comments):\n    i = 1\n    data = []\n    print \"Getting an average delay time over the last 5 verifications.\"\n    for comment in comments:\n        #Print progress\n        print i\n        i = i+1\n        #Crawl\n        post_id = comment.id\n        print post_id\n        verification_created = comment.created_utc #\n        parent = r.get_info(thing_id=comment.parent_id) # ONE API call --> 2 seconds.\n        parent_created = parent.created_utc #\n        verification_delay = verification_created - parent_created #\n        data.append({'post_id': post_id,\n                     'verification_created': int(verification_created),\n                     'parent_created': int(parent_created),\n                     'verification_delay': int(verification_delay)\n                     })\n    return data\n\ndef hourly_crawl(comments):\n    i = 1\n    data = []\n    subreddits = []\n    tipped = []\n    for comment in comments:\n        if i==1:\n            newest = comment.created_utc\n        if comment.created_utc < newest-3600:\n            print \"The latest hour's verifications have now been fetched\"\n            break\n        else:\n            subreddits.append(str(comment.subreddit))\n            tipped.append(float(comment.body.encode('utf-8').split(' ^Dogecoins')[0].split('__^\\xc3\\x90')[-1]))\n            last = comment.created_utc\n        i = i+1\n    print str(i) + \" comments in an hour\"\n    data = {'amount_tipped': int(sum(tipped)),\n             'many_comments': i-1,\n             'hour': time.strftime('%H', time.gmtime(last)),\n             'date': time.strftime('%d\/%m\/%y', time.gmtime(last)),\n             'time': last,\n             'subs': Counter(subreddits) }\n    return data\n\n########################\n###### CRUNCHERS #######\n########################\n\ndef daily_cruncher(days=1, save=False):\n    \"\"\" Creates a file from dogetipdata2.json with the last day's worth of data. \"\"\"\n    day = []\n    with open('server\/JSON\/dogetipdata2.json') as f:\n        dogetipdata = json.load(f)\n    for a in dogetipdata:\n        if a[0] >= (dogetipdata[-1][0] - (86400*days)): # 86400 seconds in a day\n            day.append([a[0],a[1]])\n    if save == True and days == 1:\n        with open('server\/JSON\/24h.json', 'wb') as f:\n            json.dump(day, f)\n    return day\n\ndef records_cruncher():\n    \"\"\" Creates records.json with all-time records \"\"\"\n    with open('server\/JSON\/hourly.json') as f:\n        hourly = json.load(f)\n    #Greatest hourly tipping rate\n    max_tips = greatest('amount_tipped', hourly)\n    max_verifications = greatest('many_comments', hourly)\n    records = {'tips': max_tips,\n                'verifications': max_verifications}\n    with open('server\/JSON\/records.json', 'wb') as f:\n        json.dump(records, f)\n    return records\n\n\ndef frontpage_data():\n    \"\"\" Creates frontpagedata.json with all the data the front page needs \"\"\"\n    delay = daily_cruncher(2, False)\n    records = records_cruncher()\n    with open('server\/JSON\/hourly.json') as f:\n        hourly_report = json.load(f)[-1]\n    with open('server\/JSON\/frontpage.json', 'wb') as f:\n        json.dump({'delay': delay,\n                   'hourly': hourly_report,\n                   'records': records}, f)\n\ndef zip_it_up():\n    \"\"\" Creates a zip file with all the JSON, returns the unzipped size\"\"\"\n    zf = zipfile.ZipFile(\"server\/dogecoin_tip_data.zip\", \"w\")\n    folder_size = 0\n    for dirname, subdirs, files in os.walk('server\/JSON'):\n        zf.write(dirname)\n        for filename in files:\n            zf.write(os.path.join(dirname, filename))\n            folder_size += os.path.getsize(os.path.join(dirname, filename))\n    zf.close()\n    print \"ZIP file created.\"\n    data_size = str(folder_size\/1024) + \" KB\"\n    with open('server\/JSON\/frontpage.json') as f:\n        almost_ready_data = json.load(f)\n    almost_ready_data['data_size'] = data_size\n    with open('server\/JSON\/frontpage.json', 'wb') as f:\n        json.dump(almost_ready_data, f)\n\n\n\n\n########################\n###### COUNTERS ########\n########################\n\ndef average(what, data):\n    list_numbers = []\n    for number in data:\n        list_numbers.append(number[what])\n    average = sum(list_numbers) \/ float(len(list_numbers))\n    return average\n\ndef greatest(what, listed_object_data):\n    b = {what: 0}\n    for a in listed_object_data:\n        if a[what] >= b[what]:\n            b = a\n    return b       \n\ndef mode(what, data): # TO DO!\n    pass\n\n########################\n######## LOGIC #########\n########################\n\nr = praw.Reddit('Such Dogetipbot Delay (collecting stats about \/u\/dogetipbot) v0.1 by \/u\/MaximaxII')\n\ntipbot = r.get_redditor('dogetipbot')\nj = 4 #Depends on what time you launch it!\n#0 for 15, 1 for 30, 2 for 45, 4 for 00\nwhile True:\n    j = j+1\n    start = time.time()\n    ## VERIFICATION DELAY ##\n    comments = tipbot.get_comments(limit=limit)\n    data = crawl(comments) # 10 seconds\n    short_data = [average('verification_created', data), average('verification_delay', data)] #x,y pair\n    \n    with open('server\/JSON\/dogetipdata2.json') as f:\n        json_data = json.load(f)\n    json_data.append(short_data)\n    with open('server\/JSON\/dogetipdata2.json', 'wb') as f:\n        json.dump(json_data, f)\n        \n    print 'Average delay for the last ' + str(limit) + ' verifications: ' + str(average('verification_delay', data))\n    print 'Average time for the last ' + str(limit) + ' verifications: ' + str(average('verification_created', data))\n    json_data = []\n    data = []\n    ## HOURLY STATS ##\n    if j == 4:\n        #The maximum is supposed to be 300... but it sometimes exceeds that, somehow. So 500 it is.\n        comments = tipbot.get_comments(limit=500) \n        data2 = hourly_crawl(comments)\n        with open('server\/JSON\/hourly.json') as f:\n            json_data = json.load(f)\n        json_data.append(data2)\n        with open('server\/JSON\/hourly.json', 'wb') as f:\n            json.dump(json_data, f)\n        print \"HOURLY scan is now done\"\n        j = 0\n        data2 = []\n        json_data = []\n        \n    frontpage_data()\n    daily_cruncher(1,True)\n    zip_it_up()\n    end = time.time()\n    print \"Going to sleep. Execution time was: \" + str(end - start)\n    time.sleep(900 - (end - start)) # exactly 15 minutes","license":"mit","hash":-8269656168473488778,"line_mean":32.2319587629,"line_max":117,"alpha_frac":0.5702761402,"autogenerated":false},
{"repo_name":"evanbiederstedt\/RRBSfun","path":"epiphen\/total_chr10.py","copies":"2","size":"32998","content":"import glob\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 50) # print all rows\n\n\nimport os\nos.chdir(\"\/gpfs\/commons\/home\/biederstedte-934\/evan_projects\/correct_phylo_files\")\n\n\nnormalB = glob.glob(\"binary_position_RRBS_normal_B_cell*\")\nmcell = glob.glob(\"binary_position_RRBS_NormalBCD19pCD27mcell*\")\npcell = glob.glob(\"binary_position_RRBS_NormalBCD19pCD27pcell*\")\ncd19cell = glob.glob(\"binary_position_RRBS_NormalBCD19pcell*\")\ncw154 = glob.glob(\"binary_position_RRBS_cw154*\")\ntrito = glob.glob(\"binary_position_RRBS_trito_pool*\")\n\n\nprint(len(normalB))\nprint(len(mcell))\nprint(len(pcell))\nprint(len(cd19cell))\nprint(len(cw154))\nprint(len(trito))\n\ntotalfiles = normalB + mcell + pcell + cd19cell + cw154 + trito\nprint(len(totalfiles))\n\ndf_list = []\nfor file in totalfiles:\n    df = pd.read_csv(file)\n    df = df.drop(\"Unnamed: 0\", axis=1)\n    df[\"chromosome\"] = df[\"position\"].map(lambda x: str(x)[:5])\n    df = df[df[\"chromosome\"] == \"chr10\"]\n    df = df.drop(\"chromosome\", axis=1)\n    df_list.append(df)\n\n\nprint(len(df_list))\n\n\ntotal_matrix = pd.concat([df.set_index(\"position\") for df in df_list], axis=1).reset_index().astype(object)\n\ntotal_matrix = total_matrix.drop(\"index\", axis=1)\n\nlen(total_matrix.columns)\n\n\ntotal_matrix.columns = [\"RRBS_normal_B_cell_A1_24_TAAGGCGA.ACAACC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.ACCGCG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.ACGTGG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.AGGATG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.ATAGCG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.ATCGAC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.CAAGAG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.CATGAC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.CGGTAG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.CTATTG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.CTCAGC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.GACACG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.GCTGCC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.GGCATC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.GTGAGG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.GTTGAG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.TAGCGG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.TATCTC\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.TCTCTG\",\n                        \"RRBS_normal_B_cell_A1_24_TAAGGCGA.TGACAG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.ACAACC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.ACCGCG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.ACTCAC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.ATAGCG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CAAGAG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CATGAC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CCTTCG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CGGTAG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CTATTG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.CTCAGC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.GACACG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.GCATTC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.GGCATC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.GTGAGG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.GTTGAG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.TAGCGG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.TATCTC\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.TCTCTG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.TGACAG\",\n                        \"RRBS_normal_B_cell_B1_24_CGTACTAG.TGCTGC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ACAACC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ACCGCG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ACGTGG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ACTCAC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.AGGATG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ATAGCG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.ATCGAC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.CAAGAG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.CATGAC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.CGGTAG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.CTATTG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GACACG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GCATTC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GCTGCC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GGCATC\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GTGAGG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.GTTGAG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.TAGCGG\",\n                        \"RRBS_normal_B_cell_C1_24_AGGCAGAA.TATCTC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.ACAACC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.ACCGCG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.ACGTGG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.ACTCAC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.AGGATG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.ATCGAC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CAAGAG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CATGAC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CCTTCG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CGGTAG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CTATTG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.CTCAGC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.GACACG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.GCATTC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.GCTGCC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.GGCATC\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.GTTGAG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.TAGCGG\",\n                        \"RRBS_normal_B_cell_D1_24_TCCTGAGC.TATCTC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ACAACC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ACCGCG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ACGTGG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ACTCAC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.AGGATG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ATAGCG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.ATCGAC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.CAAGAG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.CATGAC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.CGGTAG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.CTATTG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.CTCAGC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.GACACG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.GCATTC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.GCTGCC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.GGCATC\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.GTGAGG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.TAGCGG\",\n                        \"RRBS_normal_B_cell_G1_22_GGACTCCT.TATCTC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.ACCGCG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.ACGTGG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.ACTCAC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.AGGATG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.ATCGAC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.CAAGAG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.CATGAC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.CCTTCG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.CTATTG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.CTCAGC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.GCATTC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.GCTGCC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.GGCATC\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.GTGAGG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.GTTGAG\",\n                        \"RRBS_normal_B_cell_H1_22_TAGGCATG.TCTCTG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GACACG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GCTGCC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GGCATC\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27mcell1_22_CGAGGCTG.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.GACACG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27mcell23_44_GTAGAGGA.TCTCTG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.GACACG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27mcell45_66_TAAGGCGA.TCTCTG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.GACACG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.GGCATC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27mcell67_88_CGTACTAG.TCTCTG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.GCTGCC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.GGCATC\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell1_22_TAGGCATG.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GACACG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GCTGCC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GGCATC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27pcell23_44_CTCTCTAC.TCTCTG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.CAAGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.GACACG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27pcell45_66_CAGAGAGG.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ACAACC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ACCGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ACGTGG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ACTCAC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.AGGATG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ATAGCG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.ATCGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.CATGAC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.CCTTCG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.CGGTAG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.CTATTG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.CTCAGC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GACACG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GCATTC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GCTGCC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GGCATC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GTGAGG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.GTTGAG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.TAGCGG\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.TATCTC\",\n                        \"RRBS_NormalBCD19pCD27pcell67_88_GCTACGCT.TCTCTG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ACAACC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ACCGCG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ACGTGG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ACTCAC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.AGGATG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ATAGCG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.ATCGAC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CAAGAG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CATGAC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CCTTCG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CGGTAG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CTATTG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.CTCAGC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.GACACG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.GCATTC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.GCTGCC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.GGCATC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.GTTGAG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.TAGCGG\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.TATCTC\",\n                        \"RRBS_NormalBCD19pcell1_22_TAAGGCGA.TCTCTG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ACAACC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ACCGCG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ACGTGG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ACTCAC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.AGGATG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ATAGCG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.ATCGAC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.CATGAC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.CCTTCG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.CGGTAG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.CTATTG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.CTCAGC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.GACACG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.GCATTC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.GCTGCC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.GGCATC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.GTGAGG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.TAGCGG\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.TATCTC\",\n                        \"RRBS_NormalBCD19pcell23_44_CGTACTAG.TCTCTG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ACAACC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ACCGCG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ACGTGG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ACTCAC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.AGGATG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ATAGCG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.ATCGAC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CAAGAG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CATGAC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CCTTCG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CGGTAG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CTATTG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.CTCAGC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GACACG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GCATTC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GCTGCC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GGCATC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GTGAGG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.GTTGAG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.TAGCGG\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.TATCTC\",\n                        \"RRBS_NormalBCD19pcell45_66_AGGCAGAA.TCTCTG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ACAACC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ACCGCG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ACGTGG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ACTCAC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.AGGATG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ATAGCG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.ATCGAC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CAAGAG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CATGAC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CCTTCG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CGGTAG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CTATTG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.CTCAGC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.GCATTC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.GCTGCC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.GGCATC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.GTGAGG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.GTTGAG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.TAGCGG\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.TATCTC\",\n                        \"RRBS_NormalBCD19pcell67_88_TCCTGAGC.TCTCTG\",\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ACAACC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ACCGCG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ACGTGG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ACTCAC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.AGGATG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ATAGCG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.ATCGAC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.CAAGAG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.CATGAC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.CCTTCG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.CGGTAG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.CTCAGC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.GACACG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.GCATTC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.GCTGCC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.GGCATC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.GTGAGG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.TAGCGG',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.TATCTC',\n                        'RRBS_cw154_CutSmart_proteinase_K_TAGGCATG.TCTCTG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ACAACC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ACCGCG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ACGTGG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ACTCAC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.AGGATG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ATAGCG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.ATCGAC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.CATGAC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.CCTTCG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.CGGTAG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.CTATTG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.CTCAGC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GACACG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GCATTC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GCTGCC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GGCATC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GTGAGG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.GTTGAG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.TAGCGG',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.TATCTC',\n                        'RRBS_cw154_Tris_protease_CTCTCTAC.TCTCTG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ACAACC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ACCGCG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ACGTGG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ACTCAC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.AGGATG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ATAGCG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.ATCGAC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.CATGAC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.CCTTCG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.CGGTAG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.CTATTG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.CTCAGC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GACACG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GCATTC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GCTGCC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GGCATC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GTGAGG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.GTTGAG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.TAGCGG',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.TATCTC',\n                        'RRBS_cw154_Tris_protease_GR_CAGAGAGG.TCTCTG',\n                        'RRBS_trito_pool_1_TAAGGCGA.ACAACC',\n                        'RRBS_trito_pool_1_TAAGGCGA.ACGTGG',\n                        'RRBS_trito_pool_1_TAAGGCGA.ACTCAC',\n                        'RRBS_trito_pool_1_TAAGGCGA.ATAGCG',\n                        'RRBS_trito_pool_1_TAAGGCGA.ATCGAC',\n                        'RRBS_trito_pool_1_TAAGGCGA.CAAGAG',\n                        'RRBS_trito_pool_1_TAAGGCGA.CATGAC',\n                        'RRBS_trito_pool_1_TAAGGCGA.CCTTCG',\n                        'RRBS_trito_pool_1_TAAGGCGA.CGGTAG',\n                        'RRBS_trito_pool_1_TAAGGCGA.CTATTG',\n                        'RRBS_trito_pool_1_TAAGGCGA.GACACG',\n                        'RRBS_trito_pool_1_TAAGGCGA.GCATTC',\n                        'RRBS_trito_pool_1_TAAGGCGA.GCTGCC',\n                        'RRBS_trito_pool_1_TAAGGCGA.GGCATC',\n                        'RRBS_trito_pool_1_TAAGGCGA.GTGAGG',\n                        'RRBS_trito_pool_1_TAAGGCGA.GTTGAG',\n                        'RRBS_trito_pool_1_TAAGGCGA.TAGCGG',\n                        'RRBS_trito_pool_1_TAAGGCGA.TATCTC',\n                        'RRBS_trito_pool_1_TAAGGCGA.TCTCTG',\n                        'RRBS_trito_pool_1_TAAGGCGA.TGACAG',\n                        'RRBS_trito_pool_1_TAAGGCGA.TGCTGC',\n                        'RRBS_trito_pool_2_CGTACTAG.ACAACC',\n                        'RRBS_trito_pool_2_CGTACTAG.ACGTGG',\n                        'RRBS_trito_pool_2_CGTACTAG.ACTCAC',\n                        'RRBS_trito_pool_2_CGTACTAG.AGGATG',\n                        'RRBS_trito_pool_2_CGTACTAG.ATAGCG',\n                        'RRBS_trito_pool_2_CGTACTAG.ATCGAC',\n                        'RRBS_trito_pool_2_CGTACTAG.CAAGAG',\n                        'RRBS_trito_pool_2_CGTACTAG.CATGAC',\n                        'RRBS_trito_pool_2_CGTACTAG.CCTTCG',\n                        'RRBS_trito_pool_2_CGTACTAG.CGGTAG',\n                        'RRBS_trito_pool_2_CGTACTAG.CTATTG',\n                        'RRBS_trito_pool_2_CGTACTAG.GACACG',\n                        'RRBS_trito_pool_2_CGTACTAG.GCATTC',\n                        'RRBS_trito_pool_2_CGTACTAG.GCTGCC',\n                        'RRBS_trito_pool_2_CGTACTAG.GGCATC',\n                        'RRBS_trito_pool_2_CGTACTAG.GTGAGG',\n                        'RRBS_trito_pool_2_CGTACTAG.GTTGAG',\n                        'RRBS_trito_pool_2_CGTACTAG.TAGCGG',\n                        'RRBS_trito_pool_2_CGTACTAG.TATCTC',\n                        'RRBS_trito_pool_2_CGTACTAG.TCTCTG',\n                        'RRBS_trito_pool_2_CGTACTAG.TGACAG']\n      \nprint(total_matrix.shape)\n\n\n\ntotal_matrix = total_matrix.applymap(lambda x: int(x) if pd.notnull(x) else str(\"?\"))\n\n\n\ntotal_matrix = total_matrix.astype(str).apply(''.join)\n\n\n\n\ntott = pd.Series(total_matrix.index.astype(str).str.cat(total_matrix.astype(str),'    '))\n\n\ntott.to_csv(\"total_chrom10.phy\", header=None, index=None)\n\nprint(tott.shape)\n\n             \n\n","license":"mit","hash":-4215377344139906476,"line_mean":62.8259187621,"line_max":107,"alpha_frac":0.5384265713,"autogenerated":false},
{"repo_name":"uptown\/django-town","path":"django_town\/social\/models\/geo.py","copies":"1","size":"1054","content":"from django.db import models\n\n\n\n#\n# class Country(models.Model):\n# name = models.CharField(max_length=200, unique=True)\n#     ascii_name = models.CharField(max_length=200, db_index=True)\n#\n#     class Meta:\n#         app_label = 'social'\n#\n#\n# class Locality(models.Model):\n#     name = models.CharField(max_length=200)\n#     ascii_name = models.CharField(max_length=200, db_index=True)\n#     country = models.ForeignKey(Country)\n#\n#     class Meta:\n#         unique_together = ('name', 'country')\n#         app_label = 'social'\n\nclass AddressComponentType(models.Model):\n    name = models.CharField(max_length=50, unique=True)\n\n\nclass AddressComponent(models.Model):\n    name = models.CharField(max_length=200, db_index=True)\n    ascii_name = models.CharField(max_length=200)\n    parent = models.ForeignKey(\"AddressComponent\", default=None, null=True)\n    types = models.ManyToManyField(AddressComponentType)\n    depth = models.SmallIntegerField(db_index=True)\n\n    class Meta:\n        unique_together = ('parent', 'name')\n        app_label = 'social'\n\n","license":"mit","hash":-3999580481196573742,"line_mean":27.4864864865,"line_max":75,"alpha_frac":0.6764705882,"autogenerated":false},
{"repo_name":"enjaz\/enjaz","path":"accounts\/migrations\/0011_fill_new_common_profile_fields.py","copies":"2","size":"1292","content":"# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\ndef fill_fields(apps, schema_editor):\n    CommonProfile = apps.get_model('accounts', 'CommonProfile')\n    CommonProfile.objects.filter(city='R').update(city=\"\u0627\u0644\u0631\u064a\u0627\u0636\")\n    CommonProfile.objects.filter(city='J').update(city=\"\u062c\u062f\u0629\")\n    CommonProfile.objects.filter(city='A').update(city=\"\u0627\u0644\u0623\u062d\u0633\u0627\u0621\")\n    CommonProfile.objects.filter(is_student=True).update(profile_type='S')\n    CommonProfile.objects.filter(is_student=False).update(profile_type='E')\n\ndef empty_fields(apps, schema_editor):\n    CommonProfile = apps.get_model('accounts', 'CommonProfile')\n    CommonProfile.objects.filter(city=\"\u0627\u0644\u0631\u064a\u0627\u0636\").update(city='R')\n    CommonProfile.objects.filter(city=\"\u062c\u062f\u0629\").update(city='J')\n    CommonProfile.objects.filter(city=\"\u0627\u0644\u0623\u062d\u0633\u0627\u0621\").update(city='A')\n    CommonProfile.objects.filter(profile_type='S').update(is_student=True, profile_type=\"\")\n    CommonProfile.objects.filter(profile_type='E').update(is_student=False, profile_type=\"\")\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('accounts', '0010_nonuser'),\n    ]\n\n    operations = [\n       migrations.RunPython(\n            fill_fields,\n            reverse_code=empty_fields)\n    ]\n","license":"agpl-3.0","hash":-5490091436627801995,"line_mean":38.375,"line_max":92,"alpha_frac":0.6992063492,"autogenerated":false},
{"repo_name":"pyQode\/pyqode.designer","path":"pyqode\/designer.py","copies":"1","size":"4544","content":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n#\n#The MIT License (MIT)\n#\n#Copyright (c) <2013> <Colin Duquesnoy and others, see AUTHORS.txt>\n#\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the \"Software\"), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n#copies of the Software, and to permit persons to whom the Software is\n#furnished to do so, subject to the following conditions:\n#\n#The above copyright notice and this permission notice shall be included in\n#all copies or substantial portions of the Software.\n#\n#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n#THE SOFTWARE.\n#\n\"\"\"\nRun this script to run qt designer with the pyqode plugins.\n\nRequires\n---------\n\nOn linux you need to install python-qt and python-qt-dev.\n\nOn windows you just have to install PyQt with its designer.\n\nUsage\n-----\n\nYou can try the pyqode qt designer plugin without installing pyqode, just run\ndesigner.pyw from the source package.\n\nIf pyqode is installed, this script is installed into the Scripts folder on\nwindows or in a standard bin folder on linux. Open a terminal and run\n**pyqode_designer**.\n\"\"\"\nimport multiprocessing\nimport os\nos.environ.setdefault(\"QT_API\", \"PyQt\")\nimport pkg_resources\nimport subprocess\nimport sys\n\n\ndef get_pth_sep():\n    \"\"\"\n    Gets platform dependand path separator\n    \"\"\"\n    if sys.platform == \"win32\":\n        sep = ';'\n    else:\n        sep = ':'\n    return sep\n\n\ndef set_plugins_path(env, sep):\n    \"\"\"\n    Sets PYQTDESIGNERPATH\n    \"\"\"\n    paths = \"\"\n    dict = {}\n    if sys.platform != \"win32\":\n        # if the packages were installed through apt-get, then there is no\n        # entry point.\n        # We can check for all directories in pyqode root dir to find plugins\n        import pyqode\n        root = os.path.dirname(pyqode.__file__)\n        for dir_name in os.listdir(root):\n            dir_name = os.path.join(root, dir_name)\n            if os.path.isdir(dir_name) and not \"_\" in dir_name:\n                for sub_dir in os.listdir(dir_name):\n                    if sub_dir == \"plugins\":\n                        pth = os.path.join(dir_name, sub_dir)\n                        paths += pth + sep\n    for entrypoint in pkg_resources.iter_entry_points(\"pyqode_plugins\"):\n        try:\n            plugin = entrypoint.load()\n        except pkg_resources.DistributionNotFound:\n            pass\n        except ImportError:\n            print('failed to import plugin: %r' % entrypoint)\n        else:\n            pth = os.path.dirname(plugin.__file__)\n            print('plugin loaded: %s' % pth)\n            if not pth in dict:\n                paths += pth + sep\n                dict[pth] = None\n    if 'PYQTDESIGNERPATH' in env:\n        pyqt_designer_path = env['PYQTDESIGNERPATH']\n        env['PYQTDESIGNERPATH'] = pyqt_designer_path + sep + paths\n    else:\n        env['PYQTDESIGNERPATH'] = paths\n    print(\"pyQode plugins paths: %s\" % env[\"PYQTDESIGNERPATH\"])\n\n\ndef run(env):\n    \"\"\"\n    Runs qt designer with our customised environment.\n    \"\"\"\n    p = None\n    env[\"PYQODE_NO_COMPLETION_SERVER\"] = \"1\"\n    try:\n        p = subprocess.Popen([\"designer-qt4\"], env=env)\n        if p.wait():\n            raise OSError()\n    except OSError:\n        try:\n            p = subprocess.Popen([\"designer\"], env=env)\n            if p.wait():\n                raise OSError()\n        except OSError:\n            print(\"Failed to start Qt Designer\")\n    if p:\n        return p.wait()\n    return -1\n\n\ndef check_env(env):\n    \"\"\"\n    Ensures all key and values are strings on windows.\n    \"\"\"\n    if sys.platform == \"win32\":\n        win_env = {}\n        for key, value in env.items():\n            win_env[str(key)] = str(value)\n        env = win_env\n    return env\n\n\ndef main():\n    \"\"\"\n    Runs the Qt Designer with an adapted plugin path.\n    \"\"\"\n    sep = get_pth_sep()\n    env = os.environ.copy()\n    set_plugins_path(env, sep)\n    env = check_env(env)\n    return run(env)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n","license":"mit","hash":6129902559767657663,"line_mean":29.0927152318,"line_max":78,"alpha_frac":0.6274207746,"autogenerated":false},
{"repo_name":"mjoblin\/neotiles","path":"examples\/growing_tile.py","copies":"1","size":"2595","content":"# =============================================================================\n# Draws a single red tile, 1x1, at the top left of the matrix and then grows\n# it out in both dimensions until it fills the full width of the matrix.  When\n# it gets to the full width it shrinks the tile back again.  Repeat.\n# =============================================================================\n\nimport time\n\ntry:\n    from neopixel import ws\n    STRIP_TYPE = ws.WS2811_STRIP_GRB\nexcept ImportError:\n    STRIP_TYPE = None\n\nfrom neotiles import MatrixSize, TileManager, PixelColor, Tile, TileSize\nfrom neotiles.matrixes import NTNeoPixelMatrix, NTRGBMatrix\n\n\n# Matrix size.  cols, rows.\nMATRIX_SIZE = MatrixSize(8, 8)\n\n# For a neopixel matrix.\nLED_PIN = 18\n\n# For an RGB matrix.\nCHAIN = 2\n\n\n# -----------------------------------------------------------------------------\n\ndef main():\n    # Initialize our matrix, animating at 10 frames per second.\n    tiles = TileManager(\n        NTNeoPixelMatrix(MATRIX_SIZE, LED_PIN, strip_type=STRIP_TYPE),\n        draw_fps=10\n    )\n    #tiles = TileManager(NTRGBMatrix(chain_length=CHAIN), draw_fps=10)\n\n    # Use the default Tile class, setting its color to red.  We don't need to\n    # override the draw method for this example as the default block drawing\n    # implementation is enough.\n    growing_tile = Tile(default_color=PixelColor(1, 0, 0))\n\n    # Assign the tile to the tile manager.\n    tiles.register_tile(growing_tile, size=(1, 1), root=(0, 0))\n\n    # Kick off the matrix animation loop.\n    tiles.draw_hardware_matrix()\n\n    # Keep track of whether we're getting bigger or smaller.\n    getting_bigger = True\n\n    try:\n        while True:\n            # Get the current size of the tile.\n            size = growing_tile.size\n\n            # Switch between growing bigger and smaller when we reach the\n            # boundaries of the matrix.\n            if getting_bigger and (\n                    size.cols >= MATRIX_SIZE.cols or\n                    size.rows >= MATRIX_SIZE.rows):\n                getting_bigger = False\n            elif not getting_bigger and size.cols <= 1:\n                getting_bigger = True\n\n            # Change the tile size.\n            size_change = 1 if getting_bigger else -1\n            growing_tile.size = TileSize(\n                size.cols + size_change, size.rows + size_change)\n\n            time.sleep(0.5)\n    except KeyboardInterrupt:\n        tiles.draw_stop()\n        tiles.clear_hardware_matrix()\n\n\n# =============================================================================\n\nif __name__ == '__main__':\n    main()\n","license":"mit","hash":4035265971433079169,"line_mean":31.037037037,"line_max":79,"alpha_frac":0.5568400771,"autogenerated":false},
{"repo_name":"opentrials\/collectors","path":"collectors\/isrctn\/parser.py","copies":"2","size":"5033","content":"# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom .. import base\nfrom .record import Record\n\n\n# Module API\n\ndef parse_record(res):\n\n    # Init data\n    data = {}\n\n    # General\n\n    key = 'isrctn_id'\n    path = '.ComplexTitle_primary::text'\n    value = res.css(path).extract_first()\n    data[key] = value\n\n    key = 'doi_isrctn_id'\n    path = '.ComplexTitle_secondary::text'\n    value = res.css(path).extract_first()\n    data[key] = value\n\n    key = 'title'\n    path = '\/\/h1\/text()'\n    value = res.xpath(path).extract_first()\n    data[key] = value\n\n    kpath = '.Meta_name'\n    vpath = '.Meta_name+.Meta_value'\n    subdata = _parse_dict(res, kpath, vpath)\n    data.update(subdata)\n\n    tag = 'h3'\n    text = 'Plain English Summary'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Contact information\n\n    key = 'contacts'\n    tag = 'h2'\n    text = 'Contact information'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    first = 'type'\n    section = _select_parent(res, tag, text)\n    value = _parse_list(section, kpath, vpath, first)\n    data.update({key: value})\n\n    # Additional identifiers\n\n    tag = 'h2'\n    text = 'Additional identifiers'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Study information\n\n    tag = 'h2'\n    text = 'Study information'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Eligibility\n\n    tag = 'h2'\n    text = 'Eligibility'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Locations\n\n    tag = 'h2'\n    text = 'Locations'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Sponsor information\n\n    key = 'sponsors'\n    tag = 'h2'\n    text = 'Sponsor information'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    first = 'organisation'\n    section = _select_parent(res, tag, text)\n    value = _parse_list(section, kpath, vpath, first)\n    data.update({key: value})\n\n    # Funders\n\n    key = 'funders'\n    tag = 'h2'\n    text = 'Funders'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    first = 'funder_type'\n    section = _select_parent(res, tag, text)\n    value = _parse_list(section, kpath, vpath, first)\n    data.update({key: value})\n\n    # Results and publications\n\n    tag = 'h2'\n    text = 'Results and Publications'\n    kpath = '.Info_section_title'\n    vpath = '.Info_section_title+p'\n    section = _select_parent(res, tag, text)\n    subdata = _parse_dict(section, kpath, vpath)\n    data.update(subdata)\n\n    # Create record\n    record = Record.create(res.url, data)\n\n    return record\n\n\n# Internal\n\ndef _select_parent(sel, tag, text):\n    return sel.xpath('\/\/%s[contains(.,\"%s\")]\/..' % (tag, text))\n\n\ndef _parse_dict(sel, kpath, vpath):\n    \"\"\"parse data from title-paragraph like html.\n    \"\"\"\n    data = {}\n    key = None\n    value = None\n    for sel in sel.css('%s, %s' % (kpath, vpath)):\n        if sel.css(kpath):\n            key = None\n            value = None\n            texts = sel.xpath('.\/\/text()').extract()\n            if texts:\n                key = base.helpers.slugify(' '.join(texts).strip())\n        else:\n            if key is not None:\n                value = None\n                texts = sel.xpath('.\/\/text()').extract()\n                if texts:\n                    value = ' '.join(texts).strip()\n        if key and value:\n            data[key] = value\n    return data\n\n\ndef _parse_list(sel, kpath, vpath, first):\n    \"\"\"parse data from title-paragraph like html.\n    \"\"\"\n    data = []\n    item = {}\n    key = None\n    value = None\n    for sel in sel.css('%s, %s' % (kpath, vpath)):\n        if sel.css(kpath):\n            key = None\n            value = None\n            texts = sel.xpath('.\/\/text()').extract()\n            if texts:\n                key = base.helpers.slugify(' '.join(texts).strip())\n        else:\n            if key is not None:\n                value = None\n                texts = sel.xpath('.\/\/text()').extract()\n                if texts:\n                    value = ' '.join(texts).strip()\n        if key and value:\n            item[key] = value\n        if key == first and value is None and item:\n            data.append(item)\n            item = {}\n    if item:\n        data.append(item)\n    return data\n","license":"mit","hash":-660575543947889522,"line_mean":24.5482233503,"line_max":67,"alpha_frac":0.5664613551,"autogenerated":false},
{"repo_name":"Guavus\/bigtop","path":"bigtop-packages\/src\/charm\/zookeeper\/layer-zookeeper\/tests\/01-deploy-smoke.py","copies":"1","size":"2613","content":"#!\/usr\/bin\/python3\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport amulet\nimport re\nimport unittest\n\n\nclass TestDeploy(unittest.TestCase):\n    \"\"\"\n    Deployment test for Apache Zookeeper quorum\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.d = amulet.Deployment(series='xenial')\n\n        cls.d.add('zookeeper', charm='cs:xenial\/zookeeper', units=3)\n\n        cls.d.setup(timeout=1800)\n        cls.d.sentry.wait_for_messages({'zookeeper': re.compile('ready')},\n                                       timeout=1800)\n        cls.unit = cls.d.sentry['zookeeper'][0]\n\n    def test_deploy(self):\n        \"\"\"Verify zk quorum is running\"\"\"\n        output, retcode = self.unit.run(\"pgrep -a java\")\n        assert 'QuorumPeerMain' in output, \"Zookeeper QuorumPeerMain daemon is not started\"\n\n    def test_quorum(self):\n        \"\"\"\n        Verify that our peers are talking to each other, and taking on\n        appropriate roles.\n        \"\"\"\n        for unit in self.d.sentry['zookeeper']:\n            output, _ = unit.run(\n                \"\/usr\/lib\/zookeeper\/bin\/zkServer.sh status\"\n            )\n            # Unit should be a leader or follower\n            self.assertTrue(\"leader\" in output or \"follower\" in output)\n\n    def test_smoke(self):\n        \"\"\"Validates Zookeeper using the Bigtop 'zookeeper' smoke test.\"\"\"\n        smk_uuids = []\n\n        for unit in self.d.sentry['zookeeper']:\n            smk_uuids.append(unit.run_action('smoke-test'))\n\n        for smk_uuid in smk_uuids:\n            # 'zookeeper' smoke takes a while (bigtop tests are slow)\n            result = self.d.action_fetch(smk_uuid, timeout=1800, full_output=True)\n            # actions set status=completed on success\n            if (result['status'] != \"completed\"):\n                self.fail('Zookeeper smoke-test failed: %s' % result)\n\n\nif __name__ == '__main__':\n    unittest.main()\n","license":"apache-2.0","hash":-72016806262073970,"line_mean":35.2916666667,"line_max":91,"alpha_frac":0.644087256,"autogenerated":false},
{"repo_name":"william-richard\/moto","path":"moto\/instance_metadata\/responses.py","copies":"2","size":"1525","content":"from __future__ import unicode_literals\nimport datetime\nimport json\nfrom six.moves.urllib.parse import urlparse\n\nfrom moto.core.responses import BaseResponse\n\n\nclass InstanceMetadataResponse(BaseResponse):\n    def metadata_response(self, request, full_url, headers):\n        \"\"\"\n        Mock response for localhost metadata\n\n        http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/AESDG-chapter-instancedata.html\n        \"\"\"\n\n        parsed_url = urlparse(full_url)\n        tomorrow = datetime.datetime.utcnow() + datetime.timedelta(days=1)\n        credentials = dict(\n            AccessKeyId=\"test-key\",\n            SecretAccessKey=\"test-secret-key\",\n            Token=\"test-session-token\",\n            Expiration=tomorrow.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        )\n\n        path = parsed_url.path\n\n        meta_data_prefix = \"\/latest\/meta-data\/\"\n        # Strip prefix if it is there\n        if path.startswith(meta_data_prefix):\n            path = path[len(meta_data_prefix) :]\n\n        if path == \"\":\n            result = \"iam\"\n        elif path == \"iam\":\n            result = json.dumps({\"security-credentials\": {\"default-role\": credentials}})\n        elif path == \"iam\/security-credentials\/\":\n            result = \"default-role\"\n        elif path == \"iam\/security-credentials\/default-role\":\n            result = json.dumps(credentials)\n        else:\n            raise NotImplementedError(\n                \"The {0} metadata path has not been implemented\".format(path)\n            )\n        return 200, headers, result\n","license":"apache-2.0","hash":8244830061161153983,"line_mean":32.8888888889,"line_max":90,"alpha_frac":0.6059016393,"autogenerated":false},
{"repo_name":"PMBio\/limix","path":"limix\/deprecated\/modules\/qtl.py","copies":"1","size":"44863","content":"# Copyright(c) 2014, The LIMIX developers (Christoph Lippert, Paolo Francesco Casale, Oliver Stegle)\n#\n#Licensed under the Apache License, Version 2.0 (the \"License\");\n#you may not use this file except in compliance with the License.\n#You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n#Unless required by applicable law or agreed to in writing, software\n#distributed under the License is distributed on an \"AS IS\" BASIS,\n#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#See the License for the specific language governing permissions and\n#limitations under the License.\n\"\"\"\nqtl.py contains wrappers around C++ Limix objects to streamline common tasks in GWAS.\n\"\"\"\n\nimport numpy as np\nimport scipy.stats as st\nimport limix\nimport scipy as sp\nimport limix.deprecated\nimport limix.deprecated as dlimix\nimport limix.deprecated.utils.preprocess as preprocess\nimport limix.deprecated.stats.fdr as FDR\nfrom . import varianceDecomposition as VAR\nimport time\n\n\nclass lmm:\n\tdef __init__(self, snps, pheno, K=None, covs=None, test='lrt', NumIntervalsDelta0=100, NumIntervalsDeltaAlt=100, searchDelta=False, verbose=None):\n\t\t\"\"\"\n\t\tUnivariate fixed effects linear mixed model test for all SNPs\n\n\t\tIf phenotypes have missing values, then the subset of individuals used for each phenotype column\n\t\twill be subsetted\n\n\t\tArgs:\n\t\t\tsnps:   [N x S] np.array of S SNPs for N individuals\n\t\t\tpheno:  [N x P] np.array of P phenotype sfor N individuals\n\t\t\tK:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\t\tcovs:   [N x D] np.array of D covariates for N individuals\n\t\t\ttest:   'lrt' for likelihood ratio test (default) or 'f' for F-test\n\t\t\tNumIntervalsDelta0:     number of steps for delta optimization on the null model (100)\n\t\t\tNumIntervalsDeltaAlt:   number of steps for delta optimization on the alt. model (100), requires searchDelta=True to have an effect.\n\t\t\tsearchDelta:     Carry out delta optimization on the alternative model? if yes We use NumIntervalsDeltaAlt steps\n\t\t\tverbose: print verbose output? (False)\n\t\t\"\"\"\n\t\t#create column of 1 for fixed if nothing provide\n\t\tif len(pheno.shape)==1:\n\t\t\tpheno = pheno[:,sp.newaxis]\n\n\t\tself.verbose = dlimix.getVerbose(verbose)\n\t\tself.snps = snps\n\t\tself.pheno = pheno\n\t\tself.K = K\n\t\tself.covs = covs\n\t\tself.test = test\n\t\tself.NumIntervalsDelta0 = NumIntervalsDelta0\n\t\tself.NumIntervalsDeltaAlt = NumIntervalsDeltaAlt\n\t\tself.searchDelta = searchDelta\n\t\tself.verbose = verbose\n\t\tself.N       = self.pheno.shape[0]\n\t\tself.P       = self.pheno.shape[1]\n\t\tself.Iok     = ~(np.isnan(self.pheno).any(axis=1))\n\t\tif self.K is None:\n\t\t\tself.searchDelta=False\n\t\t\tself.K = np.eye(self.snps.shape[0])\n\t\tif self.covs is None:\n\t\t\tself.covs = np.ones((self.snps.shape[0],1))\n\n\t\tself._lmm = None\n\t\t#run\n\t\tself.verbose = verbose\n\t\tself.process()\n\n\tdef process(self):\n\t\tt0 = time.time()\n\t\tif self._lmm is None:\n\t\t\tself._lmm = limix.deprecated.CLMM()\n\t\t\tself._lmm.setK(self.K)\n\t\t\tself._lmm.setSNPs(self.snps)\n\t\t\tself._lmm.setPheno(self.pheno)\n\t\t\tself._lmm.setCovs(self.covs)\n\t\t\tif self.test=='lrt':\n\t\t\t\tself._lmm.setTestStatistics(self._lmm.TEST_LRT)\n\t\t\telif self.test=='f':\n\t\t\t\tself._lmm.setTestStatistics(self._lmm.TEST_F)\n\t\t\telse:\n\t\t\t\tprint((self.test))\n\t\t\t\traise NotImplementedError(\"only f and lrt are implemented\")\n\t\t\t#set number of delta grid optimizations?\n\t\t\tself._lmm.setNumIntervals0(self.NumIntervalsDelta0)\n\t\t\tif self.searchDelta:\n\t\t\t\tself._lmm.setNumIntervalsAlt(self.NumIntervalsDeltaAlt)\n\t\t\telse:\n\t\t\t\tself._lmm.setNumIntervalsAlt(0)\n\n\t\tif not np.isnan(self.pheno).any():\n\t\t\t#process\n\t\t\tself._lmm.process()\n\t\t\tself.pvalues = self._lmm.getPv()\n\t\t\tself.beta_snp = self._lmm.getBetaSNP()\n\t\t\tself.beta_ste = self._lmm.getBetaSNPste()\n\t\t\tself.ldelta_0 = self._lmm.getLdelta0()\n\t\t\tself.ldelta_alt = self._lmm.getLdeltaAlt()\n\t\t\tself.NLL_0 = self._lmm.getNLL0()\n\t\t\tself.NLL_alt = self._lmm.getNLLAlt()\n\t\telse:\n\t\t\tif self._lmm is not None:\n\t\t\t\traise Exception('cannot reuse a CLMM object if missing variables are present')\n\t\t\telse:\n\t\t\t\tself._lmm = limix.deprecated.CLMM()\n\t\t\t#test all phenotypes separately\n\t\t\tself.pvalues = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.beta_snp = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.beta_ste = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.ldelta_0 = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.ldelta_alt = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.NLL_0 = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.NLL_alt = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tself.test_statistics = np.zeros((self.phenos.shape[1],self.snps.shape[1]))\n\t\t\tfor ip in np.arange(self.phenos.shape[1]):\n\t\t\t\tpheno_ = self.phenos[:,ip]\n\t\t\t\ti_nonz = ~(pheno_.isnan())\n\n\t\t\t\tself._lmm.setK(self.K[i_nonz,i_nonz])\n\t\t\t\tself._lmm.setSNPs(self.snps[i_nonz])\n\t\t\t\tself._lmm.setPheno(pheno_[i_nonz,np.newaxis])\n\t\t\t\tself._lmm.setCovs(self.covs[i_nonz])\n\t\t\t\tself._lmm.process()\n\t\t\t\tself.pvalues[ip:ip+1] = self._lmm.getPv()\n\t\t\t\tself.beta_snp[ip:ip+1] = self._lmm.getBetaSNP()\n\t\t\t\tself.beta_ste[ip:ip+1] = self._lmm.getBetaSNPste()\n\t\t\t\tself.ldelta_0[ip:ip+1] = self._lmm.getLdelta0()\n\t\t\t\tself.ldelta_alt[ip:ip+1] = self._lmm.getLdeltaAlt()\n\t\t\t\tself.NLL_0[ip:ip+1] = self._lmm.getNLL0()\n\t\t\t\tself.NLL_alt[ip:ip+1] = self._lmm.getNLLAlt()\n\t\t\t\tself.test_statistics[ip:ip+1] = self._lmm.getTestStatistics()\n\t\t\t\tpass\n\t\tif self._lmm.getTestStatistics() == self._lmm.TEST_LRT and self.test != \"lrt\":\n\t\t\traise NotImplementedError(\"only f and lrt are implemented\")\n\t\telif self._lmm.getTestStatistics() == self._lmm.TEST_F and self.test != \"f\":\n\t\t\traise NotImplementedError(\"only f and lrt are implemented\")\n\n\t\tif self._lmm.getTestStatistics() == self._lmm.TEST_F:\n\t\t\tself.test_statistics = (self.beta_snp*self.beta_snp)\/(self.beta_ste*self.beta_ste)\n\t\tif self._lmm.getTestStatistics() == self._lmm.TEST_LRT:\n\t\t\tself.test_statistics = 2.0 * (self.NLL_0 - self.NLL_alt)\n\t\tt1=time.time()\n\n\t\tif self.verbose:\n\t\t\tprint((\"finished GWAS testing in %.2f seconds\" %(t1-t0)))\n\n\tdef setCovs(self,covs):\n\t\tself._lmm.setCovs(covs)\n\n\tdef getBetaSNP(self):\n\t\treturn self.beta_snp\n\n\tdef getPv(self):\n\t\t\"\"\"\n\t\tReturns:\n\t\t\t[P x S] np.array of P-values\n\t\t\"\"\"\n\t\treturn self.pvalues\n\ndef test_lm(snps,pheno, covs=None, test='lrt',verbose=None):\n\t\"\"\"\n\tUnivariate fixed effects linear model test for all SNPs\n\t(wrapper around test_lmm, using identity kinship)\n\n\tIf phenotypes have missing values, then the subset of individuals used for each phenotype column\n\twill be subsetted\n\n\tArgs:\n\t\tsnps:   [N x S] np.array of S SNPs for N individuals\n\t\tpheno:  [N x 1] np.array of 1 phenotype for N individuals\n\t\tcovs:   [N x D] np.array of D covariates for N individuals\n\t\ttest:   'lrt' for likelihood ratio test (default) or 'f' for F-test\n\t\tverbose: print verbose output? (False)\n\n\tReturns:\n\t\tlimix LMM object\n\t\"\"\"\n\tlm = test_lmm(snps=snps,pheno=pheno,K=None,covs=covs, test=test,verbose=verbose)\n\treturn lm\n\ndef test_lmm(snps,pheno,K=None,covs=None, test='lrt',NumIntervalsDelta0=100,NumIntervalsDeltaAlt=100,searchDelta=False,verbose=None):\n\t\"\"\"\n\tUnivariate fixed effects linear mixed model test for all SNPs\n\n\tIf phenotypes have missing values, then the subset of individuals used for each phenotype column\n\twill be subsetted\n\n\tArgs:\n\t\tsnps:   [N x S] np.array of S SNPs for N individuals\n\t\tpheno:  [N x 1] np.array of 1 phenotype for N individuals\n\t\tK:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\tcovs:   [N x D] np.array of D covariates for N individuals\n\t\ttest:   'lrt' for likelihood ratio test (default) or 'f' for F-test\n\t\tNumIntervalsDelta0:     number of steps for delta optimization on the null model (100)\n\t\tNumIntervalsDeltaAlt:   number of steps for delta optimization on the alt. model (100), requires searchDelta=True to have an effect.\n\t\tsearchDelta:     Carry out delta optimization on the alternative model? if yes We use NumIntervalsDeltaAlt steps\n\t\tverbose: print verbose output? (False)\n\n\tReturns:\n\t\tLMM object\n\t\"\"\"\n\tlmm_ = lmm(snps=snps, pheno=pheno, K=K, covs=covs, test=test, NumIntervalsDelta0=NumIntervalsDelta0, NumIntervalsDeltaAlt=NumIntervalsDeltaAlt, searchDelta=searchDelta, verbose=verbose)\n\treturn lmm_\n\n\ndef test_lmm_kronecker(snps,phenos,covs=None,Acovs=None,Asnps=None,K1r=None,K1c=None,K2r=None,K2c=None,trait_covar_type='lowrank_diag',rank=1,NumIntervalsDelta0=100,NumIntervalsDeltaAlt=100,searchDelta=False):\n    \"\"\"\n    simple wrapper for kroneckerLMM code\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        phenos: [N x P] np.array of P phenotypes for N individuals\n        covs:           list of np.arrays holding covariates. Each covs[i] has one corresponding Acovs[i]\n        Acovs:          list of np.arrays holding the phenotype design matrices for covariates.\n                        Each covs[i] has one corresponding Acovs[i].\n        Asnps:          single np.array of I0 interaction variables to be included in the\n                        background model when testing for interaction with Inters\n                        If not provided, the alternative model will be the independent model\n        K1r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K1c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        trait_covar_type:     type of covaraince to use. Default 'freeform'. possible values are\n                        'freeform': free form optimization,\n                        'fixed': use a fixed matrix specified in covar_K0,\n                        'diag': optimize a diagonal matrix,\n                        'lowrank': optimize a low rank matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_id': optimize a low rank matrix plus the weight of a constant diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_diag': optimize a low rank matrix plus a free diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'block': optimize the weight of a constant P x P block matrix of ones,\n                        'block_id': optimize the weight of a constant P x P block matrix of ones plus the weight of a constant diagonal matrix,\n                        'block_diag': optimize the weight of a constant P x P block matrix of ones plus a free diagonal matrix,\n        rank:           rank of a possible lowrank component (default 1)\n        NumIntervalsDelta0:  number of steps for delta optimization on the null model (100)\n        NumIntervalsDeltaAlt:number of steps for delta optimization on the alt. model (100), requires searchDelta=True to have an effect.\n        searchDelta:    Boolean indicator if delta is optimized during SNP testing (default False)\n\n    Returns:\n        CKroneckerLMM object\n        P-values for all SNPs from liklelihood ratio test\n    \"\"\"\n    #0. checks\n    N  = phenos.shape[0]\n    P  = phenos.shape[1]\n\n    if K1r==None:\n        K1r = np.dot(snps,snps.T)\n    else:\n        assert K1r.shape[0]==N, 'K1r: dimensions dismatch'\n        assert K1r.shape[1]==N, 'K1r: dimensions dismatch'\n\n    if K2r==None:\n        K2r = np.eye(N)\n    else:\n        assert K2r.shape[0]==N, 'K2r: dimensions dismatch'\n        assert K2r.shape[1]==N, 'K2r: dimensions dismatch'\n\n    covs, Acovs = _updateKronCovs(covs,Acovs,N,P)\n\n    #Asnps can be several designs\n    if Asnps is None:\n        Asnps = [np.ones([1,P])]\n    if (type(Asnps)!=list):\n        Asnps = [Asnps]\n    assert len(Asnps)>0, \"need at least one Snp design matrix\"\n\n    #one row per column design matrix\n    pv = np.zeros((len(Asnps),snps.shape[1]))\n\n    #1. run GP model to infer suitable covariance structure\n    if K1c==None or K2c==None:\n        vc = _estimateKronCovariances(phenos=phenos, K1r=K1r, K2r=K2r, K1c=K1c, K2c=K2c, covs=covs, Acovs=Acovs, trait_covar_type=trait_covar_type, rank=rank)\n        K1c = vc.getTraitCovar(0)\n        K2c = vc.getTraitCovar(1)\n    else:\n        assert K1c.shape[0]==P, 'K1c: dimensions dismatch'\n        assert K1c.shape[1]==P, 'K1c: dimensions dismatch'\n        assert K2c.shape[0]==P, 'K2c: dimensions dismatch'\n        assert K2c.shape[1]==P, 'K2c: dimensions dismatch'\n\n    #2. run kroneckerLMM\n\n    lmm = limix.deprecated.CKroneckerLMM()\n    lmm.setK1r(K1r)\n    lmm.setK1c(K1c)\n    lmm.setK2r(K2r)\n    lmm.setK2c(K2c)\n    lmm.setSNPs(snps)\n    #add covariates\n    for ic  in range(len(Acovs)):\n        lmm.addCovariates(covs[ic],Acovs[ic])\n    lmm.setPheno(phenos)\n\n\n    #delta serch on alt. model?\n    if searchDelta:\n        lmm.setNumIntervalsAlt(NumIntervalsDeltaAlt)\n    else:\n        lmm.setNumIntervalsAlt(0)\n    lmm.setNumIntervals0(NumIntervalsDelta0)\n\n    for iA in range(len(Asnps)):\n        #add SNP design\n        lmm.setSNPcoldesign(Asnps[iA])\n        lmm.process()\n        pv[iA,:] = lmm.getPv()[0]\n    return lmm,pv\n\n\ndef test_interaction_lmm_kronecker(snps,phenos,covs=None,Acovs=None,Asnps1=None,Asnps0=None,K1r=None,K1c=None,K2r=None,K2c=None,trait_covar_type='lowrank_diag',rank=1,NumIntervalsDelta0=100,NumIntervalsDeltaAlt=100,searchDelta=False,return_lmm=False):\n    \"\"\"\n    I-variate fixed effects interaction test for phenotype specific SNP effects\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        phenos: [N x P] np.array of P phenotypes for N individuals\n        covs:           list of np.arrays holding covariates. Each covs[i] has one corresponding Acovs[i]\n        Acovs:          list of np.arrays holding the phenotype design matrices for covariates.\n                        Each covs[i] has one corresponding Acovs[i].\n        Asnps1:         list of np.arrays of I interaction variables to be tested for N\n                        individuals. Note that it is assumed that Asnps0 is already included.\n                        If not provided, the alternative model will be the independent model\n        Asnps0:         single np.array of I0 interaction variables to be included in the\n                        background model when testing for interaction with Inters\n        K1r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K1c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        trait_covar_type:     type of covaraince to use. Default 'freeform'. possible values are\n                        'freeform': free form optimization,\n                        'fixed': use a fixed matrix specified in covar_K0,\n                        'diag': optimize a diagonal matrix,\n                        'lowrank': optimize a low rank matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_id': optimize a low rank matrix plus the weight of a constant diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_diag': optimize a low rank matrix plus a free diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'block': optimize the weight of a constant P x P block matrix of ones,\n                        'block_id': optimize the weight of a constant P x P block matrix of ones plus the weight of a constant diagonal matrix,\n                        'block_diag': optimize the weight of a constant P x P block matrix of ones plus a free diagonal matrix,\n        rank:           rank of a possible lowrank component (default 1)\n        NumIntervalsDelta0:  number of steps for delta optimization on the null model (100)\n        NumIntervalsDeltaAlt:number of steps for delta optimization on the alt. model (100), requires searchDelta=True to have an effect.\n        searchDelta:     Carry out delta optimization on the alternative model? if yes We use NumIntervalsDeltaAlt steps\n    Returns:\n        pv:     P-values of the interaction test\n        pv0:    P-values of the null model\n        pvAlt:  P-values of the alternative model\n    \"\"\"\n    S=snps.shape[1]\n    #0. checks\n    N  = phenos.shape[0]\n    P  = phenos.shape[1]\n\n    if K1r==None:\n        K1r = np.dot(snps,snps.T)\n    else:\n        assert K1r.shape[0]==N, 'K1r: dimensions dismatch'\n        assert K1r.shape[1]==N, 'K1r: dimensions dismatch'\n\n    if K2r==None:\n        K2r = np.eye(N)\n    else:\n        assert K2r.shape[0]==N, 'K2r: dimensions dismatch'\n        assert K2r.shape[1]==N, 'K2r: dimensions dismatch'\n\n    covs,Acovs = _updateKronCovs(covs,Acovs,N,P)\n\n    #Asnps can be several designs\n    if (Asnps0 is None):\n        Asnps0 = [np.ones([1,P])]\n    if Asnps1 is None:\n        Asnps1 = [np.eye([P])]\n    if (type(Asnps0)!=list):\n        Asnps0 = [Asnps0]\n    if (type(Asnps1)!=list):\n        Asnps1 = [Asnps1]\n    assert (len(Asnps0)==1) and (len(Asnps1)>0), \"need at least one Snp design matrix for null and alt model\"\n\n    #one row per column design matrix\n    pv = np.zeros((len(Asnps1),snps.shape[1]))\n    lrt = np.zeros((len(Asnps1),snps.shape[1]))\n    pvAlt = np.zeros((len(Asnps1),snps.shape[1]))\n    lrtAlt = np.zeros((len(Asnps1),snps.shape[1]))\n\n    #1. run GP model to infer suitable covariance structure\n    if K1c==None or K2c==None:\n        vc = _estimateKronCovariances(phenos=phenos, K1r=K1r, K2r=K2r, K1c=K1c, K2c=K2c, covs=covs, Acovs=Acovs, trait_covar_type=trait_covar_type, rank=rank)\n        K1c = vc.getTraitCovar(0)\n        K2c = vc.getTraitCovar(1)\n    else:\n        assert K1c.shape[0]==P, 'K1c: dimensions dismatch'\n        assert K1c.shape[1]==P, 'K1c: dimensions dismatch'\n        assert K2c.shape[0]==P, 'K2c: dimensions dismatch'\n        assert K2c.shape[1]==P, 'K2c: dimensions dismatch'\n\n    #2. run kroneckerLMM for null model\n    lmm = limix.deprecated.CKroneckerLMM()\n    lmm.setK1r(K1r)\n    lmm.setK1c(K1c)\n    lmm.setK2r(K2r)\n    lmm.setK2c(K2c)\n    lmm.setSNPs(snps)\n    #add covariates\n    for ic  in range(len(Acovs)):\n        lmm.addCovariates(covs[ic],Acovs[ic])\n    lmm.setPheno(phenos)\n\n    #delta serch on alt. model?\n    if searchDelta:\n        lmm.setNumIntervalsAlt(NumIntervalsDeltaAlt)\n        lmm.setNumIntervals0_inter(NumIntervalsDeltaAlt)\n    else:\n        lmm.setNumIntervalsAlt(0)\n        lmm.setNumIntervals0_inter(0)\n\n\n    lmm.setNumIntervals0(NumIntervalsDelta0)\n    #add SNP design\n    lmm.setSNPcoldesign0_inter(Asnps0[0])\n    for iA in range(len(Asnps1)):\n        lmm.setSNPcoldesign(Asnps1[iA])\n        lmm.process()\n\n        pvAlt[iA,:] = lmm.getPv()[0]\n        pv[iA,:] = lmm.getPv()[1]\n        pv0 = lmm.getPv()[2][np.newaxis,:]\n    if return_lmm:\n        return pv,pv0,pvAlt,lmm\n    else:\n        return pv,pv0,pvAlt\n\n\ndef test_interaction_lmm(snps,pheno,Inter,Inter0=None,covs=None,K=None,test='lrt'):\n    \"\"\"\n    I-variate fixed effects interaction test for phenotype specific SNP effects\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        pheno:  [N x 1] np.array of 1 phenotype for N individuals\n        Inter:  [N x I] np.array of I interaction variables to be tested for N\n                        individuals (optional). If not provided, only the SNP is\n                        included in the null model.\n        Inter0: [N x I0] np.array of I0 interaction variables to be included in the\n                         background model when testing for interaction with Inter\n        covs:   [N x D] np.array of D covariates for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        test:    'lrt' for likelihood ratio test (default) or 'f' for F-test\n\n    Returns:\n        limix LMM object\n    \"\"\"\n    N=snps.shape[0]\n    if covs is None:\n        covs = np.ones((N,1))\n    if K is None:\n        K = np.eye(N)\n    if Inter0 is None:\n        Inter0=np.ones([N,1])\n    assert (pheno.shape[0]==N and K.shape[0]==N and K.shape[1]==N and covs.shape[0]==N and Inter0.shape[0]==N and Inter.shape[0]==N), \"shapes missmatch\"\n    lmi = limix.deprecated.CInteractLMM()\n    lmi.setK(K)\n    lmi.setSNPs(snps)\n    lmi.setPheno(pheno)\n    lmi.setCovs(covs)\n    lmi.setInter0(Inter0)\n    lmi.setInter(Inter)\n    if test=='lrt':\n        lmi.setTestStatistics(lmi.TEST_LRT)\n    elif test=='f':\n        lmi.setTestStatistics(lmi.TEST_F)\n    else:\n        print(test)\n        raise NotImplementedError(\"only f or lrt are implemented\")\n    lmi.process()\n    return lmi\n\n\n\"\"\" MULTI LOCUS MODEL \"\"\"\n\n\ndef forward_lmm(snps,pheno,K=None,covs=None,qvalues=False,threshold=5e-8,maxiter=2,test='lrt',verbose=None,**kw_args):\n    \"\"\"\n    univariate fixed effects test with forward selection\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        pheno:  [N x 1] np.array of 1 phenotype for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        covs:   [N x D] np.array of D covariates for N individuals\n        threshold:      (float) P-value thrashold for inclusion in forward selection (default 5e-8)\n        maxiter:        (int) maximum number of interaction scans. First scan is\n                        without inclusion, so maxiter-1 inclusions can be performed. (default 2)\n        test:           'lrt' for likelihood ratio test (default) or 'f' for F-test\n        verbose: print verbose output? (False)\n\n    Returns:\n        lm:     limix LMM object\n        RV:     dictionary\n                RV['iadded']:   array of indices of SNPs included in order of inclusion\n                RV['pvadded']:  array of Pvalues obtained by the included SNPs in iteration\n                                before inclusion\n                RV['pvall']:    [Nadded x S] np.array of Pvalues for all iterations\n    \"\"\"\n    verbose = dlimix.getVerbose(verbose)\n\n    if K is None:\n        K=np.eye(snps.shape[0])\n    if covs is None:\n        covs = np.ones((snps.shape[0],1))\n    #assert single trait\n    assert pheno.shape[1]==1, 'forward_lmm only supports single phenotypes'\n\n    lm = test_lmm(snps,pheno,K=K,covs=covs,test=test,**kw_args)\n    pvall = []\n    betall = []\n    pv = lm.getPv().ravel()\n    beta = lm.getBetaSNP().ravel()\n    #hack to avoid issues with degenerate pv\n    pv[sp.isnan(pv)] = 1\n    pvall.append(pv)\n    betall.append(beta)\n    imin= pv.argmin()\n    niter = 1\n    #start stuff\n    iadded = []\n    pvadded = []\n    qvadded = []\n    if qvalues:\n        assert pv.shape[0]==1, \"This is untested with the fdr package. pv.shape[0]==1 failed\"\n        qvall = []\n        qv  = FDR.qvalues(pv)\n        qvall.append(qv)\n        score=qv.min()\n    else:\n        score=pv.min()\n    while (score<threshold) and niter<maxiter:\n        t0=time.time()\n        iadded.append(imin)\n        pvadded.append(pv[imin])\n        if qvalues:\n            qvadded.append(qv[0,imin])\n        covs=np.concatenate((covs,snps[:,imin:(imin+1)]),1)\n        lm.setCovs(covs)\n        lm.process()\n        pv = lm.getPv().ravel()\n        beta = lm.getBetaSNP().ravel()\n        pv[sp.isnan(pv)] = 1\n        pvall.append(pv)\n        betall.append(beta)\n        imin= pv.argmin()\n        if qvalues:\n            qv = FDR.qvalues(pv)\n            qvall[niter:niter+1,:] = qv\n            score = qv.min()\n        else:\n            score = pv.min()\n        t1=time.time()\n        if verbose:\n            print((\"finished GWAS testing in %.2f seconds\" %(t1-t0)))\n        niter=niter+1\n    RV = {}\n    RV['iadded']  = iadded\n    RV['pvadded'] = pvadded\n    RV['pvall']   = np.array(pvall)\n    RV['betall']   = np.array(betall)\n    if qvalues:\n        RV['qvall'] = np.array(qvall)\n        RV['qvadded'] = qvadded\n    return lm,RV\n\n\n#TOOD: use **kw_args to forward params.. see below\ndef forward_lmm_kronecker(snps,phenos,Asnps=None,Acond=None,K1r=None,K1c=None,K2r=None,K2c=None,covs=None,Acovs=None,threshold=5e-8,maxiter=2,qvalues=False, update_covariances = False,verbose=None,**kw_args):\n    \"\"\"\n    Kronecker fixed effects test with forward selection\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        pheno:  [N x P] np.array of 1 phenotype for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        covs:   [N x D] np.array of D covariates for N individuals\n        threshold:      (float) P-value thrashold for inclusion in forward selection (default 5e-8)\n        maxiter:        (int) maximum number of interaction scans. First scan is\n                        without inclusion, so maxiter-1 inclusions can be performed. (default 2)\n        qvalues:        Use q-value threshold and return q-values in addition (default False)\n        update_covar:   Boolean indicator if covariances should be re-estimated after each forward step (default False)\n\n    Returns:\n        lm:             lmix LMMi object\n        resultStruct with elements:\n            iadded:         array of indices of SNPs included in order of inclusion\n            pvadded:        array of Pvalues obtained by the included SNPs in iteration\n                            before inclusion\n            pvall:     [Nadded x S] np.array of Pvalues for all iterations.\n        Optional:      corresponding q-values\n            qvadded\n            qvall\n    \"\"\"\n    verbose = dlimix.getVerbose(verbose)\n    #0. checks\n    N  = phenos.shape[0]\n    P  = phenos.shape[1]\n\n    if K1r==None:\n        K1r = np.dot(snps,snps.T)\n    else:\n        assert K1r.shape[0]==N, 'K1r: dimensions dismatch'\n        assert K1r.shape[1]==N, 'K1r: dimensions dismatch'\n\n    if K2r==None:\n        K2r = np.eye(N)\n    else:\n        assert K2r.shape[0]==N, 'K2r: dimensions dismatch'\n        assert K2r.shape[1]==N, 'K2r: dimensions dismatch'\n\n    covs,Acovs = _updateKronCovs(covs,Acovs,N,P)\n\n    if Asnps is None:\n        Asnps = [np.ones([1,P])]\n    if (type(Asnps)!=list):\n        Asnps = [Asnps]\n    assert len(Asnps)>0, \"need at least one Snp design matrix\"\n\n    if Acond is None:\n        Acond = Asnps\n    if (type(Acond)!=list):\n        Acond = [Acond]\n    assert len(Acond)>0, \"need at least one Snp design matrix\"\n\n    #1. run GP model to infer suitable covariance structure\n    if K1c==None or K2c==None:\n        vc = _estimateKronCovariances(phenos=phenos, K1r=K1r, K2r=K2r, K1c=K1c, K2c=K2c, covs=covs, Acovs=Acovs, **kw_args)\n        K1c = vc.getTraitCovar(0)\n        K2c = vc.getTraitCovar(1)\n    else:\n        vc = None\n        assert K1c.shape[0]==P, 'K1c: dimensions dismatch'\n        assert K1c.shape[1]==P, 'K1c: dimensions dismatch'\n        assert K2c.shape[0]==P, 'K2c: dimensions dismatch'\n        assert K2c.shape[1]==P, 'K2c: dimensions dismatch'\n    t0 = time.time()\n    lm,pv = test_lmm_kronecker(snps=snps,phenos=phenos,Asnps=Asnps,K1r=K1r,K2r=K2r,K1c=K1c,K2c=K2c,covs=covs,Acovs=Acovs)\n\n    #get pv\n    #start stuff\n    iadded = []\n    pvadded = []\n    qvadded = []\n    time_el = []\n    pvall = []\n    qvall = None\n    t1=time.time()\n    if verbose:\n        print((\"finished GWAS testing in %.2f seconds\" %(t1-t0)))\n    time_el.append(t1-t0)\n    pvall.append(pv)\n    imin= np.unravel_index(pv.argmin(),pv.shape)\n    score=pv[imin].min()\n    niter = 1\n    if qvalues:\n        assert pv.shape[0]==1, \"This is untested with the fdr package. pv.shape[0]==1 failed\"\n        qvall = []\n        qv  = FDR.qvalues(pv)\n        qvall.append(qv)\n        score=qv[imin]\n    #loop:\n    while (score<threshold) and niter<maxiter:\n        t0=time.time()\n        pvadded.append(pv[imin])\n        iadded.append(imin)\n        if qvalues:\n            qvadded.append(qv[imin])\n        if update_covariances and vc is not None:\n            vc.addFixedTerm(snps[:,imin[1]:(imin[1]+1)],Acond[imin[0]])\n            vc.setScales()#CL: don't know what this does, but findLocalOptima crashes becahuse vc.noisPos=None\n            vc.findLocalOptima(fast=True)\n            K1c = vc.getTraitCovar(0)\n            K2c = vc.getTraitCovar(1)\n            lm.setK1c(K1c)\n            lm.setK2c(K2c)\n        lm.addCovariates(snps[:,imin[1]:(imin[1]+1)],Acond[imin[0]])\n        for i in range(len(Asnps)):\n            #add SNP design\n            lm.setSNPcoldesign(Asnps[i])\n            lm.process()\n            pv[i,:] = lm.getPv()[0]\n        pvall.append(pv.ravel())\n        imin= np.unravel_index(pv.argmin(),pv.shape)\n        if qvalues:\n            qv = FDR.qvalues(pv)\n            qvall[niter:niter+1,:] = qv\n            score = qv[imin].min()\n        else:\n            score = pv[imin].min()\n        t1=time.time()\n        if verbose:\n            print((\"finished GWAS testing in %.2f seconds\" %(t1-t0)))\n        time_el.append(t1-t0)\n        niter=niter+1\n    RV = {}\n    RV['iadded']  = iadded\n    RV['pvadded'] = pvadded\n    RV['pvall']   = np.array(pvall)\n    RV['time_el'] = time_el\n    if qvalues:\n        RV['qvall'] = qvall\n        RV['qvadded'] = qvadded\n    return lm,RV\n\n\n\n\"\"\" INTERNAL \"\"\"\ndef _estimateKronCovariances(phenos,K1r=None,K1c=None,K2r=None,K2c=None,covs=None,Acovs=None,trait_covar_type='lowrank_diag',rank=1,lambd=None,verbose=True,init_method='random',old_opt=True):\n\t\"\"\"\n\testimates the background covariance model before testing\n\n\tArgs:\n\t\tphenos: [N x P] np.array of P phenotypes for N individuals\n\t\tK1r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\tK1c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\tK2r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\tK2c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n\t\t\t\t\t\tIf not provided, then linear regression analysis is performed\n\t\tcovs:           list of np.arrays holding covariates. Each covs[i] has one corresponding Acovs[i]\n\t\tAcovs:          list of np.arrays holding the phenotype design matrices for covariates.\n\t\t\t\t\t\tEach covs[i] has one corresponding Acovs[i].\n\t\ttrait_covar_type:     type of covaraince to use. Default 'freeform'. possible values are\n\t\t\t\t\t\t'freeform': free form optimization,\n\t\t\t\t\t\t'fixed': use a fixed matrix specified in covar_K0,\n\t\t\t\t\t\t'diag': optimize a diagonal matrix,\n\t\t\t\t\t\t'lowrank': optimize a low rank matrix. The rank of the lowrank part is specified in the variable rank,\n\t\t\t\t\t\t'lowrank_id': optimize a low rank matrix plus the weight of a constant diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n\t\t\t\t\t\t'lowrank_diag': optimize a low rank matrix plus a free diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n\t\t\t\t\t\t'block': optimize the weight of a constant P x P block matrix of ones,\n\t\t\t\t\t\t'block_id': optimize the weight of a constant P x P block matrix of ones plus the weight of a constant diagonal matrix,\n\t\t\t\t\t\t'block_diag': optimize the weight of a constant P x P block matrix of ones plus a free diagonal matrix,\n\t\trank:           rank of a possible lowrank component (default 1)\n\n\tReturns:\n\t\tVarianceDecomposition object\n\t\"\"\"\n\tprint(\".. Training the backgrond covariance with a GP model\")\n\tvc = VAR.VarianceDecomposition(phenos)\n\tif K1r is not None:\n\t\tvc.addRandomEffect(K1r,trait_covar_type=trait_covar_type,rank=rank)\n\tif K2r is not None:\n\t\t#TODO: fix this; forces second term to be the noise covariance\n\t\tvc.addRandomEffect(is_noise=True,K=K2r,trait_covar_type=trait_covar_type,rank=rank)\n\tfor ic  in range(len(Acovs)):\n\t\tvc.addFixedEffect(covs[ic],Acovs[ic])\n\tstart = time.time()\n\tif old_opt:\n\t\tconv = vc.optimize(fast=True)\n\telif lambd is not None:\n\t\tconv = vc.optimize(init_method=init_method,verbose=verbose,lambd=lambd)\n\telse:\n\t\tconv = vc.optimize(init_method=init_method,verbose=verbose)\n\tassert conv, \"Variance Decomposition has not converged\"\n\ttime_el = time.time()-start\n\tprint((\"Background model trained in %.2f s\" % time_el))\n\treturn vc\n\ndef _updateKronCovs(covs,Acovs,N,P):\n    \"\"\"\n    make sure that covs and Acovs are lists\n    \"\"\"\n    if (covs is None) and (Acovs is None):\n        covs = [np.ones([N,1])]\n        Acovs = [np.eye(P)]\n\n    if Acovs is None or covs is None:\n        raise Exception(\"Either Acovs or covs is None, while the other isn't\")\n\n    if (type(Acovs)!=list) and (type(covs)!=list):\n        Acovs= [Acovs]\n        covs = [covs]\n    if (type(covs)!=list) or (type(Acovs)!=list) or (len(covs)!=len(Acovs)):\n        raise Exception(\"Either Acovs or covs is not a list or they missmatch in length\")\n    return covs, Acovs\n\n\n\"\"\" DEPRECATED AND\/OR NOT USED\"\"\"\n\ndef test_interaction_kronecker_deprecated(snps,phenos,covs=None,Acovs=None,Asnps1=None,Asnps0=None,K1r=None,K1c=None,K2r=None,K2c=None,trait_covar_type='lowrank_diag',rank=1,searchDelta=False):\n    \"\"\"\n    I-variate fixed effects interaction test for phenotype specific SNP effects.\n    (Runs multiple likelihood ratio tests and computes the P-values in python from the likelihood ratios)\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        phenos: [N x P] np.array of P phenotypes for N individuals\n        covs:           list of np.arrays holding covariates. Each covs[i] has one corresponding Acovs[i]\n        Acovs:          list of np.arrays holding the phenotype design matrices for covariates.\n                        Each covs[i] has one corresponding Acovs[i].\n        Asnps1:         list of np.arrays of I interaction variables to be tested for N\n                        individuals. Note that it is assumed that Asnps0 is already included.\n                        If not provided, the alternative model will be the independent model\n        Asnps0:         single np.array of I0 interaction variables to be included in the\n                        background model when testing for interaction with Inters\n        K1r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K1c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2r:    [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        K2c:    [P x P] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        trait_covar_type:     type of covaraince to use. Default 'freeform'. possible values are\n                        'freeform': free form optimization,\n                        'fixed': use a fixed matrix specified in covar_K0,\n                        'diag': optimize a diagonal matrix,\n                        'lowrank': optimize a low rank matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_id': optimize a low rank matrix plus the weight of a constant diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'lowrank_diag': optimize a low rank matrix plus a free diagonal matrix. The rank of the lowrank part is specified in the variable rank,\n                        'block': optimize the weight of a constant P x P block matrix of ones,\n                        'block_id': optimize the weight of a constant P x P block matrix of ones plus the weight of a constant diagonal matrix,\n                        'block_diag': optimize the weight of a constant P x P block matrix of ones plus a free diagonal matrix,\n        rank:           rank of a possible lowrank component (default 1)\n        searchDelta:    Boolean indicator if delta is optimized during SNP testing (default False)\n\n    Returns:\n        pv:     P-values of the interaction test\n        lrt0:   log likelihood ratio statistics of the null model\n        pv0:    P-values of the null model\n        lrt:    log likelihood ratio statistics of the interaction test\n        lrtAlt: log likelihood ratio statistics of the alternative model\n        pvAlt:  P-values of the alternative model\n    \"\"\"\n    S=snps.shape[1]\n    #0. checks\n    N  = phenos.shape[0]\n    P  = phenos.shape[1]\n\n    if K1r==None:\n        K1r = np.dot(snps,snps.T)\n    else:\n        assert K1r.shape[0]==N, 'K1r: dimensions dismatch'\n        assert K1r.shape[1]==N, 'K1r: dimensions dismatch'\n\n    if K2r==None:\n        K2r = np.eye(N)\n    else:\n        assert K2r.shape[0]==N, 'K2r: dimensions dismatch'\n        assert K2r.shape[1]==N, 'K2r: dimensions dismatch'\n\n    covs,Acovs = _updateKronCovs(covs,Acovs,N,P)\n\n    #Asnps can be several designs\n    if (Asnps0 is None):\n        Asnps0 = [np.ones([1,P])]\n    if Asnps1 is None:\n        Asnps1 = [np.eye([P])]\n    if (type(Asnps0)!=list):\n        Asnps0 = [Asnps0]\n    if (type(Asnps1)!=list):\n        Asnps1 = [Asnps1]\n    assert (len(Asnps0)==1) and (len(Asnps1)>0), \"need at least one Snp design matrix for null and alt model\"\n\n    #one row per column design matrix\n    pv = np.zeros((len(Asnps1),snps.shape[1]))\n    lrt = np.zeros((len(Asnps1),snps.shape[1]))\n    pvAlt = np.zeros((len(Asnps1),snps.shape[1]))\n    lrtAlt = np.zeros((len(Asnps1),snps.shape[1]))\n\n    #1. run GP model to infer suitable covariance structure\n    if K1c==None or K2c==None:\n        vc = _estimateKronCovariances(phenos=phenos, K1r=K1r, K2r=K2r, K1c=K1c, K2c=K2c, covs=covs, Acovs=Acovs, trait_covar_type=trait_covar_type, rank=rank)\n        K1c = vc.getTraitCovar(0)\n        K2c = vc.getTraitCovar(1)\n    else:\n        assert K1c.shape[0]==P, 'K1c: dimensions dismatch'\n        assert K1c.shape[1]==P, 'K1c: dimensions dismatch'\n        assert K2c.shape[0]==P, 'K2c: dimensions dismatch'\n        assert K2c.shape[1]==P, 'K2c: dimensions dismatch'\n\n    #2. run kroneckerLMM for null model\n    lmm = limix.deprecated.CKroneckerLMM()\n    lmm.setK1r(K1r)\n    lmm.setK1c(K1c)\n    lmm.setK2r(K2r)\n    lmm.setK2c(K2c)\n    lmm.setSNPs(snps)\n    #add covariates\n    for ic  in range(len(Acovs)):\n        lmm.addCovariates(covs[ic],Acovs[ic])\n    lmm.setPheno(phenos)\n    if searchDelta:      lmm.setNumIntervalsAlt(100)\n    else:                   lmm.setNumIntervalsAlt(0)\n    lmm.setNumIntervals0(100)\n    #add SNP design\n    lmm.setSNPcoldesign(Asnps0[0])\n    lmm.process()\n    dof0 = Asnps0[0].shape[0]\n    pv0 = lmm.getPv()\n    lrt0 = st.chi2.isf(pv0,dof0)\n    for iA in range(len(Asnps1)):\n        dof1 = Asnps1[iA].shape[0]\n        dof = dof1-dof0\n        lmm.setSNPcoldesign(Asnps1[iA])\n        lmm.process()\n        pvAlt[iA,:] = lmm.getPv()[0]\n        lrtAlt[iA,:] = st.chi2.isf(pvAlt[iA,:],dof1)\n        lrt[iA,:] = lrtAlt[iA,:] - lrt0[0] # Don't need the likelihood ratios, as null model is the same between the two models\n        pv[iA,:] = st.chi2.sf(lrt[iA,:],dof)\n    return pv,lrt0,pv0,lrt,lrtAlt,pvAlt\n\n#TODO: we need to fix. THis does not work as interact_GxE is not existing\n#I vote we also use **kw_args to forward parameters to interact_Gxe?\ndef test_interaction_GxG(pheno,snps1,snps2=None,K=None,covs=None,test='lrt'):\n    \"\"\"\n    Epistasis test between two sets of SNPs\n\n    Args:\n        pheno:  [N x 1] np.array of 1 phenotype for N individuals\n        snps1:  [N x S1] np.array of S1 SNPs for N individuals\n        snps2:  [N x S2] np.array of S2 SNPs for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        covs:   [N x D] np.array of D covariates for N individuals\n        test:    'lrt' for likelihood ratio test (default) or 'f' for F-test\n\n    Returns:\n        pv:     [S2 x S1] np.array of P values for epistasis tests beten all SNPs in\n                snps1 and snps2\n    \"\"\"\n    if K is None:\n        K=np.eye(N)\n    N=snps1.shape[0]\n    if snps2 is None:\n        snps2 = snps1\n    return test_interaction_GxE_1dof(snps=snps1,pheno=pheno,env=snps2,covs=covs,K=K,test=test)\n\n\ndef test_interaction_GxE_1dof(snps,pheno,env,K=None,covs=None, test='lrt',verbose=None):\n    \"\"\"\n    Univariate GxE fixed effects interaction linear mixed model test for all\n    pairs of SNPs and environmental variables.\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals\n        pheno:  [N x 1] np.array of 1 phenotype for N individuals\n        env:    [N x E] np.array of E environmental variables for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        covs:   [N x D] np.array of D covariates for N individuals\n        test:    'lrt' for likelihood ratio test (default) or 'f' for F-test\n        verbose: print verbose output? (False)\n\n    Returns:\n        pv:     [E x S] np.array of P values for interaction tests between all\n                E environmental variables and all S SNPs\n    \"\"\"\n    verbose = dlimix.getVerbose(verbose)\n    N=snps.shape[0]\n    if K is None:\n        K=np.eye(N)\n    if covs is None:\n        covs = np.ones((N,1))\n    assert (env.shape[0]==N and pheno.shape[0]==N and K.shape[0]==N and K.shape[1]==N and covs.shape[0]==N), \"shapes missmatch\"\n    Inter0 = np.ones((N,1))\n    pv = np.zeros((env.shape[1],snps.shape[1]))\n    if verbose:\n        print((\"starting %i interaction scans for %i SNPs each.\" % (env.shape[1], snps.shape[1])))\n    t0=time.time()\n    for i in range(env.shape[1]):\n        t0_i = time.time()\n        cov_i = np.concatenate((covs,env[:,i:(i+1)]),1)\n        lm_i = test_interaction_lmm(snps=snps,pheno=pheno,covs=cov_i,Inter=env[:,i:(i+1)],Inter0=Inter0,test=test)\n        pv[i,:]=lm_i.getPv()[0,:]\n        t1_i = time.time()\n        if verbose:\n            print((\"Finished %i out of %i interaction scans in %.2f seconds.\"%((i+1),env.shape[1],(t1_i-t0_i))))\n    t1 = time.time()\n    print((\"-----------------------------------------------------------\\nFinished all %i interaction scans in %.2f seconds.\"%(env.shape[1],(t1-t0))))\n    return pv\n\n\ndef phenSpecificEffects(snps,pheno1,pheno2,K=None,covs=None,test='lrt'):\n    \"\"\"\n    Univariate fixed effects interaction test for phenotype specific SNP effects\n\n    Args:\n        snps:   [N x S] np.array of S SNPs for N individuals (test SNPs)\n        pheno1: [N x 1] np.array of 1 phenotype for N individuals\n        pheno2: [N x 1] np.array of 1 phenotype for N individuals\n        K:      [N x N] np.array of LMM-covariance\/kinship koefficients (optional)\n                        If not provided, then linear regression analysis is performed\n        covs:   [N x D] np.array of D covariates for N individuals\n        test:    'lrt' for likelihood ratio test (default) or 'f' for F-test\n\n    Returns:\n        limix LMM object\n    \"\"\"\n    N=snps.shape[0]\n    if K is None:\n        K=np.eye(N)\n    assert (pheno1.shape[1]==pheno2.shape[1]), \"Only consider equal number of phenotype dimensions\"\n    if covs is None:\n        covs = np.ones(N,1)\n    assert (pheno1.shape[1]==1 and pheno2.shape[1]==1 and pheno1.shape[0]==N and pheno2.shape[0]==N and K.shape[0]==N and K.shape[1]==N and covs.shape[0]==N), \"shapes missmatch\"\n    Inter = np.zeros((N*2,1))\n    Inter[0:N,0]=1\n    Inter0 = np.ones((N*2,1))\n    Yinter=np.concatenate((pheno1,pheno2),0)\n    Xinter = np.tile(snps,(2,1))\n    Covitner= np.tile(covs(2,1))\n    lm = test_interaction_lmm(snps=Xinter,pheno=Yinter,covs=Covinter,Inter=Inter,Inter0=Inter0,test=test)\n    return lm\n","license":"apache-2.0","hash":3088100344953002130,"line_mean":42.5140640155,"line_max":251,"alpha_frac":0.6410627912,"autogenerated":false},
{"repo_name":"mikhtonyuk\/3DEngine","path":"Misc\/Skripts\/validation.py","copies":"1","size":"2147","content":"import sys\r\nimport io\r\n\r\nMAX_PARAMS = int(sys.argv[1])\r\n\r\n#=======================================================================================\r\n\r\nheader = '''\r\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\r\n#define _VT_RET(x) VariantTraits<Utils::TStripType<x>::ref2ptr>::type\r\n#define _VT_PARAM(x) VariantTraits<Utils::TStripType<x>::noref>::type\r\n\t\t\r\n'''\r\n\r\nvalidate_func = '''\r\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\r\n\r\ntemplate<%targs%class RT>\r\ninline ValidationEntry* ValidationInfo(RT (CALL_CONV *)(%args%))\r\n{\r\n\tstatic ValidationEntry e[] = { %args_t%_VT_RET(RT), 0};\r\n\treturn e;\r\n}\r\n'''\r\n\r\nvalidate_method = '''\r\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\r\n\r\ntemplate<class C, %targs%class RT>\r\nValidationEntry* ValidationInfo(RT (CALL_CONV C::*func)(%args%))\r\n{\r\n\tstatic ValidationEntry e[] = { T_CUSTOM, %args_t%_VT_RET(RT), 0};\r\n\treturn e;\r\n}\r\n'''\r\n\r\n#=======================================================================================\r\n\r\nwith open('validation.inc', 'w') as file:\r\n\r\n\tfile.write(header)\r\n\r\n\tfor i in range(0, MAX_PARAMS + 1):\r\n\t\r\n\t\tblock = validate_func\r\n\t\t\r\n\t\ttargs = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\ttargs += \"class A\" + str(j) + \", \"\r\n\t\t\r\n\t\targs = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\targs += \"A\" + str(j) + \", \"\r\n\t\targs = args[:-2]\r\n\t\t\t\r\n\t\targs_t = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\targs_t += \"_VT_PARAM(A\" + str(j) + \"), \"\r\n\t\t\t\r\n\t\tblock = block.replace(\"%targs%\", targs)\r\n\t\tblock = block.replace(\"%args%\", args)\r\n\t\tblock = block.replace(\"%args_t%\", args_t)\r\n\t\t\r\n\t\tfile.write(block)\r\n\t\t\r\n\t\t\r\n\t\t\r\n\tfor i in range(0, MAX_PARAMS + 1):\r\n\t\r\n\t\tblock = validate_method\r\n\t\t\r\n\t\ttargs = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\ttargs += \"class A\" + str(j) + \", \"\r\n\t\t\r\n\t\targs = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\targs += \"A\" + str(j) + \", \"\r\n\t\targs = args[:-2]\r\n\t\t\t\r\n\t\targs_t = \"\"\r\n\t\tfor j in range(1, i + 1):\r\n\t\t\targs_t += \"_VT_PARAM(A\" + str(j) + \"), \"\r\n\t\t\t\r\n\t\tblock = block.replace(\"%targs%\", targs)\r\n\t\tblock = block.replace(\"%args%\", args)\r\n\t\tblock = block.replace(\"%args_t%\", args_t)\r\n\t\t\r\n\t\tfile.write(block)\r\n\t\r\n\r\n","license":"gpl-3.0","hash":3876049789243577389,"line_mean":21.5934065934,"line_max":88,"alpha_frac":0.43223102,"autogenerated":false},
{"repo_name":"venicegeo\/eventkit-cloud","path":"eventkit_cloud\/utils\/geocoding\/geocode_auth_response.py","copies":"1","size":"2115","content":"import logging\nimport os\nfrom eventkit_cloud.utils.geocoding.geocode_auth import (\n    get_session_cookies,\n    get_auth_headers,\n    update_session_cookies,\n)\nfrom eventkit_cloud.utils import auth_requests\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass GeocodeAuthResponse(object):\n    def get_response(self, payload):\n        error_message = (\n            \"The Geocoding service received an error. Please try again or contact an Eventkit administrator.\"\n        )\n        if os.getenv(\"GEOCODING_AUTH_CERT\"):\n            response = get_cached_response(self.url, payload)\n            if not response:\n                response = get_auth_response(self.url, payload)\n            if response:\n                return response\n            else:\n                raise Exception(error_message)\n        else:\n            response = auth_requests.get(self.url, params=payload)\n            if not response.ok:\n                raise Exception(error_message)\n            return response\n\n\ndef get_cached_response(url, payload):\n    cookies = get_session_cookies()\n    headers = get_auth_headers()\n    response = auth_requests.get(url, params=payload, cookies=cookies, headers=headers)\n    if response.ok and check_data(response):\n        return response\n\n\ndef get_auth_response(url, payload):\n    auth_session = auth_requests.AuthSession()\n    response = auth_session.get(url, params=payload, cert_var=\"GEOCODING_AUTH_CERT\")\n    if response.ok and check_data(response):\n        update_session_cookies(auth_session.session.cookies)\n        return response\n\n\ndef check_data(response):\n    # This assumes that a geojson is a valid response, so we're checking for a\n    # geometry object, feature object, or FeatureCollection, but aren't verifying that its a valid\n    # geojson.  Nominatim may not return a geojson in the initial response, so check for geojson field.\n    try:\n        if any(field in response.json() for field in (\"features\", \"geometry\", \"coordinates\", \"geojson\")):\n            return True\n    except Exception:\n        logger.warning(\"Invalid response.\")\n        logger.debug(response.content)\n    return False\n","license":"bsd-3-clause","hash":1901826481872045220,"line_mean":34.25,"line_max":109,"alpha_frac":0.6619385343,"autogenerated":false},
{"repo_name":"GLolol\/PyLink","path":"protocols\/ircs2s_common.py","copies":"1","size":"30695","content":"\"\"\"\nircs2s_common.py: Common base protocol class with functions shared by TS6 and P10-based protocols.\n\"\"\"\n\nimport re\nimport time\n\nfrom pylinkirc import conf\nfrom pylinkirc.classes import IRCNetwork, ProtocolError\nfrom pylinkirc.log import log\n\n__all__ = ['IncrementalUIDGenerator', 'IRCCommonProtocol', 'IRCS2SProtocol']\n\n\nclass IncrementalUIDGenerator():\n    \"\"\"\n    Incremental UID Generator module, adapted from InspIRCd source:\n    https:\/\/github.com\/inspircd\/inspircd\/blob\/f449c6b296ab\/src\/server.cpp#L85-L156\n    \"\"\"\n\n    def __init__(self, sid):\n        if not (hasattr(self, 'allowedchars') and hasattr(self, 'length')):\n             raise RuntimeError(\"Allowed characters list not defined. Subclass \"\n                                \"%s by defining self.allowedchars and self.length \"\n                                \"and then calling super().__init__().\" % self.__class__.__name__)\n        self.uidchars = [self.allowedchars[0]]*self.length\n        self.sid = str(sid)\n\n    def increment(self, pos=None):\n        \"\"\"\n        Increments the UID generator to the next available UID.\n        \"\"\"\n        # Position starts at 1 less than the UID length.\n        if pos is None:\n            pos = self.length - 1\n\n        # If we're at the last character in the list of allowed ones, reset\n        # and increment the next level above.\n        if self.uidchars[pos] == self.allowedchars[-1]:\n            self.uidchars[pos] = self.allowedchars[0]\n            self.increment(pos-1)\n        else:\n            # Find what position in the allowed characters list we're currently\n            # on, and add one.\n            idx = self.allowedchars.find(self.uidchars[pos])\n            self.uidchars[pos] = self.allowedchars[idx+1]\n\n    def next_uid(self):\n        \"\"\"\n        Returns the next unused UID for the server.\n        \"\"\"\n        uid = self.sid + ''.join(self.uidchars)\n        self.increment()\n        return uid\n\nclass IRCCommonProtocol(IRCNetwork):\n\n    COMMON_PREFIXMODES = [('h', 'halfop'), ('a', 'admin'), ('q', 'owner'), ('y', 'owner')]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._caps = {}\n        self._use_builtin_005_handling = False  # Disabled by default for greater security\n        self.protocol_caps |= {'has-irc-modes', 'can-manage-bot-channels'}\n\n    def post_connect(self):\n        self._caps.clear()\n\n    def validate_server_conf(self):\n        \"\"\"Validates that the server block given contains the required keys.\"\"\"\n        for k in self.conf_keys:\n            log.debug('(%s) Checking presence of conf key %r', self.name, k)\n            conf.validate(k in self.serverdata,\n                     \"Missing option %r in server block for network %s.\"\n                     % (k, self.name))\n\n        port = self.serverdata['port']\n        conf.validate(isinstance(port, int) and 0 < port < 65535,\n                 \"Invalid port %r for network %s\"\n                 % (port, self.name))\n\n    @staticmethod\n    def parse_args(args):\n        \"\"\"\n        Parses a string or list of of RFC1459-style arguments, where \":\" may\n        be used for multi-word arguments that last until the end of a line.\n        \"\"\"\n        if isinstance(args, str):\n            args = args.split(' ')\n\n        real_args = []\n        for idx, arg in enumerate(args):\n            if arg.startswith(':') and idx != 0:\n                # \":\" is used to begin multi-word arguments that last until the end of the message.\n                # Use list splicing here to join them into one argument, and then add it to our list of args.\n                joined_arg = ' '.join(args[idx:])[1:]  # Cut off the leading : as well\n                real_args.append(joined_arg)\n                break\n            elif arg.strip():  # Skip empty args that aren't part of the multi-word arg\n                real_args.append(arg)\n\n        return real_args\n\n    @classmethod\n    def parse_prefixed_args(cls, args):\n        \"\"\"Similar to parse_args(), but stripping leading colons from the first argument\n        of a line (usually the sender field).\"\"\"\n        args = cls.parse_args(args)\n        args[0] = args[0].split(':', 1)[1]\n        return args\n\n    @staticmethod\n    def parse_isupport(args, fallback=''):\n        \"\"\"\n        Parses a string of capabilities in the 005 \/ RPL_ISUPPORT format.\n        \"\"\"\n\n        if isinstance(args, str):\n            args = args.split(' ')\n\n        caps = {}\n        for cap in args:\n            try:\n                # Try to split it as a KEY=VALUE pair.\n                key, value = cap.split('=', 1)\n            except ValueError:\n                key = cap\n                value = fallback\n            caps[key] = value\n\n        return caps\n\n    @staticmethod\n    def parse_isupport_prefixes(args):\n        \"\"\"\n        Separates prefixes field like \"(qaohv)~&@%+\" into a dict mapping mode characters to mode\n        prefixes.\n        \"\"\"\n        prefixsearch = re.search(r'\\(([A-Za-z]+)\\)(.*)', args)\n        return dict(zip(prefixsearch.group(1), prefixsearch.group(2)))\n\n    @classmethod\n    def parse_message_tags(cls, data):\n        \"\"\"\n        Parses IRCv3.2 message tags from a message, as described at http:\/\/ircv3.net\/specs\/core\/message-tags-3.2.html\n\n        data is a list of command arguments, split by spaces.\n        \"\"\"\n        # Example query:\n        # @aaa=bbb;ccc;example.com\/ddd=eee :nick!ident@host.com PRIVMSG me :Hello\n        if data[0].startswith('@'):\n            tagdata = data[0].lstrip('@').split(';')\n            for idx, tag in enumerate(tagdata):\n                tag = tag.replace('\\\\s', ' ')\n                tag = tag.replace('\\\\r', '\\r')\n                tag = tag.replace('\\\\n', '\\n')\n                tag = tag.replace('\\\\:', ';')\n\n                # We want to drop lone \\'s but keep \\\\ as \\ ...\n                tag = tag.replace('\\\\\\\\', '\\x00')\n                tag = tag.replace('\\\\', '')\n                tag = tag.replace('\\x00', '\\\\')\n                tagdata[idx] = tag\n\n            results = cls.parse_isupport(tagdata, fallback='')\n            return results\n        return {}\n\n    def handle_away(self, source, command, args):\n        \"\"\"Handles incoming AWAY messages.\"\"\"\n        # TS6:\n        # <- :6ELAAAAAB AWAY :Auto-away\n        # <- :6ELAAAAAB AWAY\n        # P10:\n        # <- ABAAA A :blah\n        # <- ABAAA A\n        if source not in self.users:\n            return\n\n        try:\n            self.users[source].away = text = args[0]\n        except IndexError:  # User is unsetting away status\n            self.users[source].away = text = ''\n        return {'text': text}\n\n    def handle_error(self, numeric, command, args):\n        \"\"\"Handles ERROR messages - these mean that our uplink has disconnected us!\"\"\"\n        raise ProtocolError('Received an ERROR, disconnecting!')\n\n    def handle_pong(self, source, command, args):\n        \"\"\"Handles incoming PONG commands.\"\"\"\n        if source == self.uplink:\n            self.lastping = time.time()\n\n    def handle_005(self, source, command, args):\n        \"\"\"\n        Handles 005 \/ RPL_ISUPPORT. This is used by at least Clientbot and ngIRCd (for server negotiation).\n        \"\"\"\n        # ngIRCd:\n        # <- :ngircd.midnight.local 005 pylink-devel.int NETWORK=ngircd-test :is my network name\n        # <- :ngircd.midnight.local 005 pylink-devel.int RFC2812 IRCD=ngIRCd CHARSET=UTF-8 CASEMAPPING=ascii PREFIX=(qaohv)~&@%+ CHANTYPES=#&+ CHANMODES=beI,k,l,imMnOPQRstVz CHANLIMIT=#&+:10 :are supported on this server\n        # <- :ngircd.midnight.local 005 pylink-devel.int CHANNELLEN=50 NICKLEN=21 TOPICLEN=490 AWAYLEN=127 KICKLEN=400 MODES=5 MAXLIST=beI:50 EXCEPTS=e INVEX=I PENALTY :are supported on this server\n\n        # Regular clientbot, connecting to InspIRCd:\n        # <- :millennium.overdrivenetworks.com 005 ice AWAYLEN=200 CALLERID=g CASEMAPPING=rfc1459 CHANMODES=IXbegw,k,FJLfjl,ACKMNOPQRSTUcimnprstz CHANNELLEN=64 CHANTYPES=# CHARSET=ascii ELIST=MU ESILENCE EXCEPTS=e EXTBAN=,ACNOQRSTUcmprsuz FNC INVEX=I :are supported by this server\n        # <- :millennium.overdrivenetworks.com 005 ice KICKLEN=255 MAP MAXBANS=60 MAXCHANNELS=30 MAXPARA=32 MAXTARGETS=20 MODES=20 NAMESX NETWORK=OVERdrive-IRC NICKLEN=21 OVERRIDE PREFIX=(Yqaohv)*~&@%+ SILENCE=32 :are supported by this server\n        # <- :millennium.overdrivenetworks.com 005 ice SSL=[::]:6697 STARTTLS STATUSMSG=*~&@%+ TOPICLEN=307 UHNAMES USERIP VBANLIST WALLCHOPS WALLVOICES WATCH=32 :are supported by this server\n\n        if not self._use_builtin_005_handling:\n            log.warning(\"(%s) Got spurious 005 message from %s: %r\", self.name, source, args)\n            return\n\n        newcaps = self.parse_isupport(args[1:-1])\n        self._caps.update(newcaps)\n        log.debug('(%s) handle_005: self._caps is %s', self.name, self._caps)\n\n        if 'CHANMODES' in newcaps:\n            self.cmodes['*A'], self.cmodes['*B'], self.cmodes['*C'], self.cmodes['*D'] = \\\n                newcaps['CHANMODES'].split(',')\n        log.debug('(%s) handle_005: cmodes: %s', self.name, self.cmodes)\n\n        if 'USERMODES' in newcaps:\n            self.umodes['*A'], self.umodes['*B'], self.umodes['*C'], self.umodes['*D'] = \\\n                newcaps['USERMODES'].split(',')\n        log.debug('(%s) handle_005: umodes: %s', self.name, self.umodes)\n\n        if 'CASEMAPPING' in newcaps:\n            self.casemapping = newcaps.get('CASEMAPPING', self.casemapping)\n            log.debug('(%s) handle_005: casemapping set to %s', self.name, self.casemapping)\n\n        if 'PREFIX' in newcaps:\n            self.prefixmodes = prefixmodes = self.parse_isupport_prefixes(newcaps['PREFIX'])\n            log.debug('(%s) handle_005: prefix modes set to %s', self.name, self.prefixmodes)\n\n            # Autodetect common prefix mode names.\n            for char, modename in self.COMMON_PREFIXMODES:\n                # Don't overwrite existing named mode definitions.\n                if char in self.prefixmodes and modename not in self.cmodes:\n                    self.cmodes[modename] = char\n                    log.debug('(%s) handle_005: autodetecting mode %s (%s) as %s', self.name,\n                              char, self.prefixmodes[char], modename)\n\n        # https:\/\/defs.ircdocs.horse\/defs\/isupport.html\n        if 'EXCEPTS' in newcaps:\n            # Handle EXCEPTS=e or EXCEPTS fields\n            self.cmodes['banexception'] = newcaps.get('EXCEPTS') or 'e'\n            log.debug('(%s) handle_005: got cmode banexception=%r', self.name, self.cmodes['banexception'])\n\n        if 'INVEX' in newcaps:\n            # Handle INVEX=I, INVEX fields\n            self.cmodes['invex'] = newcaps.get('INVEX') or 'I'\n            log.debug('(%s) handle_005: got cmode invex=%r', self.name, self.cmodes['invex'])\n\n        if 'NICKLEN' in newcaps:\n            # Handle NICKLEN=number\n            assert newcaps['NICKLEN'], \"Got NICKLEN tag with no content?\"\n            self.maxnicklen = int(newcaps['NICKLEN'])\n            log.debug('(%s) handle_005: got %r for maxnicklen', self.name, self.maxnicklen)\n\n        if 'DEAF' in newcaps:\n            # Handle DEAF=D, DEAF fields\n            self.umodes['deaf'] = newcaps.get('DEAF') or 'D'\n            log.debug('(%s) handle_005: got umode deaf=%r', self.name, self.umodes['deaf'])\n\n        if 'CALLERID' in newcaps:\n            # Handle CALLERID=g, CALLERID fields\n            self.umodes['callerid'] = newcaps.get('CALLERID') or 'g'\n            log.debug('(%s) handle_005: got umode callerid=%r', self.name, self.umodes['callerid'])\n\n        if 'STATUSMSG' in newcaps:\n            # Note: This assumes that all available prefixes can be used in STATUSMSG too.\n            # Even though this isn't always true, I don't see the point in making things\n            # any more complicated.\n            self.protocol_caps |= {'has-statusmsg'}\n\n    def _send_with_prefix(self, source, msg, **kwargs):\n        \"\"\"Sends a RFC1459-style raw command from the given sender.\"\"\"\n        self.send(':%s %s' % (self._expandPUID(source), msg), **kwargs)\n\nclass IRCS2SProtocol(IRCCommonProtocol):\n    COMMAND_TOKENS = {}\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.protocol_caps |= {'can-spawn-clients', 'has-ts', 'can-host-relay',\n                               'can-track-servers'}\n\n        # Alias\n        self.handle_squit = self._squit\n\n    def handle_events(self, data):\n        \"\"\"Event handler for RFC1459-like protocols.\n\n        This passes most commands to the various handle_ABCD() functions\n        elsewhere defined protocol modules, coersing various sender prefixes\n        from nicks and server names to UIDs and SIDs respectively,\n        whenever possible.\n\n        Commands sent without an explicit sender prefix will have them set to\n        the SID of the uplink server.\n        \"\"\"\n        data = data.split(\" \")\n\n        tags = self.parse_message_tags(data)\n        if tags:\n            # If we have message tags, split off the first argument.\n            data = data[1:]\n\n        args = self.parse_args(data)\n\n        sender = args[0]\n        sender = sender.lstrip(':')\n\n        # If the sender isn't in numeric format, try to convert it automatically.\n        sender_sid = self._get_SID(sender)\n        sender_uid = self._get_UID(sender)\n\n        if sender_sid in self.servers:\n            # Sender is a server (converting from name to SID gave a valid result).\n            sender = sender_sid\n        elif sender_uid in self.users:\n            # Sender is a user (converting from name to UID gave a valid result).\n            sender = sender_uid\n        elif not (args[0].startswith(':')):\n            # No sender prefix; treat as coming from uplink IRCd.\n            sender = self.uplink\n            args.insert(0, sender)\n\n        raw_command = args[1].upper()\n        args = args[2:]\n\n        log.debug('(%s) Found message sender as %s, raw_command=%r, args=%r', self.name, sender, raw_command, args)\n\n        # For P10, convert the command token into a regular command, if present.\n        command = self.COMMAND_TOKENS.get(raw_command, raw_command)\n        if command != raw_command:\n            log.debug('(%s) Translating token %s to command %s', self.name, raw_command, command)\n\n        if self.is_internal_client(sender) or self.is_internal_server(sender):\n            log.warning(\"(%s) Received command %s being routed the wrong way!\", self.name, command)\n            return\n\n        if command == 'ENCAP':\n            # Special case for TS6 encapsulated commands (ENCAP), in forms like this:\n            # <- :00A ENCAP * SU 42XAAAAAC :GLolol\n            command = args[1]\n            args = args[2:]\n            log.debug(\"(%s) Rewriting incoming ENCAP to command %s (args: %s)\", self.name, command, args)\n\n        try:\n            func = getattr(self, 'handle_'+command.lower())\n        except AttributeError:  # Unhandled command\n            pass\n        else:\n            parsed_args = func(sender, command, args)\n            if parsed_args is not None:\n                if tags:\n                    parsed_args['tags'] = tags  # Add message tags to this hook payload.\n                return [sender, command, parsed_args]\n\n    def invite(self, source, target, channel):\n        \"\"\"Sends an INVITE from a PyLink client.\"\"\"\n        if not self.is_internal_client(source):\n            raise LookupError('No such PyLink client exists.')\n\n        self._send_with_prefix(source, 'INVITE %s %s' % (self._expandPUID(target), channel))\n\n    def kick(self, numeric, channel, target, reason=None):\n        \"\"\"Sends kicks from a PyLink client\/server.\"\"\"\n\n        if (not self.is_internal_client(numeric)) and \\\n                (not self.is_internal_server(numeric)):\n            raise LookupError('No such PyLink client\/server exists.')\n\n        if not reason:\n            reason = 'No reason given'\n\n        # Mangle kick targets for IRCds that require it.\n        real_target = self._expandPUID(target)\n\n        self._send_with_prefix(numeric, 'KICK %s %s :%s' % (channel, real_target, reason))\n\n        # We can pretend the target left by its own will; all we really care about\n        # is that the target gets removed from the channel userlist, and calling\n        # handle_part() does that just fine.\n        self.handle_part(target, 'KICK', [channel])\n\n    def numeric(self, source, numeric, target, text):\n        \"\"\"Sends raw numerics from a server to a remote client. This is used for WHOIS replies.\"\"\"\n        # Mangle the target for IRCds that require it.\n        target = self._expandPUID(target)\n\n        self._send_with_prefix(source, '%s %s %s' % (numeric, target, text))\n\n    def part(self, client, channel, reason=None):\n        \"\"\"Sends a part from a PyLink client.\"\"\"\n        if not self.is_internal_client(client):\n            log.error('(%s) Error trying to part %r from %r (no such client exists)', self.name, client, channel)\n            raise LookupError('No such PyLink client exists.')\n        msg = \"PART %s\" % channel\n        if reason:\n            msg += \" :%s\" % reason\n        self._send_with_prefix(client, msg)\n        self.handle_part(client, 'PART', [channel])\n\n    def _ping_uplink(self):\n        \"\"\"Sends a PING to the uplink.\n\n        This is mostly used by PyLink internals to check whether the remote link is up.\"\"\"\n        if self.sid and self.connected.is_set():\n            self._send_with_prefix(self.sid, 'PING %s' % self._expandPUID(self.uplink))\n\n    def quit(self, numeric, reason):\n        \"\"\"Quits a PyLink client.\"\"\"\n        if self.is_internal_client(numeric):\n            self._send_with_prefix(numeric, \"QUIT :%s\" % reason)\n            self._remove_client(numeric)\n        else:\n            raise LookupError(\"No such PyLink client exists.\")\n\n    def message(self, numeric, target, text):\n        \"\"\"Sends a PRIVMSG from a PyLink client.\"\"\"\n        if not self.is_internal_client(numeric):\n            raise LookupError('No such PyLink client exists.')\n\n        # Mangle message targets for IRCds that require it.\n        target = self._expandPUID(target)\n\n        self._send_with_prefix(numeric, 'PRIVMSG %s :%s' % (target, text))\n\n    def notice(self, numeric, target, text):\n        \"\"\"Sends a NOTICE from a PyLink client or server.\"\"\"\n        if (not self.is_internal_client(numeric)) and \\\n                (not self.is_internal_server(numeric)):\n            raise LookupError('No such PyLink client\/server exists.')\n\n        # Mangle message targets for IRCds that require it.\n        target = self._expandPUID(target)\n\n        self._send_with_prefix(numeric, 'NOTICE %s :%s' % (target, text))\n\n    def squit(self, source, target, text='No reason given'):\n        \"\"\"SQUITs a PyLink server.\"\"\"\n        # -> SQUIT 9PZ :blah, blah\n        log.debug('(%s) squit: source=%s, target=%s', self.name, source, target)\n        self._send_with_prefix(source, 'SQUIT %s :%s' % (self._expandPUID(target), text))\n        self.handle_squit(source, 'SQUIT', [target, text])\n\n    def topic(self, source, target, text):\n        \"\"\"Sends a TOPIC change from a PyLink client or server.\"\"\"\n        if (not self.is_internal_client(source)) and (not self.is_internal_server(source)):\n            raise LookupError('No such PyLink client\/server exists.')\n\n        self._send_with_prefix(source, 'TOPIC %s :%s' % (target, text))\n        self._channels[target].topic = text\n        self._channels[target].topicset = True\n    topic_burst = topic\n\n    def handle_invite(self, numeric, command, args):\n        \"\"\"Handles incoming INVITEs.\"\"\"\n        # TS6:\n        #  <- :70MAAAAAC INVITE 0ALAAAAAA #blah 12345\n        # P10:\n        #  <- ABAAA I PyLink-devel #services 1460948992\n        #  Note that the target is a nickname, not a numeric.\n\n        target = self._get_UID(args[0])\n        channel = args[1]\n\n        curtime = int(time.time())\n        try:\n            ts = int(args[2])\n        except IndexError:\n            ts = curtime\n\n        ts = ts or curtime  # Treat 0 timestamps (e.g. inspircd) as the current time.\n\n        return {'target': target, 'channel': channel, 'ts': ts}\n\n    def handle_kick(self, source, command, args):\n        \"\"\"Handles incoming KICKs.\"\"\"\n        # :70MAAAAAA KICK #test 70MAAAAAA :some reason\n        channel = args[0]\n        kicked = self._get_UID(args[1])\n\n        try:\n            reason = args[2]\n        except IndexError:\n            reason = ''\n\n        log.debug('(%s) Removing kick target %s from %s', self.name, kicked, channel)\n        self.handle_part(kicked, 'KICK', [channel, reason])\n        return {'channel': channel, 'target': kicked, 'text': reason}\n\n    def handle_kill(self, source, command, args):\n        \"\"\"Handles incoming KILLs.\"\"\"\n        killed = self._get_UID(args[0])\n        # Some IRCds send explicit QUIT messages for their killed clients in addition to KILL,\n        # meaning that our target client may have been removed already. If this is the case,\n        # don't bother forwarding this message on.\n        # Generally, we only need to distinguish between KILL and QUIT if the target is\n        # one of our clients, in which case the above statement isn't really applicable.\n        if killed in self.users:\n            userdata = self._remove_client(killed)\n        else:\n            return\n\n        # TS6-style kills look something like this:\n        # <- :GL KILL 38QAAAAAA :hidden-1C620195!GL (test)\n        # What we actually want is to format a pretty kill message, in the form\n        # \"Killed (killername (reason))\".\n\n        if '!' in args[1].split(\" \", 1)[0]:\n            try:\n                # Get the nick or server name of the caller.\n                killer = self.get_friendly_name(source)\n            except KeyError:\n                # Killer was... neither? We must have aliens or something. Fallback\n                # to the given \"UID\".\n                killer = source\n\n            # Get the reason, which is enclosed in brackets.\n            killmsg = ' '.join(args[1].split(\" \")[1:])[1:-1]\n            if not killmsg:\n                log.warning('(%s) Failed to extract kill reason: %r', self.name, args)\n                killmsg = args[1]\n        else:\n            # We already have a preformatted kill, so just pass it on as is.\n            # XXX: this does create a convoluted kill string if we want to forward kills\n            # over relay.\n            # InspIRCd:\n            # <- :1MLAAAAA1 KILL 0ALAAAAAC :Killed (GL (test))\n            # ngIRCd:\n            # <- :GL KILL PyLink-devel :KILLed by GL: ?\n            killmsg = args[1]\n\n        return {'target': killed, 'text': killmsg, 'userdata': userdata}\n\n    def _check_cloak_change(self, uid):  # Stub by default\n        return\n\n    def _check_umode_away_change(self, uid):\n        # Handle away status changes based on umode +a\n        awaymode = self.umodes.get('away')\n        if uid in self.users and awaymode:\n            u = self.users[uid]\n            old_away_status = u.away\n\n            # Check whether the user is marked away, and send a hook update only if the status has changed.\n            away_status = (awaymode, None) in u.modes\n            if away_status != bool(old_away_status):\n                # This sets a dummy away reason of \"Away\" because no actual text is provided.\n                self.call_hooks([uid, 'AWAY', {'text': 'Away' if away_status else ''}])\n\n    def _check_oper_status_change(self, uid, modes):\n        if uid in self.users:\n            u = self.users[uid]\n            if 'servprotect' in self.umodes and (self.umodes['servprotect'], None) in u.modes:\n                opertype = 'Network Service'\n            elif 'netadmin' in self.umodes and (self.umodes['netadmin'], None) in u.modes:\n                opertype = 'Network Administrator'\n            elif 'admin' in self.umodes and (self.umodes['admin'], None) in u.modes:\n                opertype = 'Server Administrator'\n            else:\n                opertype = 'IRC Operator'\n\n            if ('+o', None) in modes:\n                self.call_hooks([uid, 'CLIENT_OPERED', {'text': opertype}])\n\n    def handle_mode(self, source, command, args):\n        \"\"\"Handles mode changes.\"\"\"\n        # InspIRCd:\n        # <- :70MAAAAAA MODE 70MAAAAAA -i+xc\n\n        # P10:\n        # <- ABAAA M GL -w\n        # <- ABAAA M #test +v ABAAB 1460747615\n        # <- ABAAA OM #test +h ABAAA\n        target = self._get_UID(args[0])\n        if self.is_channel(target):\n            channeldata = self._channels[target].deepcopy()\n        else:\n            channeldata = None\n\n        modestrings = args[1:]\n        changedmodes = self.parse_modes(target, modestrings)\n        self.apply_modes(target, changedmodes)\n\n        if target in self.users:\n            # Target was a user. Check for any cloak and away status changes.\n            self._check_cloak_change(target)\n            self._check_umode_away_change(target)\n            self._check_oper_status_change(target, changedmodes)\n\n        return {'target': target, 'modes': changedmodes, 'channeldata': channeldata}\n\n    def handle_part(self, source, command, args):\n        \"\"\"Handles incoming PART commands.\"\"\"\n        channels = args[0].split(',')\n\n        for channel in channels.copy():\n            if channel not in self._channels or source not in self._channels[channel].users:\n                # Ignore channels the user isn't on, and remove them from any hook payloads.\n                channels.remove(channel)\n\n            self._channels[channel].remove_user(source)\n            try:\n                self.users[source].channels.discard(channel)\n            except KeyError:\n                log.debug(\"(%s) handle_part: KeyError trying to remove %r from %r's channel list?\", self.name, channel, source)\n\n            try:\n                reason = args[1]\n            except IndexError:\n                reason = ''\n\n        if channels:\n            return {'channels': channels, 'text': reason}\n\n    def handle_privmsg(self, source, command, args):\n        \"\"\"Handles incoming PRIVMSG\/NOTICE.\"\"\"\n        # TS6:\n        # <- :70MAAAAAA PRIVMSG #dev :afasfsa\n        # <- :70MAAAAAA NOTICE 0ALAAAAAA :afasfsa\n        # P10:\n        # <- ABAAA P AyAAA :privmsg text\n        # <- ABAAA O AyAAA :notice text\n        raw_target = args[0]\n        server_check = None\n        if '@' in raw_target and not self.is_channel(raw_target.lstrip(''.join(self.prefixmodes.values()))):\n            log.debug('(%s) Processing user@server message with target %s',\n                      self.name, raw_target)\n            raw_target, server_check = raw_target.split('@', 1)\n\n            if not self.is_server_name(server_check):\n                log.warning('(%s) Got user@server message with invalid server '\n                            'name %r (full target: %r)', self.name, server_check,\n                            args[0])\n                return\n\n        target = self._get_UID(raw_target)\n\n        if server_check is not None:\n            not_found = False\n            if target not in self.users:\n                # Most IRCds don't check locally if the target nick actually exists.\n                # If it doesn't, send an error back.\n                not_found = True\n            else:\n                # I guess we can technically leave this up to the IRCd to do the right\n                # checks here, but maybe that ruins the point of this 'security feature'\n                # in the first place.\n                log.debug('(%s) Checking if target %s\/%s exists on server %s',\n                          self.name, target, raw_target, server_check)\n                sid = self._get_SID(server_check)\n\n                if not sid:\n                    log.debug('(%s) Failed user@server server check: %s does not exist.',\n                              self.name, server_check)\n                    not_found = True\n                elif sid != self.get_server(target):\n                    log.debug(\"(%s) Got user@server message for %s\/%s, but they \"\n                              \"aren't on the server %s\/%s. (full target: %r)\",\n                              self.name, target, raw_target, server_check, sid,\n                              args[0])\n                    not_found = True\n\n            if not_found:\n                self.numeric(self.sid, 401, source, '%s :No such nick' %\n                             args[0])\n                return\n\n        # Coerse =#channel from Charybdis op moderated +z to @#channel.\n        if target.startswith('='):\n            target = '@' + target[1:]\n\n        return {'target': target, 'text': args[1]}\n\n    handle_notice = handle_privmsg\n\n    def handle_quit(self, numeric, command, args):\n        \"\"\"Handles incoming QUIT commands.\"\"\"\n        # TS6:\n        # <- :1SRAAGB4T QUIT :Quit: quit message goes here\n        # P10:\n        # <- ABAAB Q :Killed (GL_ (bangbang))\n        userdata = self._remove_client(numeric)\n        if userdata:\n            try:\n                reason = args[0]\n            except IndexError:\n                reason = ''\n            return {'text': reason, 'userdata': userdata}\n\n    def handle_stats(self, numeric, command, args):\n        \"\"\"Handles the IRC STATS command.\"\"\"\n        # IRCds are mostly consistent with this syntax, with the caller being the source,\n        # the stats type as arg 0, and the target server (SID or hostname) as arg 1\n        # <- :42XAAAAAB STATS c :7PY\n        return {'stats_type': args[0], 'target': self._get_SID(args[1])}\n\n    def handle_topic(self, numeric, command, args):\n        \"\"\"Handles incoming TOPIC changes from clients.\"\"\"\n        # <- :70MAAAAAA TOPIC #test :test\n        channel = args[0]\n        topic = args[1]\n\n        oldtopic = self._channels[channel].topic\n        self._channels[channel].topic = topic\n        self._channels[channel].topicset = True\n\n        return {'channel': channel, 'setter': numeric, 'text': topic,\n                'oldtopic': oldtopic}\n\n    def handle_time(self, numeric, command, args):\n        \"\"\"Handles incoming \/TIME requests.\"\"\"\n        return {'target': args[0]}\n\n    def handle_whois(self, numeric, command, args):\n        \"\"\"Handles incoming WHOIS commands..\"\"\"\n        # TS6:\n        # <- :42XAAAAAB WHOIS 5PYAAAAAA :pylink-devel\n        # P10:\n        # <- ABAAA W Ay :PyLink-devel\n\n        # First argument is the server that should reply to the WHOIS request\n        # or the server hosting the UID given. We can safely assume that any\n        # WHOIS commands received are for us, since we don't host any real servers\n        # to route it to.\n\n        return {'target': self._get_UID(args[-1])}\n\n    def handle_version(self, numeric, command, args):\n        \"\"\"Handles requests for the PyLink server version.\"\"\"\n        return {}  # See coremods\/handlers.py for how this hook is used\n","license":"mpl-2.0","hash":9017971914250878804,"line_mean":40.7051630435,"line_max":280,"alpha_frac":0.5765759896,"autogenerated":false},
{"repo_name":"FePhyFoFum\/PyPHLAWD","path":"src\/find_good_clusters_for_concat_batch.py","copies":"1","size":"9142","content":"import sys\nimport os\nimport shlex\nimport subprocess\nimport check_for_programs\nfrom conf import DI\nfrom conf import relcut\nfrom conf import abscut\nfrom conf import abscutint\nfrom conf import usecython\nfrom conf import smallest_cluster\nfrom conf import cluster_prop\nfrom conf import py\nimport emoticons\nfrom clint.textui import colored\nif usecython:\n    import cnode as node\nelse:\n    import node\nimport argparse as ap\n\n\"\"\"\nsame as find_good_clusters_for_concat.py, except automated.\n- accepts default clusters, builds trees and trims tips, concatenates, renames\n\"\"\"\n\n\n\"\"\"\nthis assumes that you have already run \npost_process_cluster_info.py startdir\n\"\"\"\n\nmat_nms = []\nclusterind = {}\nkeepers = set()#node\n\ndef generate_argparser():\n    parser = ap.ArgumentParser(prog=\"find_good_clusters_for_concat_batch.py\",\n        formatter_class=ap.ArgumentDefaultsHelpFormatter)\n    parser = ap.ArgumentParser()\n    parser.add_argument(\"-d\", \"--dir\", type=str, required=True,\n        help=(\"Starting directory.\"))\n    parser.add_argument(\"-b\",\"--database\",type=str,required=True,\n        help=(\"Database with sequences.\"))\n    parser.add_argument(\"-i\", \"--includetrivial\", action=\"store_true\", required=False,\n        default=False, help=(\"Include trivial clusters (default = False)\"))\n    return parser\n\ndef record_info_table(infilecsv):\n    tf = open(infilecsv,\"r\")\n    mat = []\n    #transpose the matrix\n    first = True\n    for i in tf:\n        spls = i.strip().split(\",\")\n        if len(spls) == 1:\n            continue\n        if first == True:\n            first = False\n            count = 0\n            for j in spls[1:]:\n                clusterind[count] = j\n                mat.append(0)\n                mat_nms.append([])\n                count += 1\n        else:\n            count = 0\n            for j in spls[1:]:\n                if j == \"x\":\n                    mat[count] += 1\n                    mat_nms[count].append(spls[0])\n                count += 1\n    tf.close()\n\n# where default clusters are selected\n# picked by coverage (conditioned on minimum size of 3 (rooted) or 4 (unrooted)) and smallest size defined\n# if trivial is True, and=y cluster > cluster_prop is retained\ndef check_info_table(tree, trivial):\n    for i in tree.iternodes(order=\"POSTORDER\"):\n        if \"names\" not in i.data:\n            continue\n        if i.parent != None:\n            for j in clusterind:\n                inter = set(i.data[\"names\"]).intersection(set(mat_nms[j]))\n                pinter = set(i.parent.data[\"names\"]).intersection(set(mat_nms[j]))\n                if len(inter) > 0 and len(pinter) == len(inter) and len(inter) == len(mat_nms[j]):\n                    # whether the `and` below should be `or` (like for the root).\n                    if len(inter) \/ float(len(i.data[\"names\"])) > cluster_prop or len(inter) > smallest_cluster:\n                        if not trivial:\n                            if (len(inter) > 3):\n                                print(i.label,clusterind[j],len(inter),len(i.data[\"names\"]))\n                                keepers.add(clusterind[j].replace(\".fa\",\".aln\"))\n                        else:\n                            print(i.label,clusterind[j],len(inter),len(i.data[\"names\"]))\n                            keepers.add(clusterind[j].replace(\".fa\",\".aln\"))\n        else:\n            for j in clusterind: # root\n                inter = set(i.data[\"names\"]).intersection(set(mat_nms[j]))\n                if len(inter)\/float(len(i.data[\"names\"])) > cluster_prop or len(inter) > smallest_cluster:\n                    if not trivial: # default: require a phylogenetically minimum size of at least 4\n                        if (len(inter) > 3):\n                            print(clusterind[j],len(inter),len(i.data[\"names\"]))\n                            keepers.add(clusterind[j].replace(\".fa\",\".aln\"))\n                    else:\n                        print(clusterind[j],len(inter),len(i.data[\"names\"]))\n                        keepers.add(clusterind[j].replace(\".fa\",\".aln\"))\n                        \n    return\n\ndef make_trim_trees(alignments):\n    fasttreename = \"FastTree\"\n    if check_for_programs.which_program(\"FastTree\") == None:\n        if check_for_programs.which_program(\"fasttree\") != None:\n            fasttreename = \"fasttree\"\n        else:\n            print(colored.red(\"FastTree NOT IN PATH\"),colored.red(emoticons.get_ran_emot(\"sad\")))\n            sys.exit(1)\n    newalns = {}\n    for i in alignments:\n        print(\"making tree for\",i)\n        cmd = fasttreename+\" -nt -gtr \"+i+\" > \"+i.replace(\".aln\",\".tre\")+\" 2> \/dev\/null\"\n        os.system(cmd)\n        cmd = py+\" \"+DI+\"trim_tips.py \"+i.replace(\".aln\",\".tre\")+\" \"+str(relcut)+\" \"+str(abscut)\n        #print cmd\n        p = subprocess.Popen(cmd, shell=True,stderr = subprocess.PIPE,stdout = subprocess.PIPE)\n        outtre = p.stdout.read().strip()\n        outrem = p.stderr.read().strip()\n        removetax = set()\n        if len(outrem) > 0:\n            outrem = outrem.decode(\"utf-8\")\n            print(\"  removing\",len(str(outrem).split(\"\\n\")),\"tips\")\n            for j in str(outrem).split(\"\\n\"):\n                taxon = j.split(\" \")[1]\n                removetax.add(taxon)\n        cmd = py+\" \"+DI+\"trim_internal_edges.py \"+i.replace(\".aln\",\".tre\")+\" \"+str(abscutint)\n        #print cmd\n        p = subprocess.Popen(cmd, shell=True,stderr = subprocess.PIPE,stdout = subprocess.PIPE)\n        outtre = p.stdout.read().strip()\n        outrem = p.stderr.read().strip()\n        if len(outrem) > 0:\n            outrem = outrem.decode(\"utf-8\")\n            print(\"  removing\",len(str(outrem).split(\"\\n\")),\"tips\")\n            for j in str(outrem).split(\"\\n\"):\n                taxon = j.split(\" \")[1]\n                removetax.add(taxon)\n        if len(removetax) > 0:\n            cmd = \"pxrms -s \"+i+\" -n \"+\",\".join(list(removetax))+\" -o \"+i.replace(\".aln\",\".aln.ed\")\n            newalns[i] = i.replace(\".aln\",\".aln.ed\")\n            #print cmd\n            os.system(cmd)\n        \n    return newalns\n\nif __name__ == \"__main__\":\n    parser = generate_argparser()\n    if len(sys.argv[1:]) == 0:\n        sys.argv.append(\"-h\")\n    args = parser.parse_args(sys.argv[1:])\n    dbname = args.database\n    cld = args.dir\n    #take off the trailing slash if there is one\n    if cld[-1] == \"\/\":\n        cld = cld[0:-1]\n    \n    trivial = args.includetrivial\n    count = 0\n    tree = node.Node()\n    nodes = {}\n    firstnode = True\n    \n    #build a tree from the directory\n    for root, dirs, files in os.walk(cld, topdown = True):\n        if \"clusters\" in root:\n            continue\n        if \"clusters\" in dirs:\n            if firstnode == True:\n                tree.label = root.split(\"\/\")[-1]\n                firstnode = False\n                nodes[root.split(\"\/\")[-1]] = tree\n            nd = nodes[root.split(\"\/\")[-1]]\n            nd.data[\"dir\"] = root\n            nd.data[\"names\"] = set()\n            tf = open(root+\"\/\"+root.split(\"\/\")[-1]+\".table\",\"r\")\n            for i in tf:\n                spls = i.strip().split(\"\\t\")\n                nd.data[\"names\"].add(spls[4])\n            tf.close()\n            for j in dirs:\n                if \"clusters\" not in j:\n                    cnd = node.Node()\n                    cnd.label = j\n                    cnd.parent = nd\n                    nd.add_child(cnd)\n                    nodes[j] = cnd\n            count += 1\n\n    record_info_table(cld+\"\/info.csv\")\n    #print(tree.get_newick_repr()+\";\")\n    check_info_table(tree, trivial)\n    print(len(keepers))\n    \n    ## hard-coded stuff: rename default selected clusters, make trees, concat, constraint\n    # baseid\n    baseid = cld.split(\"_\")[-1]\n    # clusters to keep\n    keeps = [cld+\"\/clusters\/\"+i for i in keepers]\n    # make trees and trim tips\n    newalns = make_trim_trees(keeps)\n    if len(newalns) > 0:\n        for i in newalns:\n            keeps.remove(i)\n            keeps.append(newalns[i])\n    # table\n    tab = cld+\"\/\"+cld.split(\"\/\")[-1]+\".table\"\n    rtn = cld.split(\"\/\")[-1]\n    # change ids to NCBIids\n    cmd = py+\" \"+DI+\"change_id_to_ncbi_fasta_mult.py \"+tab+\" \"+ \" \".join(keeps)\n    os.system(cmd)\n    # concatenate and switch to uppercase\n    outaln = cld+\"\/\"+rtn+\"_outaln\"\n    cmd = \"pxcat -u -s \"+\" \".join([i+\".rn\" for i in keeps])+\" -o \"+outaln+\" -p \"+cld+\"\/\"+rtn+\"_outpart\"\n    os.system(cmd)\n    # make constraint tree\n    ctree = cld+\"\/\"+rtn+\"_outaln.constraint.tre\"\n    cmd = py+\" \"+DI+\"make_constraint_from_ncbialn.py \"+dbname+\" \"+baseid+\" \"+outaln+\" > \"+ctree\n    print(\"make constraint tree:\")\n    print(cmd)\n    os.system(cmd)\n    \n    # rename NCBIids to names\n    # tree\n    cmd = py+\" \"+DI+\"change_ncbi_to_name_tre.py -t \"+tab+\" -i \"+ctree+\" -o \"+cld+\"\/\"+rtn+\"_outaln.constraint.cn.tre\"\n    print(\"rename taxa in tree:\")\n    print(cmd)\n    os.system(cmd)\n    # seqs\n    cmd = py+\" \"+DI+\"change_ncbi_to_name_fasta.py -t \"+tab+\" -i \"+outaln+\" -o \"+outaln+\".rn\"\n    print(\"rename taxa in concatenated alignment:\")\n    print(cmd)\n    os.system(cmd)\n\n    print(\"line for get_min:\")\n    print(\"python3 \"+DI+\"get_min_overlap_multiple_seqs.py \"+cld+\"\/\"+rtn+\"_outaln.constraint.tre \"+\" \".join([i+\".rn\" for i in keeps]))\n    ","license":"gpl-2.0","hash":1841283860664791400,"line_mean":36.4713114754,"line_max":133,"alpha_frac":0.538612995,"autogenerated":false},
{"repo_name":"pablogonzalezalba\/a-language-of-ice-and-fire","path":"parser_rules.py","copies":"1","size":"1771","content":"# -*- coding: utf-8 -*-\nfrom lexer_rules import tokens\n\nfrom expressions import *\n\ndef p_start(subexpressions):\n    'start : vars_definition expressions END'\n    subexpressions[0] = Start(subexpressions[1], subexpressions[2])\n\ndef p_vars_definition_empty(subexpressions):\n    'vars_definition :'\n    pass\n\ndef p_vars_definition(subexpressions):\n    'vars_definition : var vars_definition'\n    subexpressions[0] = VarsDefinition(subexpressions[1], subexpressions[2])\n\ndef p_var(subexpressions):\n    'var : ID VAR_DEFINITION EQUAL NUMBER'\n    subexpressions[0] = Var(subexpressions[1], subexpressions[4])\n\ndef p_expressions_empty(subexpressions):\n    'expressions :'\n    pass\n\ndef p_expressions(subexpressions):\n    'expressions : expression expressions'\n    subexpressions[0] = Expressions(subexpressions[1], subexpressions[2])\n\ndef p_expression_number(subexpressions):\n    'expression : NUMBER'\n    subexpressions[0] = Value(subexpressions[1])\n\ndef p_expression_id(subexpressions):\n    'expression : ID'\n    subexpressions[0] = Value(subexpressions[1])\n\ndef p_expression_if(subexpressions):\n    'expression : IF LPAREN expression RPAREN LBRACE expression RBRACE'\n    subexpressions[0] = If(subexpressions[3], subexpressions[6])\n\ndef p_expression_if_else(subexpressions):\n    'expression : IF LPAREN expression RPAREN LBRACE expression RBRACE ELSE LBRACE expression RBRACE'\n    subexpressions[0] = IfElse(subexpressions[3], subexpressions[6], subexpressions[10])\n\ndef p_expression_print(subexpressions):\n    'expression : PRINT LPAREN expression RPAREN'\n    subexpressions[0] = Print(subexpressions[3])\n\ndef p_expression_equal(subexpressions):\n    'expression : expression DOUBLE_EQUAL expression'\n    subexpressions[0] = DoubleEqual(subexpressions[1], subexpressions[3])\n","license":"mit","hash":4459424264619142859,"line_mean":33.0576923077,"line_max":101,"alpha_frac":0.7436476567,"autogenerated":false},
{"repo_name":"MrSenko\/Kiwi","path":"tcms\/issuetracker\/__init__.py","copies":"2","size":"1232","content":"\"\"\"\n    Kiwi TCMS supports internal bug tracking functionality and\n    integration between bug tracker and the rest of the system.\n    This behavior is defined in :mod:`tcms.issuetracker.base`.\n    The integration interface provides default behavior which can be\n    overridden in subsequent implementations if desired.\n\n    The scope is also listed below:\n\n    - *1-click bug report* - by clicking a UI element inside TestExecution\n      Kiwi TCMS will try to automatically report a new bug in the selected\n      bug tracker. If this fails will fall back to opening a new browser\n      window to manually enter the required information. Fields will be\n      pre-filled with correct information when possible.\n\n    - *automatic bug update* - when linking existing bug reports to TestExecution\n      the bug report will be \"linked\" back to the TE. By default this is achieved\n      by adding a comment to the bug report.\n\n    - *show bug info* - on pages which display bugs the tester could\n      see more contextual information by hovering the mouse over the info\n      icon. A tooltip will appear. Default implementation is to display\n      OpenGraph Protocol data for that URL. Can be customized. Information\n      is cached.\n\"\"\"\n","license":"gpl-2.0","hash":-8466630436524272772,"line_mean":48.28,"line_max":81,"alpha_frac":0.737012987,"autogenerated":false},
{"repo_name":"uwcirg\/true_nth_usa_portal","path":"portal\/migrations\/versions\/e3923b19b6f5_.py","copies":"1","size":"5441","content":"from alembic import op\nimport sqlalchemy as sa\n\n\"\"\"empty message\n\nRevision ID: e3923b19b6f5\nRevises: eff963021768\nCreate Date: 2017-09-14 11:48:47.116937\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = 'e3923b19b6f5'\ndown_revision = 'eff963021768'\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    # We'll recreate the QBs after via site_persistence.  Drop\n    # now to avoid upgrade problems\n    op.execute(\n        \"UPDATE questionnaire_responses SET questionnaire_bank_id = null\")\n    op.execute(\"DELETE FROM questionnaire_bank_questionnaires\")\n    op.execute(\"DELETE FROM questionnaire_banks\")\n\n    op.create_table(\n        'questionnaire_bank_recurs',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column(\n            'questionnaire_bank_id',\n            sa.Integer(),\n            nullable=False),\n        sa.Column('recur_id', sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            ['questionnaire_bank_id'],\n            ['questionnaire_banks.id'],\n            ondelete='CASCADE'),\n        sa.ForeignKeyConstraint(\n            ['recur_id'],\n            ['recurs.id'],\n            ondelete='CASCADE'),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint(\n            'questionnaire_bank_id',\n            'recur_id',\n            name='_questionnaire_bank_recure')\n    )\n    op.drop_table('questionnaire_bank_questionnaire_recurs')\n    op.drop_column('questionnaire_bank_questionnaires', 'days_till_overdue')\n    op.drop_column('questionnaire_bank_questionnaires', 'days_till_due')\n    op.add_column(\n        'questionnaire_banks',\n        sa.Column(\n            'expired',\n            sa.Text(),\n            nullable=True))\n    op.add_column(\n        'questionnaire_banks',\n        sa.Column(\n            'overdue',\n            sa.Text(),\n            nullable=True))\n    op.add_column(\n        'questionnaire_banks',\n        sa.Column(\n            'start',\n            sa.Text(),\n            nullable=False))\n    op.add_column(\n        'recurs',\n        sa.Column(\n            'cycle_length',\n            sa.Text(),\n            nullable=False))\n    op.add_column('recurs', sa.Column('start', sa.Text(), nullable=False))\n    op.add_column('recurs', sa.Column('termination', sa.Text(), nullable=True))\n    op.drop_constraint(u'_unique_recur', 'recurs', type_='unique')\n    op.create_unique_constraint(\n        '_unique_recur', 'recurs', [\n            'start', 'cycle_length', 'termination'])\n    op.drop_column('recurs', 'days_till_termination')\n    op.drop_column('recurs', 'days_in_cycle')\n    op.drop_column('recurs', 'days_to_start')\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        'recurs',\n        sa.Column(\n            'days_to_start',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False))\n    op.add_column(\n        'recurs',\n        sa.Column(\n            'days_in_cycle',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False))\n    op.add_column(\n        'recurs',\n        sa.Column(\n            'days_till_termination',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=True))\n    op.drop_constraint('_unique_recur', 'recurs', type_='unique')\n    op.create_unique_constraint(\n        u'_unique_recur', 'recurs', [\n            'days_to_start', 'days_in_cycle', 'days_till_termination'])\n    op.drop_column('recurs', 'termination')\n    op.drop_column('recurs', 'start')\n    op.drop_column('recurs', 'cycle_length')\n    op.drop_column('questionnaire_banks', 'start')\n    op.drop_column('questionnaire_banks', 'overdue')\n    op.drop_column('questionnaire_banks', 'expired')\n    op.add_column(\n        'questionnaire_bank_questionnaires',\n        sa.Column(\n            'days_till_due',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False))\n    op.add_column(\n        'questionnaire_bank_questionnaires',\n        sa.Column(\n            'days_till_overdue',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False))\n    op.create_table(\n        'questionnaire_bank_questionnaire_recurs',\n        sa.Column('id', sa.INTEGER(), nullable=False),\n        sa.Column(\n            'questionnaire_bank_questionnaire_id',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False),\n        sa.Column(\n            'recur_id',\n            sa.INTEGER(),\n            autoincrement=False,\n            nullable=False),\n        sa.ForeignKeyConstraint(\n            ['questionnaire_bank_questionnaire_id'],\n            [u'questionnaire_bank_questionnaires.id'],\n            name=(u'questionnaire_bank_questionna_questionnaire'\n                  '_bank_questionn_fkey'),\n            ondelete=u'CASCADE'),\n        sa.ForeignKeyConstraint(\n            ['recur_id'],\n            [u'recurs.id'],\n            name=u'questionnaire_bank_questionnaire_recurs_recur_id_fkey',\n            ondelete=u'CASCADE'),\n        sa.PrimaryKeyConstraint(\n            'id', name=u'questionnaire_bank_questionnaire_recurs_pkey'),\n        sa.UniqueConstraint(\n            'questionnaire_bank_questionnaire_id',\n            'recur_id',\n            name=u'_questionnaire_bank_questionnaire_recure')\n    )\n    op.drop_table('questionnaire_bank_recurs')\n    # ### end Alembic commands ###\n","license":"bsd-3-clause","hash":-5634950795186704233,"line_mean":31.7771084337,"line_max":79,"alpha_frac":0.5682778901,"autogenerated":false},
{"repo_name":"PFCal-dev\/HGCanalysis","path":"scripts\/submitLocalHGCalProduction.py","copies":"1","size":"1827","content":"#!\/usr\/bin\/env python\n\nimport os,sys\nimport optparse\nimport commands\nimport time\n\n\nusage = 'usage: %prog [options]'\nparser = optparse.OptionParser(usage)\nparser.add_option('-q', '--queue'      ,    dest='queue'              , help='batch queue'                                        , default='')\nparser.add_option('-n', '--njobs'      ,    dest='njobs'              , help='number of jobs'                                     , default=1,  type=int)\nparser.add_option('-o', '--options'    ,    dest='options'            , help='script options'                                     , default='')\nparser.add_option('-s', '--script'     ,    dest='script'             , help='script to run (look under scripts\/)'                , default='generateEventsFromCfi.sh')\n(opt, args) = parser.parse_args()\n\n\n#prepare working directory\ncmsswBase=os.environ['CMSSW_BASE']\njobsDir=cmsswBase+'\/src\/FARM%s'%(time.time())\nos.system('mkdir -p %s'%jobsDir)\n\n#loop over the required number of jobs\nfor n in xrange(0,opt.njobs):\n\n    #create a wrapper for standalone cmssw job\n    scriptFile = open('%s\/runJob_%d.sh'%(jobsDir,n+1), 'w')\n    scriptFile.write('#!\/bin\/bash\\n')\n    scriptFile.write('cd %s\/src\\n'%cmsswBase)\n    scriptFile.write('eval `scram r -sh`\\n')\n    scriptFile.write('cd %s\\n'%jobsDir)\n    scriptFile.write('%s %s -j %d\\n'%(opt.script,opt.options,n+1))\n    scriptFile.close()\n    os.system('chmod u+rwx %s\/runJob_%d.sh'%(jobsDir,n+1))\n\n    #submit it to the batch or run it locally\n    if opt.queue=='':\n        print 'Job #%d will run locally'%(n+1)\n        os.system('%s\/runJob_%d.sh'%(jobsDir,n+1))\n    else:\n        print 'Job #%d will run remotely'%(n+1)\n        os.system(\"bsub -o \/tmp\/%s\/std_out -q %s -R \\\"swp>2000 && pool>30000\\\" -J HGCSIM%d \\'%s\/runJob_%d.sh\\'\"%(os.environ['USER'],opt.queue,n+1,jobsDir,n+1))\n    \n","license":"gpl-3.0","hash":568233471083365732,"line_mean":41.488372093,"line_max":167,"alpha_frac":0.5703338807,"autogenerated":false},
{"repo_name":"audetto\/andsoft","path":"python\/rai\/asi\/Replay.py","copies":"1","size":"3261","content":"import os\nimport urllib.parse\nimport datetime\nimport json\nimport re\n\nfrom asi import Utils\nfrom asi import Config\nfrom asi import Base\nfrom asi import RAIUrls\n\nchannels = {\"1\": \"RaiUno\", \"2\": \"RaiDue\", \"3\": \"RaiTre\", \"23\": \"RaiGulp\", \"31\": \"RaiCinque\", \"32\": \"RaiPremium\", \"38\": \"RaiYoyo\"}\n\n# we want to extract all the\n# h264_DIGIT\n# which are now used for bwidth selection for MP4\n\ndef extractH264Ext(value):\n    res = {}\n    reg = \"^h264_(\\d+)\"\n    for k in value:\n        m = re.match(reg, k)\n        url = value[k]\n        if m and url:\n            bwidth = int(m.group(1))\n            Utils.addH264Url(res, bwidth, url)\n\n    return res\n\n\ndef parseItem(grabber, channel, date, time, value, db):\n    name = value[\"t\"]\n    desc = value[\"d\"]\n    secs = value[\"l\"]\n\n    length = None\n\n    if secs != \"\":\n        length = datetime.timedelta(seconds = int(secs))\n\n    h264 = extractH264Ext(value)\n\n    # if the detailed h264 is not found, try with \"h264\"\n    if not h264:\n        single = value[\"h264\"]\n        Utils.addH264Url(h264, 0, single)\n\n    # sometimes RAI puts the same url for h264 and TS\n    # normally this is only a valid h264,\n    # so we skip it in TS\n\n    h264Urls = h264.values()\n\n    tablet = value[\"urlTablet\"]\n    if tablet in h264Urls:\n        tablet = None\n\n    smartPhone = value[\"urlSmartPhone\"]\n    if smartPhone in h264Urls:\n        smartPhone = None\n\n    pid = value[\"i\"]\n\n    if h264 or tablet or smartPhone:\n        pid = Utils.getNewPID(db, pid)\n        p = Program(grabber, channels[channel], date, time, pid, length, name, desc, h264, tablet, smartPhone)\n        Utils.addToDB(db, p)\n\n\ndef process(grabber, f, db):\n    o = json.load(f)\n\n    for k1, v1 in o.items():\n        if k1 == \"now\":\n            continue\n        if k1 == \"defaultBannerVars\":\n            continue\n\n        channel = k1\n\n        for date, v2 in v1.items():\n            for time, value in v2.items():\n                parseItem(grabber, channel, date, time, value, db)\n\n\ndef download(db, grabber, downType):\n    progress = Utils.getProgress()\n\n    today = datetime.date.today()\n\n    folder = Config.replayFolder\n\n    for x in range(1, 8):\n        day = today - datetime.timedelta(days = x)\n        strDate = day.strftime(\"_%Y_%m_%d\")\n\n        for channel in channels.values():\n            filename = channel + strDate + \".html\"\n            url = RAIUrls.replay + \"\/\" + filename\n            localName = os.path.join(folder, filename)\n\n            f = Utils.download(grabber, progress, url, localName, downType, \"utf-8\")\n\n            if f:\n                process(grabber, f, db)\n\n\nclass Program(Base.Base):\n    def __init__(self, grabber, channel, date, hour, pid, length, title, desc, h264, tablet, smartPhone):\n        super(Program, self).__init__()\n\n        self.pid = pid\n        self.title = title\n        self.h264 = h264\n        self.description = desc\n        self.channel = channel\n\n        if tablet:    # higher quality normally\n            self.ts = tablet\n        else:\n            self.ts = smartPhone\n\n        self.datetime = datetime.datetime.strptime(date + \" \" + hour, \"%Y-%m-%d %H:%M\")\n\n        self.grabber = grabber\n        self.length = length\n\n        name = Utils.makeFilename(self.title)\n        self.filename = self.pid + \"-\" + name\n","license":"gpl-3.0","hash":8302990769300752556,"line_mean":24.2790697674,"line_max":129,"alpha_frac":0.5789635081,"autogenerated":false},
{"repo_name":"Wyliodrin\/wyliodrin-app-server","path":"arduinoyun\/wyliodrin.py","copies":"1","size":"1623","content":"\nimport socket\nimport random\nimport time\n\nboard = None\npins = [None] * 20\n\nINPUT = 0\nOUTPUT = 1\nPWM = 2\nANALOG = 3\n\ndef startFirmata():\n  global board\n  if board == None:\n    import pyfirmata\n    board = pyfirmata.Arduino ('\/dev\/ttyATH0')\n    reader = pyfirmata.util.Iterator(board)\n    reader.start()\n  \n\nUDP_IP = \"127.0.0.1\"\nUDP_PORT = 7200\n\nsock = socket.socket(socket.AF_INET, # Internet\n                      socket.SOCK_DGRAM) # UDP\n\ndef sendSignal (signal, value):\n  sock.sendto(signal+\" \"+str(value)+\" \"+str(int(round(time.time()*1000))), (UDP_IP, UDP_PORT))\n\ndef delay(milliseconds):\n  time.sleep (milliseconds\/1000)\n\ndef pinMode (pin, mode):\n  startFirmata ()\n  global board\n  global pins\n  if pins[pin] == None:\n    if mode == INPUT:\n      pins[pin] = board.get_pin ('d:'+str(pin)+':i')\n    elif mode == OUTPUT:\n      pins[pin] = board.get_pin ('d:'+str(pin)+':o')\n    elif mode == ANALOG:\n      mcu_pin = pin - 14\n      pins[pin] = board.get_pin ('a:'+str(mcu_pin)+':i')\n    elif mode == PWM:\n      pins[pin] = board.get_pin ('d:'+str(pin)+':p')\n    \n\ndef digitalWrite (pin, value):\n  startFirmata ()\n  pinMode (pin, OUTPUT)\n  pins[pin].write (value)\n\ndef digitalRead (pin):\n  startFirmata ()\n  pinMode (pin, INPUT)\n  value = pins[pin].read ()\n  if value == None: value = 0\n  # print (value)\n  return value\n\ndef analogWrite (pin, value):\n  startFirmata ()\n  pinMode (pin, PWM)\n  pins[pin].write (value)\n\ndef analogRead (pin):\n  startFirmata ()\n  if pin <= 13: pin = pin + 14\n  pinMode (pin, ANALOG)\n  value = pins[pin].read ()\n  # print (value)\n  if value == None: value = 0\n  return int(round(value * 1023))\n\n\n","license":"gpl-3.0","hash":6475490464540021376,"line_mean":19.8076923077,"line_max":94,"alpha_frac":0.6118299445,"autogenerated":false},
{"repo_name":"rfguri\/vimfiles","path":"bundle\/ycm\/third_party\/ycmd\/ycmd\/utils.py","copies":"1","size":"14629","content":"# encoding: utf-8\n#\n# Copyright (C) 2011, 2012 Google Inc.\n#\n# This file is part of ycmd.\n#\n# ycmd is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# ycmd is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with ycmd.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom future import standard_library\nstandard_library.install_aliases()\nfrom builtins import *  # noqa\nfrom future.utils import PY2, native\n\nimport os\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport tempfile\nimport time\n\n\n# Creation flag to disable creating a console window on Windows. See\n# https:\/\/msdn.microsoft.com\/en-us\/library\/windows\/desktop\/ms684863.aspx\nCREATE_NO_WINDOW = 0x08000000\n\n# Don't use this! Call PathToCreatedTempDir() instead. This exists for the sake\n# of tests.\nRAW_PATH_TO_TEMP_DIR = os.path.join( tempfile.gettempdir(), 'ycm_temp' )\n\n# Readable, writable and executable by everyone.\nACCESSIBLE_TO_ALL_MASK = ( stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH |\n                           stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )\n\nEXECUTABLE_FILE_MASK = os.F_OK | os.X_OK\n\n\n# Python 3 complains on the common open(path).read() idiom because the file\n# doesn't get closed. So, a helper func.\n# Also, all files we read are UTF-8.\ndef ReadFile( filepath ):\n  with open( filepath, encoding = 'utf8' ) as f:\n    return f.read()\n\n\n# Returns a file object that can be used to replace sys.stdout or sys.stderr\ndef OpenForStdHandle( filepath ):\n  # Need to open the file in binary mode on py2 because of bytes vs unicode.\n  # If we open in text mode (default), then third-party code that uses `print`\n  # (we're replacing sys.stdout!) with an `str` object on py2 will cause\n  # tracebacks because text mode insists on unicode objects. (Don't forget,\n  # `open` is actually `io.open` because of future builtins.)\n  # Since this function is used for logging purposes, we don't want the output\n  # to be delayed. This means no buffering for binary mode and line buffering\n  # for text mode. See https:\/\/docs.python.org\/2\/library\/io.html#io.open\n  if PY2:\n    return open( filepath, mode = 'wb', buffering = 0 )\n  return open( filepath, mode = 'w', buffering = 1 )\n\n\n# Given an object, returns a str object that's utf-8 encoded. This is meant to\n# be used exclusively when producing strings to be passed to the C++ Python\n# plugins. For other code, you likely want to use ToBytes below.\ndef ToCppStringCompatible( value ):\n  if isinstance( value, str ):\n    return native( value.encode( 'utf8' ) )\n  if isinstance( value, bytes ):\n    return native( value )\n  return native( str( value ).encode( 'utf8' ) )\n\n\n# Returns a unicode type; either the new python-future str type or the real\n# unicode type. The difference shouldn't matter.\ndef ToUnicode( value ):\n  if not value:\n    return str()\n  if isinstance( value, str ):\n    return value\n  if isinstance( value, bytes ):\n    # All incoming text should be utf8\n    return str( value, 'utf8' )\n  return str( value )\n\n\n# Consistently returns the new bytes() type from python-future. Assumes incoming\n# strings are either UTF-8 or unicode (which is converted to UTF-8).\ndef ToBytes( value ):\n  if not value:\n    return bytes()\n\n  # This is tricky. On py2, the bytes type from builtins (from python-future) is\n  # a subclass of str. So all of the following are true:\n  #   isinstance(str(), bytes)\n  #   isinstance(bytes(), str)\n  # But they don't behave the same in one important aspect: iterating over a\n  # bytes instance yields ints, while iterating over a (raw, py2) str yields\n  # chars. We want consistent behavior so we force the use of bytes().\n  if type( value ) == bytes:\n    return value\n\n  # This is meant to catch Python 2's native str type.\n  if isinstance( value, bytes ):\n    return bytes( value, encoding = 'utf8' )\n\n  if isinstance( value, str ):\n    # On py2, with `from builtins import *` imported, the following is true:\n    #\n    #   bytes(str(u'abc'), 'utf8') == b\"b'abc'\"\n    #\n    # Obviously this is a bug in python-future. So we work around it. Also filed\n    # upstream at: https:\/\/github.com\/PythonCharmers\/python-future\/issues\/193\n    # We can't just return value.encode( 'utf8' ) on both py2 & py3 because on\n    # py2 that *sometimes* returns the built-in str type instead of the newbytes\n    # type from python-future.\n    if PY2:\n      return bytes( value.encode( 'utf8' ), encoding = 'utf8' )\n    else:\n      return bytes( value, encoding = 'utf8' )\n\n  # This is meant to catch `int` and similar non-string\/bytes types.\n  return ToBytes( str( value ) )\n\n\ndef ByteOffsetToCodepointOffset( line_value, byte_offset ):\n  \"\"\"The API calls for byte offsets into the UTF-8 encoded version of the\n  buffer. However, ycmd internally uses unicode strings. This means that\n  when we need to walk 'characters' within the buffer, such as when checking\n  for semantic triggers and similar, we must use codepoint offets, rather than\n  byte offsets.\n\n  This method converts the |byte_offset|, which is a utf-8 byte offset, into\n  a codepoint offset in the unicode string |line_value|.\"\"\"\n\n  byte_line_value = ToBytes( line_value )\n  return len( ToUnicode( byte_line_value[ : byte_offset - 1 ] ) ) + 1\n\n\ndef CodepointOffsetToByteOffset( unicode_line_value, codepoint_offset ):\n  \"\"\"The API calls for byte offsets into the UTF-8 encoded version of the\n  buffer. However, ycmd internally uses unicode strings. This means that\n  when we need to walk 'characters' within the buffer, such as when checking\n  for semantic triggers and similar, we must use codepoint offets, rather than\n  byte offsets.\n\n  This method converts the |codepoint_offset| which is a unicode codepoint\n  offset into an byte offset into the utf-8 encoded bytes version of\n  |unicode_line_value|.\"\"\"\n\n  # Should be a no-op, but in case someone passes a bytes instance.\n  unicode_line_value = ToUnicode( unicode_line_value )\n  return len( ToBytes( unicode_line_value[ : codepoint_offset - 1 ] ) ) + 1\n\n\ndef PathToCreatedTempDir( tempdir = RAW_PATH_TO_TEMP_DIR ):\n  try:\n    os.makedirs( tempdir )\n    # Needed to support multiple users working on the same machine;\n    # see issue 606.\n    MakeFolderAccessibleToAll( tempdir )\n  except OSError:\n    # Folder already exists, skip folder creation.\n    pass\n  return tempdir\n\n\ndef MakeFolderAccessibleToAll( path_to_folder ):\n  current_stat = os.stat( path_to_folder )\n  flags = current_stat.st_mode | ACCESSIBLE_TO_ALL_MASK\n  os.chmod( path_to_folder, flags )\n\n\ndef GetUnusedLocalhostPort():\n  sock = socket.socket()\n  # This tells the OS to give us any free port in the range [1024 - 65535]\n  sock.bind( ( '', 0 ) )\n  port = sock.getsockname()[ 1 ]\n  sock.close()\n  return port\n\n\ndef RemoveIfExists( filename ):\n  try:\n    os.remove( filename )\n  except OSError:\n    pass\n\n\ndef PathToFirstExistingExecutable( executable_name_list ):\n  for executable_name in executable_name_list:\n    path = FindExecutable( executable_name )\n    if path:\n      return path\n  return None\n\n\ndef _GetWindowsExecutable( filename ):\n  def _GetPossibleWindowsExecutable( filename ):\n    pathext = [ ext.lower() for ext in\n                os.environ.get( 'PATHEXT', '' ).split( os.pathsep ) ]\n    base, extension = os.path.splitext( filename )\n    if extension.lower() in pathext:\n      return [ filename ]\n    else:\n      return [ base + ext for ext in pathext ]\n\n  for exe in _GetPossibleWindowsExecutable( filename ):\n    if os.path.isfile( exe ):\n      return exe\n  return None\n\n\n# Check that a given file can be accessed as an executable file, so controlling\n# the access mask on Unix and if has a valid extension on Windows. It returns\n# the path to the executable or None if no executable was found.\ndef GetExecutable( filename ):\n  if OnWindows():\n    return _GetWindowsExecutable( filename )\n\n  if ( os.path.isfile( filename )\n       and os.access( filename, EXECUTABLE_FILE_MASK ) ):\n    return filename\n  return None\n\n\n# Adapted from https:\/\/hg.python.org\/cpython\/file\/3.5\/Lib\/shutil.py#l1081\n# to be backward compatible with Python2 and more consistent to our codebase.\ndef FindExecutable( executable ):\n  # If we're given a path with a directory part, look it up directly rather\n  # than referring to PATH directories. This includes checking relative to the\n  # current directory, e.g. .\/script\n  if os.path.dirname( executable ):\n    return GetExecutable( executable )\n\n  paths = os.environ[ 'PATH' ].split( os.pathsep )\n\n  if OnWindows():\n    # The current directory takes precedence on Windows.\n    curdir = os.path.abspath( os.curdir )\n    if curdir not in paths:\n      paths.insert( 0, curdir )\n\n  for path in paths:\n    exe = GetExecutable( os.path.join( path, executable ) )\n    if exe:\n      return exe\n  return None\n\n\ndef ExecutableName( executable ):\n  return executable + ( '.exe' if OnWindows() else '' )\n\n\ndef OnWindows():\n  return sys.platform == 'win32'\n\n\ndef OnCygwin():\n  return sys.platform == 'cygwin'\n\n\ndef OnMac():\n  return sys.platform == 'darwin'\n\n\ndef ProcessIsRunning( handle ):\n  return handle is not None and handle.poll() is None\n\n\ndef WaitUntilProcessIsTerminated( handle, timeout = 5 ):\n  expiration = time.time() + timeout\n  while True:\n    if time.time() > expiration:\n      raise RuntimeError( 'Waited process to terminate for {0} seconds, '\n                          'aborting.'.format( timeout ) )\n    if not ProcessIsRunning( handle ):\n      return\n    time.sleep( 0.1 )\n\n\ndef PathsToAllParentFolders( path ):\n  folder = os.path.normpath( path )\n  if os.path.isdir( folder ):\n    yield folder\n  while True:\n    parent = os.path.dirname( folder )\n    if parent == folder:\n      break\n    folder = parent\n    yield folder\n\n\ndef ForceSemanticCompletion( request_data ):\n  return ( 'force_semantic' in request_data and\n           bool( request_data[ 'force_semantic' ] ) )\n\n\n# A wrapper for subprocess.Popen that fixes quirks on Windows.\ndef SafePopen( args, **kwargs ):\n  if OnWindows():\n    # We need this to start the server otherwise bad things happen.\n    # See issue #637.\n    if kwargs.get( 'stdin_windows' ) is subprocess.PIPE:\n      kwargs[ 'stdin' ] = subprocess.PIPE\n    # Do not create a console window\n    kwargs[ 'creationflags' ] = CREATE_NO_WINDOW\n    # Python 2 fails to spawn a process from a command containing unicode\n    # characters on Windows.  See https:\/\/bugs.python.org\/issue19264 and\n    # http:\/\/bugs.python.org\/issue1759845.\n    # Since paths are likely to contains such characters, we convert them to\n    # short ones to obtain paths with only ascii characters.\n    if PY2:\n      args = ConvertArgsToShortPath( args )\n\n  kwargs.pop( 'stdin_windows', None )\n  return subprocess.Popen( args, **kwargs )\n\n\n# We need to convert environment variables to native strings on Windows and\n# Python 2 to prevent a TypeError when passing them to a subprocess.\ndef SetEnviron( environ, variable, value ):\n  if OnWindows() and PY2:\n    environ[ native( ToBytes( variable ) ) ] = native( ToBytes( value ) )\n  else:\n    environ[ variable ] = value\n\n\n# Convert paths in arguments command to short path ones\ndef ConvertArgsToShortPath( args ):\n  def ConvertIfPath( arg ):\n    if os.path.exists( arg ):\n      return GetShortPathName( arg )\n    return arg\n\n  if isinstance( args, str ) or isinstance( args, bytes ):\n    return ConvertIfPath( args )\n  return [ ConvertIfPath( arg ) for arg in args ]\n\n\n# Get the Windows short path name.\n# Based on http:\/\/stackoverflow.com\/a\/23598461\/200291\ndef GetShortPathName( path ):\n  if not OnWindows():\n    return path\n\n  from ctypes import windll, wintypes, create_unicode_buffer\n\n  # Set the GetShortPathNameW prototype\n  _GetShortPathNameW = windll.kernel32.GetShortPathNameW\n  _GetShortPathNameW.argtypes = [ wintypes.LPCWSTR,\n                                  wintypes.LPWSTR,\n                                  wintypes.DWORD]\n  _GetShortPathNameW.restype = wintypes.DWORD\n\n  output_buf_size = 0\n\n  while True:\n    output_buf = create_unicode_buffer( output_buf_size )\n    needed = _GetShortPathNameW( path, output_buf, output_buf_size )\n    if output_buf_size >= needed:\n      return output_buf.value\n    else:\n      output_buf_size = needed\n\n\n# Shim for imp.load_source so that it works on both Py2 & Py3. See upstream\n# Python docs for info on what this does.\ndef LoadPythonSource( name, pathname ):\n  if PY2:\n    import imp\n    return imp.load_source( name, pathname )\n  else:\n    import importlib\n    return importlib.machinery.SourceFileLoader( name, pathname ).load_module()\n\n\ndef SplitLines( contents ):\n  \"\"\"Return a list of each of the lines in the unicode string |contents|.\n  Behaviour is equivalent to str.splitlines with the following exceptions:\n    - empty strings are returned as [ '' ]\n    - a trailing newline is not ignored (i.e. SplitLines( '\\n' )\n      returns [ '', '' ], not [ '' ]\"\"\"\n\n  # We often want to get a list representation of a buffer such that we can\n  # index all of the 'lines' within it. Python provides str.splitlines for this\n  # purpose, but its documented behaviors for empty strings and strings ending\n  # with a newline character are not compatible with this. As a result, we write\n  # our own wrapper to provide a splitlines implementation which returns the\n  # actual list of indexable lines in a buffer, where a line may have 0\n  # characters.\n  #\n  # NOTE: str.split( '\\n' ) actually gives this behaviour, except it does not\n  # work when running on a unix-like system and reading a file with Windows line\n  # endings.\n\n  # ''.splitlines() returns [], but we want [ '' ]\n  if contents == '':\n    return [ '' ]\n\n  lines = contents.splitlines()\n\n  # '\\n'.splitlines() returns [ '' ]. We want [ '', '' ].\n  # '\\n\\n\\n'.splitlines() returns [ '', '', '' ]. We want [ '', '', '', '' ].\n  #\n  # So we re-instate the empty entry at the end if the original string ends\n  # with a newline. Universal newlines recognise the following as\n  # line-terminators:\n  #   - '\\n'\n  #   - '\\r\\n'\n  #   - '\\r'\n  #\n  # Importantly, notice that \\r\\n also ends with \\n\n  #\n  if contents.endswith( '\\r' ) or contents.endswith( '\\n' ):\n    lines.append( '' )\n\n  return lines\n","license":"mit","hash":546686302704283199,"line_mean":32.9419953596,"line_max":80,"alpha_frac":0.6943741883,"autogenerated":false},
{"repo_name":"jwallen\/ChemPy","path":"chempy\/constants.py","copies":"1","size":"2142","content":"#!\/usr\/bin\/python\n# -*- coding: utf-8 -*-\n\n################################################################################\n#\n#   ChemPy - A chemistry toolkit for Python\n#\n#   Copyright (c) 2010 by Joshua W. Allen (jwallen@mit.edu)\n#\n#   Permission is hereby granted, free of charge, to any person obtaining a\n#   copy of this software and associated documentation files (the 'Software'),\n#   to deal in the Software without restriction, including without limitation\n#   the rights to use, copy, modify, merge, publish, distribute, sublicense,\n#   and\/or sell copies of the Software, and to permit persons to whom the\n#   Software is furnished to do so, subject to the following conditions:\n#\n#   The above copyright notice and this permission notice shall be included in\n#   all copies or substantial portions of the Software.\n#\n#   THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#   FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n#   DEALINGS IN THE SOFTWARE.\n#\n################################################################################\n\n\"\"\"\nThis module contains a number of physical constants to be made available\nthroughout ChemPy. ChemPy uses SI units throughout; accordingly, all of the\nconstants in this module are stored in combinations of meters, seconds,\nkilograms, moles, etc.\n\nThe constants available are listed below. All values were taken from\n`NIST <http:\/\/physics.nist.gov\/cuu\/Constants\/index.html>`_\n\n\"\"\"\n\nimport math\nimport cython\n\n################################################################################\n\n#: The Avogadro constant\nNa = 6.02214179e23\n\n#: The Boltzmann constant\nkB = 1.3806504e-23\n\n#: The gas law constant\nR = 8.314472\n\n#: The Planck constant\nh = 6.62606896e-34\n\n#: The speed of light in a vacuum\nc = 299792458\n\n#: pi\npi = float(math.pi)\n","license":"mit","hash":8506595939763616437,"line_mean":33.5483870968,"line_max":80,"alpha_frac":0.6591970121,"autogenerated":false},
{"repo_name":"hackers-terabit\/portage","path":"pym\/_emerge\/actions.py","copies":"1","size":"106632","content":"# Copyright 1999-2016 Gentoo Foundation\n# Distributed under the terms of the GNU General Public License v2\n\nfrom __future__ import division, print_function, unicode_literals\n\nimport errno\nimport logging\nimport operator\nimport platform\nimport pwd\nimport random\nimport re\nimport signal\nimport socket\nimport stat\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nimport time\nimport warnings\nfrom itertools import chain\n\nimport portage\nportage.proxy.lazyimport.lazyimport(globals(),\n\t'portage.dbapi._similar_name_search:similar_name_search',\n\t'portage.debug',\n\t'portage.news:count_unread_news,display_news_notifications',\n\t'portage.util._get_vm_info:get_vm_info',\n\t'portage.util.locale:check_locale',\n\t'portage.emaint.modules.sync.sync:SyncRepos',\n\t'_emerge.chk_updated_cfg_files:chk_updated_cfg_files',\n\t'_emerge.help:help@emerge_help',\n\t'_emerge.post_emerge:display_news_notification,post_emerge',\n\t'_emerge.stdout_spinner:stdout_spinner',\n)\n\nfrom portage import os\nfrom portage import shutil\nfrom portage import eapi_is_supported, _encodings, _unicode_decode\nfrom portage.cache.cache_errors import CacheError\nfrom portage.const import GLOBAL_CONFIG_PATH, VCS_DIRS, _DEPCLEAN_LIB_CHECK_DEFAULT\nfrom portage.const import SUPPORTED_BINPKG_FORMATS, TIMESTAMP_FORMAT\nfrom portage.dbapi.dep_expand import dep_expand\nfrom portage.dbapi._expand_new_virt import expand_new_virt\nfrom portage.dbapi.IndexedPortdb import IndexedPortdb\nfrom portage.dbapi.IndexedVardb import IndexedVardb\nfrom portage.dep import Atom, _repo_separator, _slot_separator\nfrom portage.eclass_cache import hashed_path\nfrom portage.exception import InvalidAtom, InvalidData, ParseError\nfrom portage.output import blue, colorize, create_color_func, darkgreen, \\\n\tred, xtermTitle, xtermTitleReset, yellow\ngood = create_color_func(\"GOOD\")\nbad = create_color_func(\"BAD\")\nwarn = create_color_func(\"WARN\")\nfrom portage.package.ebuild._ipc.QueryCommand import QueryCommand\nfrom portage.package.ebuild.doebuild import _check_temp_dir\nfrom portage._sets import load_default_config, SETPREFIX\nfrom portage._sets.base import InternalPackageSet\nfrom portage.util import cmp_sort_key, writemsg, varexpand, \\\n\twritemsg_level, writemsg_stdout\nfrom portage.util.digraph import digraph\nfrom portage.util.SlotObject import SlotObject\nfrom portage.util._async.run_main_scheduler import run_main_scheduler\nfrom portage.util._async.SchedulerInterface import SchedulerInterface\nfrom portage.util._eventloop.global_event_loop import global_event_loop\nfrom portage._global_updates import _global_updates\nfrom portage.sync.old_tree_timestamp import old_tree_timestamp_warn\nfrom portage.localization import _\nfrom portage.metadata import action_metadata\n\nfrom _emerge.clear_caches import clear_caches\nfrom _emerge.countdown import countdown\nfrom _emerge.create_depgraph_params import create_depgraph_params\nfrom _emerge.Dependency import Dependency\nfrom _emerge.depgraph import backtrack_depgraph, depgraph, resume_depgraph\nfrom _emerge.DepPrioritySatisfiedRange import DepPrioritySatisfiedRange\nfrom _emerge.emergelog import emergelog\nfrom _emerge.is_valid_package_atom import is_valid_package_atom\nfrom _emerge.MetadataRegen import MetadataRegen\nfrom _emerge.Package import Package\nfrom _emerge.ProgressHandler import ProgressHandler\nfrom _emerge.RootConfig import RootConfig\nfrom _emerge.Scheduler import Scheduler\nfrom _emerge.search import search\nfrom _emerge.SetArg import SetArg\nfrom _emerge.show_invalid_depstring_notice import show_invalid_depstring_notice\nfrom _emerge.unmerge import unmerge\nfrom _emerge.UnmergeDepPriority import UnmergeDepPriority\nfrom _emerge.UseFlagDisplay import pkg_use_display\nfrom _emerge.UserQuery import UserQuery\n\nif sys.hexversion >= 0x3000000:\n\tlong = int\n\t_unicode = str\nelse:\n\t_unicode = unicode\n\ndef action_build(emerge_config, trees=DeprecationWarning,\n\tmtimedb=DeprecationWarning, myopts=DeprecationWarning,\n\tmyaction=DeprecationWarning, myfiles=DeprecationWarning, spinner=None):\n\n\tif not isinstance(emerge_config, _emerge_config):\n\t\twarnings.warn(\"_emerge.actions.action_build() now expects \"\n\t\t\t\"an _emerge_config instance as the first parameter\",\n\t\t\tDeprecationWarning, stacklevel=2)\n\t\temerge_config = load_emerge_config(\n\t\t\taction=myaction, args=myfiles, trees=trees, opts=myopts)\n\t\tadjust_configs(emerge_config.opts, emerge_config.trees)\n\n\tsettings, trees, mtimedb = emerge_config\n\tmyopts = emerge_config.opts\n\tmyaction = emerge_config.action\n\tmyfiles = emerge_config.args\n\n\tif '--usepkgonly' not in myopts:\n\t\told_tree_timestamp_warn(settings['PORTDIR'], settings)\n\n\t# It's best for config updates in \/etc\/portage to be processed\n\t# before we get here, so warn if they're not (bug #267103).\n\tchk_updated_cfg_files(settings['EROOT'], ['\/etc\/portage'])\n\n\t# validate the state of the resume data\n\t# so that we can make assumptions later.\n\tfor k in (\"resume\", \"resume_backup\"):\n\t\tif k not in mtimedb:\n\t\t\tcontinue\n\t\tresume_data = mtimedb[k]\n\t\tif not isinstance(resume_data, dict):\n\t\t\tdel mtimedb[k]\n\t\t\tcontinue\n\t\tmergelist = resume_data.get(\"mergelist\")\n\t\tif not isinstance(mergelist, list):\n\t\t\tdel mtimedb[k]\n\t\t\tcontinue\n\t\tfor x in mergelist:\n\t\t\tif not (isinstance(x, list) and len(x) == 4):\n\t\t\t\tcontinue\n\t\t\tpkg_type, pkg_root, pkg_key, pkg_action = x\n\t\t\tif pkg_root not in trees:\n\t\t\t\t# Current $ROOT setting differs,\n\t\t\t\t# so the list must be stale.\n\t\t\t\tmergelist = None\n\t\t\t\tbreak\n\t\tif not mergelist:\n\t\t\tdel mtimedb[k]\n\t\t\tcontinue\n\t\tresume_opts = resume_data.get(\"myopts\")\n\t\tif not isinstance(resume_opts, (dict, list)):\n\t\t\tdel mtimedb[k]\n\t\t\tcontinue\n\t\tfavorites = resume_data.get(\"favorites\")\n\t\tif not isinstance(favorites, list):\n\t\t\tdel mtimedb[k]\n\t\t\tcontinue\n\n\tresume = False\n\tif \"--resume\" in myopts and \\\n\t\t(\"resume\" in mtimedb or\n\t\t\"resume_backup\" in mtimedb):\n\t\tresume = True\n\t\tif \"resume\" not in mtimedb:\n\t\t\tmtimedb[\"resume\"] = mtimedb[\"resume_backup\"]\n\t\t\tdel mtimedb[\"resume_backup\"]\n\t\t\tmtimedb.commit()\n\t\t# \"myopts\" is a list for backward compatibility.\n\t\tresume_opts = mtimedb[\"resume\"].get(\"myopts\", [])\n\t\tif isinstance(resume_opts, list):\n\t\t\tresume_opts = dict((k,True) for k in resume_opts)\n\t\tfor opt in (\"--ask\", \"--color\", \"--skipfirst\", \"--tree\"):\n\t\t\tresume_opts.pop(opt, None)\n\n\t\t# Current options always override resume_opts.\n\t\tresume_opts.update(myopts)\n\t\tmyopts.clear()\n\t\tmyopts.update(resume_opts)\n\n\t\tif \"--debug\" in myopts:\n\t\t\twritemsg_level(\"myopts %s\\n\" % (myopts,))\n\n\t\t# Adjust config according to options of the command being resumed.\n\t\tfor myroot in trees:\n\t\t\tmysettings =  trees[myroot][\"vartree\"].settings\n\t\t\tmysettings.unlock()\n\t\t\tadjust_config(myopts, mysettings)\n\t\t\tmysettings.lock()\n\t\t\tdel myroot, mysettings\n\n\tldpath_mtimes = mtimedb[\"ldpath\"]\n\tfavorites=[]\n\tbuildpkgonly = \"--buildpkgonly\" in myopts\n\tpretend = \"--pretend\" in myopts\n\tfetchonly = \"--fetchonly\" in myopts or \"--fetch-all-uri\" in myopts\n\task = \"--ask\" in myopts\n\tenter_invalid = '--ask-enter-invalid' in myopts\n\tnodeps = \"--nodeps\" in myopts\n\toneshot = \"--oneshot\" in myopts or \"--onlydeps\" in myopts\n\ttree = \"--tree\" in myopts\n\tif nodeps and tree:\n\t\ttree = False\n\t\tdel myopts[\"--tree\"]\n\t\tportage.writemsg(colorize(\"WARN\", \" * \") + \\\n\t\t\t\"--tree is broken with --nodeps. Disabling...\\n\")\n\tdebug = \"--debug\" in myopts\n\tverbose = \"--verbose\" in myopts\n\tquiet = \"--quiet\" in myopts\n\tmyparams = create_depgraph_params(myopts, myaction)\n\tmergelist_shown = False\n\n\tif pretend or fetchonly:\n\t\t# make the mtimedb readonly\n\t\tmtimedb.filename = None\n\tif '--digest' in myopts or 'digest' in settings.features:\n\t\tif '--digest' in myopts:\n\t\t\tmsg = \"The --digest option\"\n\t\telse:\n\t\t\tmsg = \"The FEATURES=digest setting\"\n\n\t\tmsg += \" can prevent corruption from being\" + \\\n\t\t\t\" noticed. The `repoman manifest` command is the preferred\" + \\\n\t\t\t\" way to generate manifests and it is capable of doing an\" + \\\n\t\t\t\" entire repository or category at once.\"\n\t\tprefix = bad(\" * \")\n\t\twritemsg(prefix + \"\\n\")\n\t\tfor line in textwrap.wrap(msg, 72):\n\t\t\twritemsg(\"%s%s\\n\" % (prefix, line))\n\t\twritemsg(prefix + \"\\n\")\n\n\tif resume:\n\t\tfavorites = mtimedb[\"resume\"].get(\"favorites\")\n\t\tif not isinstance(favorites, list):\n\t\t\tfavorites = []\n\n\t\tresume_data = mtimedb[\"resume\"]\n\t\tmergelist = resume_data[\"mergelist\"]\n\t\tif mergelist and \"--skipfirst\" in myopts:\n\t\t\tfor i, task in enumerate(mergelist):\n\t\t\t\tif isinstance(task, list) and \\\n\t\t\t\t\ttask and task[-1] == \"merge\":\n\t\t\t\t\tdel mergelist[i]\n\t\t\t\t\tbreak\n\n\t\tsuccess = False\n\t\tmydepgraph = None\n\t\ttry:\n\t\t\tsuccess, mydepgraph, dropped_tasks = resume_depgraph(\n\t\t\t\tsettings, trees, mtimedb, myopts, myparams, spinner)\n\t\texcept (portage.exception.PackageNotFound,\n\t\t\tdepgraph.UnsatisfiedResumeDep) as e:\n\t\t\tif isinstance(e, depgraph.UnsatisfiedResumeDep):\n\t\t\t\tmydepgraph = e.depgraph\n\n\t\t\tfrom portage.output import EOutput\n\t\t\tout = EOutput()\n\n\t\t\tresume_data = mtimedb[\"resume\"]\n\t\t\tmergelist = resume_data.get(\"mergelist\")\n\t\t\tif not isinstance(mergelist, list):\n\t\t\t\tmergelist = []\n\t\t\tif mergelist and debug or (verbose and not quiet):\n\t\t\t\tout.eerror(\"Invalid resume list:\")\n\t\t\t\tout.eerror(\"\")\n\t\t\t\tindent = \"  \"\n\t\t\t\tfor task in mergelist:\n\t\t\t\t\tif isinstance(task, list):\n\t\t\t\t\t\tout.eerror(indent + str(tuple(task)))\n\t\t\t\tout.eerror(\"\")\n\n\t\t\tif isinstance(e, depgraph.UnsatisfiedResumeDep):\n\t\t\t\tout.eerror(\"One or more packages are either masked or \" + \\\n\t\t\t\t\t\"have missing dependencies:\")\n\t\t\t\tout.eerror(\"\")\n\t\t\t\tindent = \"  \"\n\t\t\t\tfor dep in e.value:\n\t\t\t\t\tif dep.atom is None:\n\t\t\t\t\t\tout.eerror(indent + \"Masked package:\")\n\t\t\t\t\t\tout.eerror(2 * indent + str(dep.parent))\n\t\t\t\t\t\tout.eerror(\"\")\n\t\t\t\t\telse:\n\t\t\t\t\t\tout.eerror(indent + str(dep.atom) + \" pulled in by:\")\n\t\t\t\t\t\tout.eerror(2 * indent + str(dep.parent))\n\t\t\t\t\t\tout.eerror(\"\")\n\t\t\t\tmsg = \"The resume list contains packages \" + \\\n\t\t\t\t\t\"that are either masked or have \" + \\\n\t\t\t\t\t\"unsatisfied dependencies. \" + \\\n\t\t\t\t\t\"Please restart\/continue \" + \\\n\t\t\t\t\t\"the operation manually, or use --skipfirst \" + \\\n\t\t\t\t\t\"to skip the first package in the list and \" + \\\n\t\t\t\t\t\"any other packages that may be \" + \\\n\t\t\t\t\t\"masked or have missing dependencies.\"\n\t\t\t\tfor line in textwrap.wrap(msg, 72):\n\t\t\t\t\tout.eerror(line)\n\t\t\telif isinstance(e, portage.exception.PackageNotFound):\n\t\t\t\tout.eerror(\"An expected package is \" + \\\n\t\t\t\t\t\"not available: %s\" % str(e))\n\t\t\t\tout.eerror(\"\")\n\t\t\t\tmsg = \"The resume list contains one or more \" + \\\n\t\t\t\t\t\"packages that are no longer \" + \\\n\t\t\t\t\t\"available. Please restart\/continue \" + \\\n\t\t\t\t\t\"the operation manually.\"\n\t\t\t\tfor line in textwrap.wrap(msg, 72):\n\t\t\t\t\tout.eerror(line)\n\n\t\tif success:\n\t\t\tif dropped_tasks:\n\t\t\t\tportage.writemsg(\"!!! One or more packages have been \" + \\\n\t\t\t\t\t\"dropped due to\\n\" + \\\n\t\t\t\t\t\"!!! masking or unsatisfied dependencies:\\n\\n\",\n\t\t\t\t\tnoiselevel=-1)\n\t\t\t\tfor task, atoms in dropped_tasks.items():\n\t\t\t\t\tif not atoms:\n\t\t\t\t\t\twritemsg(\"  %s is masked or unavailable\\n\" %\n\t\t\t\t\t\t\t(task,), noiselevel=-1)\n\t\t\t\t\telse:\n\t\t\t\t\t\twritemsg(\"  %s requires %s\\n\" %\n\t\t\t\t\t\t\t(task, \", \".join(atoms)), noiselevel=-1)\n\n\t\t\t\tportage.writemsg(\"\\n\", noiselevel=-1)\n\t\t\tdel dropped_tasks\n\t\telse:\n\t\t\tif mydepgraph is not None:\n\t\t\t\tmydepgraph.display_problems()\n\t\t\tif not (ask or pretend):\n\t\t\t\t# delete the current list and also the backup\n\t\t\t\t# since it's probably stale too.\n\t\t\t\tfor k in (\"resume\", \"resume_backup\"):\n\t\t\t\t\tmtimedb.pop(k, None)\n\t\t\t\tmtimedb.commit()\n\n\t\t\treturn 1\n\telse:\n\t\tif (\"--resume\" in myopts):\n\t\t\tprint(darkgreen(\"emerge: It seems we have nothing to resume...\"))\n\t\t\treturn os.EX_OK\n\n\t\ttry:\n\t\t\tsuccess, mydepgraph, favorites = backtrack_depgraph(\n\t\t\t\tsettings, trees, myopts, myparams, myaction, myfiles, spinner)\n\t\texcept portage.exception.PackageSetNotFound as e:\n\t\t\troot_config = trees[settings['EROOT']]['root_config']\n\t\t\tdisplay_missing_pkg_set(root_config, e.value)\n\t\t\treturn 1\n\n\t\tif success and mydepgraph.need_config_reload():\n\t\t\tload_emerge_config(emerge_config=emerge_config)\n\t\t\tadjust_configs(emerge_config.opts, emerge_config.trees)\n\t\t\tsettings, trees, mtimedb = emerge_config\n\n\t\tif \"--autounmask-only\" in myopts:\n\t\t\tmydepgraph.display_problems()\n\t\t\treturn 0\n\n\t\tif not success:\n\t\t\tmydepgraph.display_problems()\n\t\t\treturn 1\n\n\tmergecount = None\n\tif \"--pretend\" not in myopts and \\\n\t\t(\"--ask\" in myopts or \"--tree\" in myopts or \\\n\t\t\"--verbose\" in myopts) and \\\n\t\tnot (\"--quiet\" in myopts and \"--ask\" not in myopts):\n\t\tif \"--resume\" in myopts:\n\t\t\tmymergelist = mydepgraph.altlist()\n\t\t\tif len(mymergelist) == 0:\n\t\t\t\tprint(colorize(\"INFORM\", \"emerge: It seems we have nothing to resume...\"))\n\t\t\t\treturn os.EX_OK\n\t\t\tfavorites = mtimedb[\"resume\"][\"favorites\"]\n\t\t\tretval = mydepgraph.display(\n\t\t\t\tmydepgraph.altlist(),\n\t\t\t\tfavorites=favorites)\n\t\t\tmydepgraph.display_problems()\n\t\t\tmergelist_shown = True\n\t\t\tif retval != os.EX_OK:\n\t\t\t\treturn retval\n\t\t\tprompt=\"Would you like to resume merging these packages?\"\n\t\telse:\n\t\t\tretval = mydepgraph.display(\n\t\t\t\tmydepgraph.altlist(),\n\t\t\t\tfavorites=favorites)\n\t\t\tmydepgraph.display_problems()\n\t\t\tmergelist_shown = True\n\t\t\tif retval != os.EX_OK:\n\t\t\t\treturn retval\n\t\t\tmergecount=0\n\t\t\tfor x in mydepgraph.altlist():\n\t\t\t\tif isinstance(x, Package) and x.operation == \"merge\":\n\t\t\t\t\tmergecount += 1\n\n\t\t\tprompt = None\n\t\t\tif mergecount==0:\n\t\t\t\tsets = trees[settings['EROOT']]['root_config'].sets\n\t\t\t\tworld_candidates = None\n\t\t\t\tif \"selective\" in myparams and \\\n\t\t\t\t\tnot oneshot and favorites:\n\t\t\t\t\t# Sets that are not world candidates are filtered\n\t\t\t\t\t# out here since the favorites list needs to be\n\t\t\t\t\t# complete for depgraph.loadResumeCommand() to\n\t\t\t\t\t# operate correctly.\n\t\t\t\t\tworld_candidates = [x for x in favorites \\\n\t\t\t\t\t\tif not (x.startswith(SETPREFIX) and \\\n\t\t\t\t\t\tnot sets[x[1:]].world_candidate)]\n\n\t\t\t\tif \"selective\" in myparams and \\\n\t\t\t\t\tnot oneshot and world_candidates:\n\t\t\t\t\t# Prompt later, inside saveNomergeFavorites.\n\t\t\t\t\tprompt = None\n\t\t\t\telse:\n\t\t\t\t\tprint()\n\t\t\t\t\tprint(\"Nothing to merge; quitting.\")\n\t\t\t\t\tprint()\n\t\t\t\t\treturn os.EX_OK\n\t\t\telif \"--fetchonly\" in myopts or \"--fetch-all-uri\" in myopts:\n\t\t\t\tprompt=\"Would you like to fetch the source files for these packages?\"\n\t\t\telse:\n\t\t\t\tprompt=\"Would you like to merge these packages?\"\n\t\tprint()\n\t\tuq = UserQuery(myopts)\n\t\tif prompt is not None and \"--ask\" in myopts and \\\n\t\t\tuq.query(prompt, enter_invalid) == \"No\":\n\t\t\tprint()\n\t\t\tprint(\"Quitting.\")\n\t\t\tprint()\n\t\t\treturn 128 + signal.SIGINT\n\t\t# Don't ask again (e.g. when auto-cleaning packages after merge)\n\t\tif mergecount != 0:\n\t\t\tmyopts.pop(\"--ask\", None)\n\n\tif (\"--pretend\" in myopts) and not (\"--fetchonly\" in myopts or \"--fetch-all-uri\" in myopts):\n\t\tif (\"--resume\" in myopts):\n\t\t\tmymergelist = mydepgraph.altlist()\n\t\t\tif len(mymergelist) == 0:\n\t\t\t\tprint(colorize(\"INFORM\", \"emerge: It seems we have nothing to resume...\"))\n\t\t\t\treturn os.EX_OK\n\t\t\tfavorites = mtimedb[\"resume\"][\"favorites\"]\n\t\t\tretval = mydepgraph.display(\n\t\t\t\tmydepgraph.altlist(),\n\t\t\t\tfavorites=favorites)\n\t\t\tmydepgraph.display_problems()\n\t\t\tmergelist_shown = True\n\t\t\tif retval != os.EX_OK:\n\t\t\t\treturn retval\n\t\telse:\n\t\t\tretval = mydepgraph.display(\n\t\t\t\tmydepgraph.altlist(),\n\t\t\t\tfavorites=favorites)\n\t\t\tmydepgraph.display_problems()\n\t\t\tmergelist_shown = True\n\t\t\tif retval != os.EX_OK:\n\t\t\t\treturn retval\n\n\telse:\n\n\t\tif not mergelist_shown:\n\t\t\t# If we haven't already shown the merge list above, at\n\t\t\t# least show warnings about missed updates and such.\n\t\t\tmydepgraph.display_problems()\n\n\n\t\tneed_write_vardb = not Scheduler. \\\n\t\t\t_opts_no_self_update.intersection(myopts)\n\n\t\tneed_write_bindb = not any(x in myopts for x in\n\t\t\t(\"--fetchonly\", \"--fetch-all-uri\",\n\t\t\t\"--pretend\", \"--usepkgonly\")) and \\\n\t\t\t(any(\"buildpkg\" in trees[eroot][\"root_config\"].\n\t\t\t\tsettings.features for eroot in trees) or\n\t\t\tany(\"buildsyspkg\" in trees[eroot][\"root_config\"].\n\t\t\t\tsettings.features for eroot in trees))\n\n\t\tif need_write_bindb or need_write_vardb:\n\n\t\t\teroots = set()\n\t\t\tebuild_eroots = set()\n\t\t\tfor x in mydepgraph.altlist():\n\t\t\t\tif isinstance(x, Package) and x.operation == \"merge\":\n\t\t\t\t\teroots.add(x.root)\n\t\t\t\t\tif x.type_name == \"ebuild\":\n\t\t\t\t\t\tebuild_eroots.add(x.root)\n\n\t\t\tfor eroot in eroots:\n\t\t\t\tif need_write_vardb and \\\n\t\t\t\t\tnot trees[eroot][\"vartree\"].dbapi.writable:\n\t\t\t\t\twritemsg_level(\"!!! %s\\n\" %\n\t\t\t\t\t\t_(\"Read-only file system: %s\") %\n\t\t\t\t\t\ttrees[eroot][\"vartree\"].dbapi._dbroot,\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\treturn 1\n\n\t\t\t\tif need_write_bindb and eroot in ebuild_eroots and \\\n\t\t\t\t\t(\"buildpkg\" in trees[eroot][\"root_config\"].\n\t\t\t\t\tsettings.features or\n\t\t\t\t\t\"buildsyspkg\" in trees[eroot][\"root_config\"].\n\t\t\t\t\tsettings.features) and \\\n\t\t\t\t\tnot trees[eroot][\"bintree\"].dbapi.writable:\n\t\t\t\t\twritemsg_level(\"!!! %s\\n\" %\n\t\t\t\t\t\t_(\"Read-only file system: %s\") %\n\t\t\t\t\t\ttrees[eroot][\"bintree\"].pkgdir,\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\treturn 1\n\n\t\tif (\"--resume\" in myopts):\n\t\t\tfavorites=mtimedb[\"resume\"][\"favorites\"]\n\n\t\telse:\n\t\t\tif \"resume\" in mtimedb and \\\n\t\t\t\"mergelist\" in mtimedb[\"resume\"] and \\\n\t\t\tlen(mtimedb[\"resume\"][\"mergelist\"]) > 1:\n\t\t\t\tmtimedb[\"resume_backup\"] = mtimedb[\"resume\"]\n\t\t\t\tdel mtimedb[\"resume\"]\n\t\t\t\tmtimedb.commit()\n\n\t\t\tmydepgraph.saveNomergeFavorites()\n\n\t\tif mergecount == 0:\n\t\t\tretval = os.EX_OK\n\t\telse:\n\t\t\tmergetask = Scheduler(settings, trees, mtimedb, myopts,\n\t\t\t\tspinner, favorites=favorites,\n\t\t\t\tgraph_config=mydepgraph.schedulerGraph())\n\n\t\t\tdel mydepgraph\n\t\t\tclear_caches(trees)\n\n\t\t\tretval = mergetask.merge()\n\n\t\t\tif retval == os.EX_OK and \\\n\t\t\t\tnot (buildpkgonly or fetchonly or pretend):\n\t\t\t\tif \"yes\" == settings.get(\"AUTOCLEAN\"):\n\t\t\t\t\tportage.writemsg_stdout(\">>> Auto-cleaning packages...\\n\")\n\t\t\t\t\tunmerge(trees[settings['EROOT']]['root_config'],\n\t\t\t\t\t\tmyopts, \"clean\", [],\n\t\t\t\t\t\tldpath_mtimes, autoclean=1)\n\t\t\t\telse:\n\t\t\t\t\tportage.writemsg_stdout(colorize(\"WARN\", \"WARNING:\")\n\t\t\t\t\t\t+ \" AUTOCLEAN is disabled.  This can cause serious\"\n\t\t\t\t\t\t+ \" problems due to overlapping packages.\\n\")\n\n\t\treturn retval\n\ndef action_config(settings, trees, myopts, myfiles):\n\tenter_invalid = '--ask-enter-invalid' in myopts\n\tuq = UserQuery(myopts)\n\tif len(myfiles) != 1:\n\t\tprint(red(\"!!! config can only take a single package atom at this time\\n\"))\n\t\tsys.exit(1)\n\tif not is_valid_package_atom(myfiles[0], allow_repo=True):\n\t\tportage.writemsg(\"!!! '%s' is not a valid package atom.\\n\" % myfiles[0],\n\t\t\tnoiselevel=-1)\n\t\tportage.writemsg(\"!!! Please check ebuild(5) for full details.\\n\")\n\t\tportage.writemsg(\"!!! (Did you specify a version but forget to prefix with '='?)\\n\")\n\t\tsys.exit(1)\n\tprint()\n\ttry:\n\t\tpkgs = trees[settings['EROOT']]['vartree'].dbapi.match(myfiles[0])\n\texcept portage.exception.AmbiguousPackageName as e:\n\t\t# Multiple matches thrown from cpv_expand\n\t\tpkgs = e.args[0]\n\tif len(pkgs) == 0:\n\t\tprint(\"No packages found.\\n\")\n\t\tsys.exit(0)\n\telif len(pkgs) > 1:\n\t\tif \"--ask\" in myopts:\n\t\t\toptions = []\n\t\t\tprint(\"Please select a package to configure:\")\n\t\t\tidx = 0\n\t\t\tfor pkg in pkgs:\n\t\t\t\tidx += 1\n\t\t\t\toptions.append(str(idx))\n\t\t\t\tprint(options[-1]+\") \"+pkg)\n\t\t\tprint(\"X) Cancel\")\n\t\t\toptions.append(\"X\")\n\t\t\tidx = uq.query(\"Selection?\", enter_invalid, responses=options)\n\t\t\tif idx == \"X\":\n\t\t\t\tsys.exit(128 + signal.SIGINT)\n\t\t\tpkg = pkgs[int(idx)-1]\n\t\telse:\n\t\t\tprint(\"The following packages available:\")\n\t\t\tfor pkg in pkgs:\n\t\t\t\tprint(\"* \"+pkg)\n\t\t\tprint(\"\\nPlease use a specific atom or the --ask option.\")\n\t\t\tsys.exit(1)\n\telse:\n\t\tpkg = pkgs[0]\n\n\tprint()\n\tif \"--ask\" in myopts:\n\t\tif uq.query(\"Ready to configure %s?\" % pkg, enter_invalid) == \"No\":\n\t\t\tsys.exit(128 + signal.SIGINT)\n\telse:\n\t\tprint(\"Configuring pkg...\")\n\tprint()\n\tebuildpath = trees[settings['EROOT']]['vartree'].dbapi.findname(pkg)\n\tmysettings = portage.config(clone=settings)\n\tvardb = trees[mysettings['EROOT']]['vartree'].dbapi\n\tdebug = mysettings.get(\"PORTAGE_DEBUG\") == \"1\"\n\tretval = portage.doebuild(ebuildpath, \"config\", settings=mysettings,\n\t\tdebug=(settings.get(\"PORTAGE_DEBUG\", \"\") == 1), cleanup=True,\n\t\tmydbapi = trees[settings['EROOT']]['vartree'].dbapi, tree=\"vartree\")\n\tif retval == os.EX_OK:\n\t\tportage.doebuild(ebuildpath, \"clean\", settings=mysettings,\n\t\t\tdebug=debug, mydbapi=vardb, tree=\"vartree\")\n\tprint()\n\ndef action_depclean(settings, trees, ldpath_mtimes,\n\tmyopts, action, myfiles, spinner, scheduler=None):\n\t# Kill packages that aren't explicitly merged or are required as a\n\t# dependency of another package. World file is explicit.\n\n\t# Global depclean or prune operations are not very safe when there are\n\t# missing dependencies since it's unknown how badly incomplete\n\t# the dependency graph is, and we might accidentally remove packages\n\t# that should have been pulled into the graph. On the other hand, it's\n\t# relatively safe to ignore missing deps when only asked to remove\n\t# specific packages.\n\n\tmsg = []\n\tif \"preserve-libs\" not in settings.features and \\\n\t\tnot myopts.get(\"--depclean-lib-check\", _DEPCLEAN_LIB_CHECK_DEFAULT) != \"n\":\n\t\tmsg.append(\"Depclean may break link level dependencies. Thus, it is\\n\")\n\t\tmsg.append(\"recommended to use a tool such as \" + good(\"`revdep-rebuild`\") + \" (from\\n\")\n\t\tmsg.append(\"app-portage\/gentoolkit) in order to detect such breakage.\\n\")\n\t\tmsg.append(\"\\n\")\n\tmsg.append(\"Always study the list of packages to be cleaned for any obvious\\n\")\n\tmsg.append(\"mistakes. Packages that are part of the world set will always\\n\")\n\tmsg.append(\"be kept.  They can be manually added to this set with\\n\")\n\tmsg.append(good(\"`emerge --noreplace <atom>`\") + \".  Packages that are listed in\\n\")\n\tmsg.append(\"package.provided (see portage(5)) will be removed by\\n\")\n\tmsg.append(\"depclean, even if they are part of the world set.\\n\")\n\tmsg.append(\"\\n\")\n\tmsg.append(\"As a safety measure, depclean will not remove any packages\\n\")\n\tmsg.append(\"unless *all* required dependencies have been resolved.  As a\\n\")\n\tmsg.append(\"consequence of this, it often becomes necessary to run \\n\")\n\tmsg.append(\"%s\" % good(\"`emerge --update --newuse --deep @world`\")\n\t\t\t+ \" prior to depclean.\\n\")\n\n\tif action == \"depclean\" and \"--quiet\" not in myopts and not myfiles:\n\t\tportage.writemsg_stdout(\"\\n\")\n\t\tfor x in msg:\n\t\t\tportage.writemsg_stdout(colorize(\"WARN\", \" * \") + x)\n\n\troot_config = trees[settings['EROOT']]['root_config']\n\tvardb = root_config.trees['vartree'].dbapi\n\n\targs_set = InternalPackageSet(allow_repo=True)\n\tif myfiles:\n\t\targs_set.update(myfiles)\n\t\tmatched_packages = False\n\t\tfor x in args_set:\n\t\t\tif vardb.match(x):\n\t\t\t\tmatched_packages = True\n\t\t\telse:\n\t\t\t\twritemsg_level(\"--- Couldn't find '%s' to %s.\\n\" % \\\n\t\t\t\t\t(x.replace(\"null\/\", \"\"), action),\n\t\t\t\t\tlevel=logging.WARN, noiselevel=-1)\n\t\tif not matched_packages:\n\t\t\twritemsg_level(\">>> No packages selected for removal by %s\\n\" % \\\n\t\t\t\taction)\n\t\t\treturn 0\n\n\t# The calculation is done in a separate function so that depgraph\n\t# references go out of scope and the corresponding memory\n\t# is freed before we call unmerge().\n\trval, cleanlist, ordered, req_pkg_count = \\\n\t\tcalc_depclean(settings, trees, ldpath_mtimes,\n\t\t\tmyopts, action, args_set, spinner)\n\n\tclear_caches(trees)\n\n\tif rval != os.EX_OK:\n\t\treturn rval\n\n\tif cleanlist:\n\t\trval = unmerge(root_config, myopts, \"unmerge\",\n\t\t\tcleanlist, ldpath_mtimes, ordered=ordered,\n\t\t\tscheduler=scheduler)\n\n\tif action == \"prune\":\n\t\treturn rval\n\n\tif not cleanlist and \"--quiet\" in myopts:\n\t\treturn rval\n\n\tset_atoms = {}\n\tfor k in (\"profile\", \"system\", \"selected\"):\n\t\ttry:\n\t\t\tset_atoms[k] = root_config.setconfig.getSetAtoms(k)\n\t\texcept portage.exception.PackageSetNotFound:\n\t\t\t# A nested set could not be resolved, so ignore nested sets.\n\t\t\tset_atoms[k] = root_config.sets[k].getAtoms()\n\n\tprint(\"Packages installed:   \" + str(len(vardb.cpv_all())))\n\tprint(\"Packages in world:    %d\" % len(set_atoms[\"selected\"]))\n\tprint(\"Packages in system:   %d\" % len(set_atoms[\"system\"]))\n\tif set_atoms[\"profile\"]:\n\t\tprint(\"Packages in profile:  %d\" % len(set_atoms[\"profile\"]))\n\tprint(\"Required packages:    \"+str(req_pkg_count))\n\tif \"--pretend\" in myopts:\n\t\tprint(\"Number to remove:     \"+str(len(cleanlist)))\n\telse:\n\t\tprint(\"Number removed:       \"+str(len(cleanlist)))\n\n\treturn rval\n\ndef calc_depclean(settings, trees, ldpath_mtimes,\n\tmyopts, action, args_set, spinner):\n\tallow_missing_deps = bool(args_set)\n\n\tdebug = '--debug' in myopts\n\txterm_titles = \"notitles\" not in settings.features\n\troot_len = len(settings[\"ROOT\"])\n\teroot = settings['EROOT']\n\troot_config = trees[eroot][\"root_config\"]\n\tpsets = root_config.setconfig.psets\n\tdeselect = myopts.get('--deselect') != 'n'\n\trequired_sets = {}\n\trequired_sets['world'] = psets['world']\n\n\t# When removing packages, a temporary version of the world 'selected'\n\t# set may be used which excludes packages that are intended to be\n\t# eligible for removal.\n\tselected_set = psets['selected']\n\trequired_sets['selected'] = selected_set\n\tprotected_set = InternalPackageSet()\n\tprotected_set_name = '____depclean_protected_set____'\n\trequired_sets[protected_set_name] = protected_set\n\n\tset_error = False\n\tset_atoms = {}\n\tfor k in (\"profile\", \"system\", \"selected\"):\n\t\ttry:\n\t\t\tset_atoms[k] = root_config.setconfig.getSetAtoms(k)\n\t\texcept portage.exception.PackageSetNotFound as e:\n\t\t\t# A nested set could not be resolved, so ignore nested sets.\n\t\t\tset_atoms[k] = root_config.sets[k].getAtoms()\n\t\t\twritemsg_level(_(\"!!! The set '%s' \"\n\t\t\t\t\"contains a non-existent set named '%s'.\\n\") %\n\t\t\t\t(k, e), level=logging.ERROR, noiselevel=-1)\n\t\t\tset_error = True\n\n\t# Support @profile as an alternative to @system.\n\tif not (set_atoms[\"system\"] or set_atoms[\"profile\"]):\n\t\twritemsg_level(_(\"!!! You have no system list.\\n\"),\n\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\tif not set_atoms[\"selected\"]:\n\t\twritemsg_level(_(\"!!! You have no world file.\\n\"),\n\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\t# Suppress world file warnings unless @world is completely empty,\n\t# since having an empty world file can be a valid state.\n\ttry:\n\t\tworld_atoms = bool(root_config.setconfig.getSetAtoms('world'))\n\texcept portage.exception.PackageSetNotFound as e:\n\t\twritemsg_level(_(\"!!! The set '%s' \"\n\t\t\t\"contains a non-existent set named '%s'.\\n\") %\n\t\t\t(\"world\", e), level=logging.ERROR, noiselevel=-1)\n\t\tset_error = True\n\telse:\n\t\tif not world_atoms:\n\t\t\twritemsg_level(_(\"!!! Your @world set is empty.\\n\"),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\tset_error = True\n\n\tif set_error:\n\t\twritemsg_level(_(\"!!! Aborting due to set configuration \"\n\t\t\t\"errors displayed above.\\n\"),\n\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\treturn 1, [], False, 0\n\n\tif action == \"depclean\":\n\t\temergelog(xterm_titles, \" >>> depclean\")\n\n\twritemsg_level(\"\\nCalculating dependencies  \")\n\tresolver_params = create_depgraph_params(myopts, \"remove\")\n\tresolver = depgraph(settings, trees, myopts, resolver_params, spinner)\n\tresolver._load_vdb()\n\tvardb = resolver._frozen_config.trees[eroot][\"vartree\"].dbapi\n\treal_vardb = trees[eroot][\"vartree\"].dbapi\n\n\tif action == \"depclean\":\n\n\t\tif args_set:\n\n\t\t\tif deselect:\n\t\t\t\t# Start with an empty set.\n\t\t\t\tselected_set = InternalPackageSet()\n\t\t\t\trequired_sets['selected'] = selected_set\n\t\t\t\t# Pull in any sets nested within the selected set.\n\t\t\t\tselected_set.update(psets['selected'].getNonAtoms())\n\n\t\t\t# Pull in everything that's installed but not matched\n\t\t\t# by an argument atom since we don't want to clean any\n\t\t\t# package if something depends on it.\n\t\t\tfor pkg in vardb:\n\t\t\t\tif spinner:\n\t\t\t\t\tspinner.update()\n\n\t\t\t\ttry:\n\t\t\t\t\tif args_set.findAtomForPackage(pkg) is None:\n\t\t\t\t\t\tprotected_set.add(\"=\" + pkg.cpv)\n\t\t\t\t\t\tcontinue\n\t\t\t\texcept portage.exception.InvalidDependString as e:\n\t\t\t\t\tshow_invalid_depstring_notice(pkg,\n\t\t\t\t\t\tpkg._metadata[\"PROVIDE\"], _unicode(e))\n\t\t\t\t\tdel e\n\t\t\t\t\tprotected_set.add(\"=\" + pkg.cpv)\n\t\t\t\t\tcontinue\n\n\telif action == \"prune\":\n\n\t\tif deselect:\n\t\t\t# Start with an empty set.\n\t\t\tselected_set = InternalPackageSet()\n\t\t\trequired_sets['selected'] = selected_set\n\t\t\t# Pull in any sets nested within the selected set.\n\t\t\tselected_set.update(psets['selected'].getNonAtoms())\n\n\t\t# Pull in everything that's installed since we don't\n\t\t# to prune a package if something depends on it.\n\t\tprotected_set.update(vardb.cp_all())\n\n\t\tif not args_set:\n\n\t\t\t# Try to prune everything that's slotted.\n\t\t\tfor cp in vardb.cp_all():\n\t\t\t\tif len(vardb.cp_list(cp)) > 1:\n\t\t\t\t\targs_set.add(cp)\n\n\t\t# Remove atoms from world that match installed packages\n\t\t# that are also matched by argument atoms, but do not remove\n\t\t# them if they match the highest installed version.\n\t\tfor pkg in vardb:\n\t\t\tif spinner is not None:\n\t\t\t\tspinner.update()\n\t\t\tpkgs_for_cp = vardb.match_pkgs(Atom(pkg.cp))\n\t\t\tif not pkgs_for_cp or pkg not in pkgs_for_cp:\n\t\t\t\traise AssertionError(\"package expected in matches: \" + \\\n\t\t\t\t\t\"cp = %s, cpv = %s matches = %s\" % \\\n\t\t\t\t\t(pkg.cp, pkg.cpv, [str(x) for x in pkgs_for_cp]))\n\n\t\t\thighest_version = pkgs_for_cp[-1]\n\t\t\tif pkg == highest_version:\n\t\t\t\t# pkg is the highest version\n\t\t\t\tprotected_set.add(\"=\" + pkg.cpv)\n\t\t\t\tcontinue\n\n\t\t\tif len(pkgs_for_cp) <= 1:\n\t\t\t\traise AssertionError(\"more packages expected: \" + \\\n\t\t\t\t\t\"cp = %s, cpv = %s matches = %s\" % \\\n\t\t\t\t\t(pkg.cp, pkg.cpv, [str(x) for x in pkgs_for_cp]))\n\n\t\t\ttry:\n\t\t\t\tif args_set.findAtomForPackage(pkg) is None:\n\t\t\t\t\tprotected_set.add(\"=\" + pkg.cpv)\n\t\t\t\t\tcontinue\n\t\t\texcept portage.exception.InvalidDependString as e:\n\t\t\t\tshow_invalid_depstring_notice(pkg,\n\t\t\t\t\tpkg._metadata[\"PROVIDE\"], _unicode(e))\n\t\t\t\tdel e\n\t\t\t\tprotected_set.add(\"=\" + pkg.cpv)\n\t\t\t\tcontinue\n\n\tif resolver._frozen_config.excluded_pkgs:\n\t\texcluded_set = resolver._frozen_config.excluded_pkgs\n\t\trequired_sets['__excluded__'] = InternalPackageSet()\n\n\t\tfor pkg in vardb:\n\t\t\tif spinner:\n\t\t\t\tspinner.update()\n\n\t\t\ttry:\n\t\t\t\tif excluded_set.findAtomForPackage(pkg):\n\t\t\t\t\trequired_sets['__excluded__'].add(\"=\" + pkg.cpv)\n\t\t\texcept portage.exception.InvalidDependString as e:\n\t\t\t\tshow_invalid_depstring_notice(pkg,\n\t\t\t\t\tpkg._metadata[\"PROVIDE\"], _unicode(e))\n\t\t\t\tdel e\n\t\t\t\trequired_sets['__excluded__'].add(\"=\" + pkg.cpv)\n\n\tsuccess = resolver._complete_graph(required_sets={eroot:required_sets})\n\twritemsg_level(\"\\b\\b... done!\\n\")\n\n\tresolver.display_problems()\n\n\tif not success:\n\t\treturn 1, [], False, 0\n\n\tdef unresolved_deps():\n\n\t\tsoname_deps = set()\n\t\tunresolvable = set()\n\t\tfor dep in resolver._dynamic_config._initially_unsatisfied_deps:\n\t\t\tif isinstance(dep.parent, Package) and \\\n\t\t\t\t(dep.priority > UnmergeDepPriority.SOFT):\n\t\t\t\tif dep.atom.soname:\n\t\t\t\t\tsoname_deps.add((dep.atom, dep.parent.cpv))\n\t\t\t\telse:\n\t\t\t\t\tunresolvable.add((dep.atom, dep.parent.cpv))\n\n\t\tif soname_deps:\n\t\t\t# Generally, broken soname dependencies can safely be\n\t\t\t# suppressed by a REQUIRES_EXCLUDE setting in the ebuild,\n\t\t\t# so they should only trigger a warning message.\n\t\t\tprefix = warn(\" * \")\n\t\t\tmsg = []\n\t\t\tmsg.append(\"Broken soname dependencies found:\")\n\t\t\tmsg.append(\"\")\n\t\t\tfor atom, parent in soname_deps:\n\t\t\t\tmsg.append(\"  %s required by:\" % (atom,))\n\t\t\t\tmsg.append(\"    %s\" % (parent,))\n\t\t\t\tmsg.append(\"\")\n\n\t\t\twritemsg_level(\"\".join(\"%s%s\\n\" % (prefix, line) for line in msg),\n\t\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\t\tif not unresolvable:\n\t\t\treturn False\n\n\t\tif unresolvable and not allow_missing_deps:\n\n\t\t\tif \"--debug\" in myopts:\n\t\t\t\twritemsg(\"\\ndigraph:\\n\\n\", noiselevel=-1)\n\t\t\t\tresolver._dynamic_config.digraph.debug_print()\n\t\t\t\twritemsg(\"\\n\", noiselevel=-1)\n\n\t\t\tprefix = bad(\" * \")\n\t\t\tmsg = []\n\t\t\tmsg.append(\"Dependencies could not be completely resolved due to\")\n\t\t\tmsg.append(\"the following required packages not being installed:\")\n\t\t\tmsg.append(\"\")\n\t\t\tfor atom, parent in unresolvable:\n\t\t\t\tif atom.package and atom != atom.unevaluated_atom and \\\n\t\t\t\t\tvardb.match(_unicode(atom)):\n\t\t\t\t\tmsg.append(\"  %s (%s) pulled in by:\" %\n\t\t\t\t\t\t(atom.unevaluated_atom, atom))\n\t\t\t\telse:\n\t\t\t\t\tmsg.append(\"  %s pulled in by:\" % (atom,))\n\t\t\t\tmsg.append(\"    %s\" % (parent,))\n\t\t\t\tmsg.append(\"\")\n\t\t\tmsg.extend(textwrap.wrap(\n\t\t\t\t\"Have you forgotten to do a complete update prior \" + \\\n\t\t\t\t\"to depclean? The most comprehensive command for this \" + \\\n\t\t\t\t\"purpose is as follows:\", 65\n\t\t\t))\n\t\t\tmsg.append(\"\")\n\t\t\tmsg.append(\"  \" + \\\n\t\t\t\tgood(\"emerge --update --newuse --deep --with-bdeps=y @world\"))\n\t\t\tmsg.append(\"\")\n\t\t\tmsg.extend(textwrap.wrap(\n\t\t\t\t\"Note that the --with-bdeps=y option is not required in \" + \\\n\t\t\t\t\"many situations. Refer to the emerge manual page \" + \\\n\t\t\t\t\"(run `man emerge`) for more information about \" + \\\n\t\t\t\t\"--with-bdeps.\", 65\n\t\t\t))\n\t\t\tmsg.append(\"\")\n\t\t\tmsg.extend(textwrap.wrap(\n\t\t\t\t\"Also, note that it may be necessary to manually uninstall \" + \\\n\t\t\t\t\"packages that no longer exist in the portage tree, since \" + \\\n\t\t\t\t\"it may not be possible to satisfy their dependencies.\", 65\n\t\t\t))\n\t\t\tif action == \"prune\":\n\t\t\t\tmsg.append(\"\")\n\t\t\t\tmsg.append(\"If you would like to ignore \" + \\\n\t\t\t\t\t\"dependencies then use %s.\" % good(\"--nodeps\"))\n\t\t\twritemsg_level(\"\".join(\"%s%s\\n\" % (prefix, line) for line in msg),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\treturn True\n\t\treturn False\n\n\tif unresolved_deps():\n\t\treturn 1, [], False, 0\n\n\tgraph = resolver._dynamic_config.digraph.copy()\n\trequired_pkgs_total = 0\n\tfor node in graph:\n\t\tif isinstance(node, Package):\n\t\t\trequired_pkgs_total += 1\n\n\tdef show_parents(child_node):\n\t\tparent_atoms = \\\n\t\t\tresolver._dynamic_config._parent_atoms.get(child_node, [])\n\n\t\t# Never display the special internal protected_set.\n\t\tparent_atoms = [parent_atom for parent_atom in parent_atoms\n\t\t\tif not (isinstance(parent_atom[0], SetArg) and\n\t\t\tparent_atom[0].name == protected_set_name)]\n\n\t\tif not parent_atoms:\n\t\t\t# With --prune, the highest version can be pulled in without any\n\t\t\t# real parent since all installed packages are pulled in.  In that\n\t\t\t# case there's nothing to show here.\n\t\t\treturn\n\t\tparent_atom_dict = {}\n\t\tfor parent, atom in parent_atoms:\n\t\t\tparent_atom_dict.setdefault(parent, []).append(atom)\n\n\t\tparent_strs = []\n\t\tfor parent, atoms in parent_atom_dict.items():\n\t\t\t# Display package atoms and soname\n\t\t\t# atoms in separate groups.\n\t\t\tatoms = sorted(atoms, reverse=True,\n\t\t\t\tkey=operator.attrgetter('package'))\n\t\t\tparent_strs.append(\"%s requires %s\" %\n\t\t\t\t(getattr(parent, \"cpv\", parent),\n\t\t\t\t\", \".join(_unicode(atom) for atom in atoms)))\n\t\tparent_strs.sort()\n\t\tmsg = []\n\t\tmsg.append(\"  %s pulled in by:\\n\" % (child_node.cpv,))\n\t\tfor parent_str in parent_strs:\n\t\t\tmsg.append(\"    %s\\n\" % (parent_str,))\n\t\tmsg.append(\"\\n\")\n\t\tportage.writemsg_stdout(\"\".join(msg), noiselevel=-1)\n\n\tdef cmp_pkg_cpv(pkg1, pkg2):\n\t\t\"\"\"Sort Package instances by cpv.\"\"\"\n\t\tif pkg1.cpv > pkg2.cpv:\n\t\t\treturn 1\n\t\telif pkg1.cpv == pkg2.cpv:\n\t\t\treturn 0\n\t\telse:\n\t\t\treturn -1\n\n\tdef create_cleanlist():\n\n\t\tif \"--debug\" in myopts:\n\t\t\twritemsg(\"\\ndigraph:\\n\\n\", noiselevel=-1)\n\t\t\tgraph.debug_print()\n\t\t\twritemsg(\"\\n\", noiselevel=-1)\n\n\t\tpkgs_to_remove = []\n\n\t\tif action == \"depclean\":\n\t\t\tif args_set:\n\n\t\t\t\tfor pkg in sorted(vardb, key=cmp_sort_key(cmp_pkg_cpv)):\n\t\t\t\t\targ_atom = None\n\t\t\t\t\ttry:\n\t\t\t\t\t\targ_atom = args_set.findAtomForPackage(pkg)\n\t\t\t\t\texcept portage.exception.InvalidDependString:\n\t\t\t\t\t\t# this error has already been displayed by now\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tif arg_atom:\n\t\t\t\t\t\tif pkg not in graph:\n\t\t\t\t\t\t\tpkgs_to_remove.append(pkg)\n\t\t\t\t\t\telif \"--verbose\" in myopts:\n\t\t\t\t\t\t\tshow_parents(pkg)\n\n\t\t\telse:\n\t\t\t\tfor pkg in sorted(vardb, key=cmp_sort_key(cmp_pkg_cpv)):\n\t\t\t\t\tif pkg not in graph:\n\t\t\t\t\t\tpkgs_to_remove.append(pkg)\n\t\t\t\t\telif \"--verbose\" in myopts:\n\t\t\t\t\t\tshow_parents(pkg)\n\n\t\telif action == \"prune\":\n\n\t\t\tfor atom in args_set:\n\t\t\t\tfor pkg in vardb.match_pkgs(atom):\n\t\t\t\t\tif pkg not in graph:\n\t\t\t\t\t\tpkgs_to_remove.append(pkg)\n\t\t\t\t\telif \"--verbose\" in myopts:\n\t\t\t\t\t\tshow_parents(pkg)\n\n\t\tif not pkgs_to_remove:\n\t\t\twritemsg_level(\n\t\t\t\t\">>> No packages selected for removal by %s\\n\" % action)\n\t\t\tif \"--verbose\" not in myopts:\n\t\t\t\twritemsg_level(\n\t\t\t\t\t\">>> To see reverse dependencies, use %s\\n\" % \\\n\t\t\t\t\t\tgood(\"--verbose\"))\n\t\t\tif action == \"prune\":\n\t\t\t\twritemsg_level(\n\t\t\t\t\t\">>> To ignore dependencies, use %s\\n\" % \\\n\t\t\t\t\t\tgood(\"--nodeps\"))\n\n\t\treturn pkgs_to_remove\n\n\tcleanlist = create_cleanlist()\n\tclean_set = set(cleanlist)\n\n\tdepclean_lib_check = cleanlist and real_vardb._linkmap is not None and \\\n\t\tmyopts.get(\"--depclean-lib-check\", _DEPCLEAN_LIB_CHECK_DEFAULT) != \"n\"\n\tpreserve_libs = \"preserve-libs\" in settings.features\n\tpreserve_libs_restrict = False\n\n\tif depclean_lib_check and preserve_libs:\n\t\tfor pkg in cleanlist:\n\t\t\tif \"preserve-libs\" in pkg.restrict:\n\t\t\t\tpreserve_libs_restrict = True\n\t\t\t\tbreak\n\n\tif depclean_lib_check and \\\n\t\t(preserve_libs_restrict or not preserve_libs):\n\n\t\t# Check if any of these packages are the sole providers of libraries\n\t\t# with consumers that have not been selected for removal. If so, these\n\t\t# packages and any dependencies need to be added to the graph.\n\t\tlinkmap = real_vardb._linkmap\n\t\tconsumer_cache = {}\n\t\tprovider_cache = {}\n\t\tconsumer_map = {}\n\n\t\twritemsg_level(\">>> Checking for lib consumers...\\n\")\n\n\t\tfor pkg in cleanlist:\n\n\t\t\tif preserve_libs and \"preserve-libs\" not in pkg.restrict:\n\t\t\t\t# Any needed libraries will be preserved\n\t\t\t\t# when this package is unmerged, so there's\n\t\t\t\t# no need to account for it here.\n\t\t\t\tcontinue\n\n\t\t\tpkg_dblink = real_vardb._dblink(pkg.cpv)\n\t\t\tconsumers = {}\n\n\t\t\tfor lib in pkg_dblink.getcontents():\n\t\t\t\tlib = lib[root_len:]\n\t\t\t\tlib_key = linkmap._obj_key(lib)\n\t\t\t\tlib_consumers = consumer_cache.get(lib_key)\n\t\t\t\tif lib_consumers is None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tlib_consumers = linkmap.findConsumers(lib_key)\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tconsumer_cache[lib_key] = lib_consumers\n\t\t\t\tif lib_consumers:\n\t\t\t\t\tconsumers[lib_key] = lib_consumers\n\n\t\t\tif not consumers:\n\t\t\t\tcontinue\n\n\t\t\tfor lib, lib_consumers in list(consumers.items()):\n\t\t\t\tfor consumer_file in list(lib_consumers):\n\t\t\t\t\tif pkg_dblink.isowner(consumer_file):\n\t\t\t\t\t\tlib_consumers.remove(consumer_file)\n\t\t\t\tif not lib_consumers:\n\t\t\t\t\tdel consumers[lib]\n\n\t\t\tif not consumers:\n\t\t\t\tcontinue\n\n\t\t\tfor lib, lib_consumers in consumers.items():\n\n\t\t\t\tsoname = linkmap.getSoname(lib)\n\n\t\t\t\tconsumer_providers = []\n\t\t\t\tfor lib_consumer in lib_consumers:\n\t\t\t\t\tproviders = provider_cache.get(lib)\n\t\t\t\t\tif providers is None:\n\t\t\t\t\t\tproviders = linkmap.findProviders(lib_consumer)\n\t\t\t\t\t\tprovider_cache[lib_consumer] = providers\n\t\t\t\t\tif soname not in providers:\n\t\t\t\t\t\t# Why does this happen?\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tconsumer_providers.append(\n\t\t\t\t\t\t(lib_consumer, providers[soname]))\n\n\t\t\t\tconsumers[lib] = consumer_providers\n\n\t\t\tconsumer_map[pkg] = consumers\n\n\t\tif consumer_map:\n\n\t\t\tsearch_files = set()\n\t\t\tfor consumers in consumer_map.values():\n\t\t\t\tfor lib, consumer_providers in consumers.items():\n\t\t\t\t\tfor lib_consumer, providers in consumer_providers:\n\t\t\t\t\t\tsearch_files.add(lib_consumer)\n\t\t\t\t\t\tsearch_files.update(providers)\n\n\t\t\twritemsg_level(\">>> Assigning files to packages...\\n\")\n\t\t\tfile_owners = {}\n\t\t\tfor f in search_files:\n\t\t\t\towner_set = set()\n\t\t\t\tfor owner in linkmap.getOwners(f):\n\t\t\t\t\towner_dblink = real_vardb._dblink(owner)\n\t\t\t\t\tif owner_dblink.exists():\n\t\t\t\t\t\towner_set.add(owner_dblink)\n\t\t\t\tif owner_set:\n\t\t\t\t\tfile_owners[f] = owner_set\n\n\t\t\tfor pkg, consumers in list(consumer_map.items()):\n\t\t\t\tfor lib, consumer_providers in list(consumers.items()):\n\t\t\t\t\tlib_consumers = set()\n\n\t\t\t\t\tfor lib_consumer, providers in consumer_providers:\n\t\t\t\t\t\towner_set = file_owners.get(lib_consumer)\n\t\t\t\t\t\tprovider_dblinks = set()\n\t\t\t\t\t\tprovider_pkgs = set()\n\n\t\t\t\t\t\tif len(providers) > 1:\n\t\t\t\t\t\t\tfor provider in providers:\n\t\t\t\t\t\t\t\tprovider_set = file_owners.get(provider)\n\t\t\t\t\t\t\t\tif provider_set is not None:\n\t\t\t\t\t\t\t\t\tprovider_dblinks.update(provider_set)\n\n\t\t\t\t\t\tif len(provider_dblinks) > 1:\n\t\t\t\t\t\t\tfor provider_dblink in provider_dblinks:\n\t\t\t\t\t\t\t\tprovider_pkg = resolver._pkg(\n\t\t\t\t\t\t\t\t\tprovider_dblink.mycpv, \"installed\",\n\t\t\t\t\t\t\t\t\troot_config, installed=True)\n\t\t\t\t\t\t\t\tif provider_pkg not in clean_set:\n\t\t\t\t\t\t\t\t\tprovider_pkgs.add(provider_pkg)\n\n\t\t\t\t\t\tif provider_pkgs:\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\tif owner_set is not None:\n\t\t\t\t\t\t\tlib_consumers.update(owner_set)\n\n\t\t\t\t\tfor consumer_dblink in list(lib_consumers):\n\t\t\t\t\t\tif resolver._pkg(consumer_dblink.mycpv, \"installed\",\n\t\t\t\t\t\t\troot_config, installed=True) in clean_set:\n\t\t\t\t\t\t\tlib_consumers.remove(consumer_dblink)\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tif lib_consumers:\n\t\t\t\t\t\tconsumers[lib] = lib_consumers\n\t\t\t\t\telse:\n\t\t\t\t\t\tdel consumers[lib]\n\t\t\t\tif not consumers:\n\t\t\t\t\tdel consumer_map[pkg]\n\n\t\tif consumer_map:\n\t\t\t# TODO: Implement a package set for rebuilding consumer packages.\n\n\t\t\tmsg = \"In order to avoid breakage of link level \" + \\\n\t\t\t\t\"dependencies, one or more packages will not be removed. \" + \\\n\t\t\t\t\"This can be solved by rebuilding \" + \\\n\t\t\t\t\"the packages that pulled them in.\"\n\n\t\t\tprefix = bad(\" * \")\n\t\t\twritemsg_level(\"\".join(prefix + \"%s\\n\" % line for \\\n\t\t\t\tline in textwrap.wrap(msg, 70)), level=logging.WARNING, noiselevel=-1)\n\n\t\t\tmsg = []\n\t\t\tfor pkg in sorted(consumer_map, key=cmp_sort_key(cmp_pkg_cpv)):\n\t\t\t\tconsumers = consumer_map[pkg]\n\t\t\t\tconsumer_libs = {}\n\t\t\t\tfor lib, lib_consumers in consumers.items():\n\t\t\t\t\tfor consumer in lib_consumers:\n\t\t\t\t\t\tconsumer_libs.setdefault(\n\t\t\t\t\t\t\tconsumer.mycpv, set()).add(linkmap.getSoname(lib))\n\t\t\t\tunique_consumers = set(chain(*consumers.values()))\n\t\t\t\tunique_consumers = sorted(consumer.mycpv \\\n\t\t\t\t\tfor consumer in unique_consumers)\n\t\t\t\tmsg.append(\"\")\n\t\t\t\tmsg.append(\"  %s pulled in by:\" % (pkg.cpv,))\n\t\t\t\tfor consumer in unique_consumers:\n\t\t\t\t\tlibs = consumer_libs[consumer]\n\t\t\t\t\tmsg.append(\"    %s needs %s\" % \\\n\t\t\t\t\t\t(consumer, ', '.join(sorted(libs))))\n\t\t\tmsg.append(\"\")\n\t\t\twritemsg_level(\"\".join(prefix + \"%s\\n\" % line for line in msg),\n\t\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\t\t\t# Add lib providers to the graph as children of lib consumers,\n\t\t\t# and also add any dependencies pulled in by the provider.\n\t\t\twritemsg_level(\">>> Adding lib providers to graph...\\n\")\n\n\t\t\tfor pkg, consumers in consumer_map.items():\n\t\t\t\tfor consumer_dblink in set(chain(*consumers.values())):\n\t\t\t\t\tconsumer_pkg = resolver._pkg(consumer_dblink.mycpv,\n\t\t\t\t\t\t\"installed\", root_config, installed=True)\n\t\t\t\t\tif not resolver._add_pkg(pkg,\n\t\t\t\t\t\tDependency(parent=consumer_pkg,\n\t\t\t\t\t\tpriority=UnmergeDepPriority(runtime=True,\n\t\t\t\t\t\t\truntime_slot_op=True),\n\t\t\t\t\t\troot=pkg.root)):\n\t\t\t\t\t\tresolver.display_problems()\n\t\t\t\t\t\treturn 1, [], False, 0\n\n\t\t\twritemsg_level(\"\\nCalculating dependencies  \")\n\t\t\tsuccess = resolver._complete_graph(\n\t\t\t\trequired_sets={eroot:required_sets})\n\t\t\twritemsg_level(\"\\b\\b... done!\\n\")\n\t\t\tresolver.display_problems()\n\t\t\tif not success:\n\t\t\t\treturn 1, [], False, 0\n\t\t\tif unresolved_deps():\n\t\t\t\treturn 1, [], False, 0\n\n\t\t\tgraph = resolver._dynamic_config.digraph.copy()\n\t\t\trequired_pkgs_total = 0\n\t\t\tfor node in graph:\n\t\t\t\tif isinstance(node, Package):\n\t\t\t\t\trequired_pkgs_total += 1\n\t\t\tcleanlist = create_cleanlist()\n\t\t\tif not cleanlist:\n\t\t\t\treturn 0, [], False, required_pkgs_total\n\t\t\tclean_set = set(cleanlist)\n\n\tif clean_set:\n\t\twritemsg_level(\">>> Calculating removal order...\\n\")\n\t\t# Use a topological sort to create an unmerge order such that\n\t\t# each package is unmerged before it's dependencies. This is\n\t\t# necessary to avoid breaking things that may need to run\n\t\t# during pkg_prerm or pkg_postrm phases.\n\n\t\t# Create a new graph to account for dependencies between the\n\t\t# packages being unmerged.\n\t\tgraph = digraph()\n\t\tdel cleanlist[:]\n\n\t\truntime = UnmergeDepPriority(runtime=True)\n\t\truntime_post = UnmergeDepPriority(runtime_post=True)\n\t\tbuildtime = UnmergeDepPriority(buildtime=True)\n\t\tpriority_map = {\n\t\t\t\"RDEPEND\": runtime,\n\t\t\t\"PDEPEND\": runtime_post,\n\t\t\t\"HDEPEND\": buildtime,\n\t\t\t\"DEPEND\": buildtime,\n\t\t}\n\n\t\tfor node in clean_set:\n\t\t\tgraph.add(node, None)\n\t\t\tfor dep_type in Package._dep_keys:\n\t\t\t\tdepstr = node._metadata[dep_type]\n\t\t\t\tif not depstr:\n\t\t\t\t\tcontinue\n\t\t\t\tpriority = priority_map[dep_type]\n\n\t\t\t\tif debug:\n\t\t\t\t\twritemsg_level(\"\\nParent:    %s\\n\"\n\t\t\t\t\t\t% (node,), noiselevel=-1, level=logging.DEBUG)\n\t\t\t\t\twritemsg_level(  \"Depstring: %s\\n\"\n\t\t\t\t\t\t% (depstr,), noiselevel=-1, level=logging.DEBUG)\n\t\t\t\t\twritemsg_level(  \"Priority:  %s\\n\"\n\t\t\t\t\t\t% (priority,), noiselevel=-1, level=logging.DEBUG)\n\n\t\t\t\ttry:\n\t\t\t\t\tatoms = resolver._select_atoms(eroot, depstr,\n\t\t\t\t\t\tmyuse=node.use.enabled, parent=node,\n\t\t\t\t\t\tpriority=priority)[node]\n\t\t\t\texcept portage.exception.InvalidDependString:\n\t\t\t\t\t# Ignore invalid deps of packages that will\n\t\t\t\t\t# be uninstalled anyway.\n\t\t\t\t\tcontinue\n\n\t\t\t\tif debug:\n\t\t\t\t\twritemsg_level(\"Candidates: [%s]\\n\" % \\\n\t\t\t\t\t\t', '.join(\"'%s'\" % (x,) for x in atoms),\n\t\t\t\t\t\tnoiselevel=-1, level=logging.DEBUG)\n\n\t\t\t\tfor atom in atoms:\n\t\t\t\t\tif not isinstance(atom, portage.dep.Atom):\n\t\t\t\t\t\t# Ignore invalid atoms returned from dep_check().\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif atom.blocker:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tmatches = vardb.match_pkgs(atom)\n\t\t\t\t\tif not matches:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tfor child_node in matches:\n\t\t\t\t\t\tif child_node in clean_set:\n\n\t\t\t\t\t\t\tmypriority = priority.copy()\n\t\t\t\t\t\t\tif atom.slot_operator_built:\n\t\t\t\t\t\t\t\tif mypriority.buildtime:\n\t\t\t\t\t\t\t\t\tmypriority.buildtime_slot_op = True\n\t\t\t\t\t\t\t\tif mypriority.runtime:\n\t\t\t\t\t\t\t\t\tmypriority.runtime_slot_op = True\n\n\t\t\t\t\t\t\tgraph.add(child_node, node, priority=mypriority)\n\n\t\tif debug:\n\t\t\twritemsg_level(\"\\nunmerge digraph:\\n\\n\",\n\t\t\t\tnoiselevel=-1, level=logging.DEBUG)\n\t\t\tgraph.debug_print()\n\t\t\twritemsg_level(\"\\n\", noiselevel=-1, level=logging.DEBUG)\n\n\t\tordered = True\n\t\tif len(graph.order) == len(graph.root_nodes()):\n\t\t\t# If there are no dependencies between packages\n\t\t\t# let unmerge() group them by cat\/pn.\n\t\t\tordered = False\n\t\t\tcleanlist = [pkg.cpv for pkg in graph.order]\n\t\telse:\n\t\t\t# Order nodes from lowest to highest overall reference count for\n\t\t\t# optimal root node selection (this can help minimize issues\n\t\t\t# with unaccounted implicit dependencies).\n\t\t\tnode_refcounts = {}\n\t\t\tfor node in graph.order:\n\t\t\t\tnode_refcounts[node] = len(graph.parent_nodes(node))\n\t\t\tdef cmp_reference_count(node1, node2):\n\t\t\t\treturn node_refcounts[node1] - node_refcounts[node2]\n\t\t\tgraph.order.sort(key=cmp_sort_key(cmp_reference_count))\n\n\t\t\tignore_priority_range = [None]\n\t\t\tignore_priority_range.extend(\n\t\t\t\trange(UnmergeDepPriority.MIN, UnmergeDepPriority.MAX + 1))\n\t\t\twhile graph:\n\t\t\t\tfor ignore_priority in ignore_priority_range:\n\t\t\t\t\tnodes = graph.root_nodes(ignore_priority=ignore_priority)\n\t\t\t\t\tif nodes:\n\t\t\t\t\t\tbreak\n\t\t\t\tif not nodes:\n\t\t\t\t\traise AssertionError(\"no root nodes\")\n\t\t\t\tif ignore_priority is not None:\n\t\t\t\t\t# Some deps have been dropped due to circular dependencies,\n\t\t\t\t\t# so only pop one node in order to minimize the number that\n\t\t\t\t\t# are dropped.\n\t\t\t\t\tdel nodes[1:]\n\t\t\t\tfor node in nodes:\n\t\t\t\t\tgraph.remove(node)\n\t\t\t\t\tcleanlist.append(node.cpv)\n\n\t\treturn 0, cleanlist, ordered, required_pkgs_total\n\treturn 0, [], False, required_pkgs_total\n\ndef action_deselect(settings, trees, opts, atoms):\n\tenter_invalid = '--ask-enter-invalid' in opts\n\troot_config = trees[settings['EROOT']]['root_config']\n\tworld_set = root_config.sets['selected']\n\tif not hasattr(world_set, 'update'):\n\t\twritemsg_level(\"World @selected set does not appear to be mutable.\\n\",\n\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\treturn 1\n\n\tpretend = '--pretend' in opts\n\tlocked = False\n\tif not pretend and hasattr(world_set, 'lock'):\n\t\tworld_set.lock()\n\t\tlocked = True\n\ttry:\n\t\tworld_set.load()\n\t\tworld_atoms = world_set.getAtoms()\n\t\tvardb = root_config.trees[\"vartree\"].dbapi\n\t\texpanded_atoms = set(atoms)\n\n\t\tfor atom in atoms:\n\t\t\tif not atom.startswith(SETPREFIX):\n\t\t\t\tif atom.cp.startswith(\"null\/\"):\n\t\t\t\t\t# try to expand category from world set\n\t\t\t\t\tnull_cat, pn = portage.catsplit(atom.cp)\n\t\t\t\t\tfor world_atom in world_atoms:\n\t\t\t\t\t\tcat, world_pn = portage.catsplit(world_atom.cp)\n\t\t\t\t\t\tif pn == world_pn:\n\t\t\t\t\t\t\texpanded_atoms.add(\n\t\t\t\t\t\t\t\tAtom(atom.replace(\"null\", cat, 1),\n\t\t\t\t\t\t\t\tallow_repo=True, allow_wildcard=True))\n\n\t\t\t\tfor cpv in vardb.match(atom):\n\t\t\t\t\tpkg = vardb._pkg_str(cpv, None)\n\t\t\t\t\texpanded_atoms.add(Atom(\"%s:%s\" % (pkg.cp, pkg.slot)))\n\n\t\tdiscard_atoms = set()\n\t\tfor atom in world_set:\n\t\t\tfor arg_atom in expanded_atoms:\n\t\t\t\tif arg_atom.startswith(SETPREFIX):\n\t\t\t\t\tif atom.startswith(SETPREFIX) and \\\n\t\t\t\t\t\targ_atom == atom:\n\t\t\t\t\t\tdiscard_atoms.add(atom)\n\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tif not atom.startswith(SETPREFIX) and \\\n\t\t\t\t\t\targ_atom.intersects(atom) and \\\n\t\t\t\t\t\tnot (arg_atom.slot and not atom.slot) and \\\n\t\t\t\t\t\tnot (arg_atom.repo and not atom.repo):\n\t\t\t\t\t\tdiscard_atoms.add(atom)\n\t\t\t\t\t\tbreak\n\t\tif discard_atoms:\n\t\t\tfor atom in sorted(discard_atoms):\n\n\t\t\t\tif pretend:\n\t\t\t\t\taction_desc = \"Would remove\"\n\t\t\t\telse:\n\t\t\t\t\taction_desc = \"Removing\"\n\n\t\t\t\tif atom.startswith(SETPREFIX):\n\t\t\t\t\tfilename = \"world_sets\"\n\t\t\t\telse:\n\t\t\t\t\tfilename = \"world\"\n\n\t\t\t\twritemsg_stdout(\n\t\t\t\t\t\">>> %s %s from \\\"%s\\\" favorites file...\\n\" %\n\t\t\t\t\t(action_desc, colorize(\"INFORM\", _unicode(atom)),\n\t\t\t\t\tfilename), noiselevel=-1)\n\n\t\t\tif '--ask' in opts:\n\t\t\t\tprompt = \"Would you like to remove these \" + \\\n\t\t\t\t\t\"packages from your world favorites?\"\n\t\t\t\tuq = UserQuery(opts)\n\t\t\t\tif uq.query(prompt, enter_invalid) == 'No':\n\t\t\t\t\treturn 128 + signal.SIGINT\n\n\t\t\tremaining = set(world_set)\n\t\t\tremaining.difference_update(discard_atoms)\n\t\t\tif not pretend:\n\t\t\t\tworld_set.replace(remaining)\n\t\telse:\n\t\t\tprint(\">>> No matching atoms found in \\\"world\\\" favorites file...\")\n\tfinally:\n\t\tif locked:\n\t\t\tworld_set.unlock()\n\treturn os.EX_OK\n\nclass _info_pkgs_ver(object):\n\tdef __init__(self, ver, repo_suffix, provide_suffix):\n\t\tself.ver = ver\n\t\tself.repo_suffix = repo_suffix\n\t\tself.provide_suffix = provide_suffix\n\n\tdef __lt__(self, other):\n\t\treturn portage.versions.vercmp(self.ver, other.ver) < 0\n\n\tdef toString(self):\n\t\t\"\"\"\n\t\tThis may return unicode if repo_name contains unicode.\n\t\tDon't use __str__ and str() since unicode triggers compatibility\n\t\tissues between python 2.x and 3.x.\n\t\t\"\"\"\n\t\treturn self.ver + self.repo_suffix + self.provide_suffix\n\ndef action_info(settings, trees, myopts, myfiles):\n\n\t# See if we can find any packages installed matching the strings\n\t# passed on the command line\n\tmypkgs = []\n\teroot = settings['EROOT']\n\tvardb = trees[eroot][\"vartree\"].dbapi\n\tportdb = trees[eroot]['porttree'].dbapi\n\tbindb = trees[eroot][\"bintree\"].dbapi\n\trepos = portdb.settings.repositories\n\tfor x in myfiles:\n\t\tany_match = False\n\t\tcp_exists = bool(vardb.match(x.cp))\n\t\tinstalled_match = vardb.match(x)\n\t\tfor installed in installed_match:\n\t\t\tmypkgs.append((installed, \"installed\"))\n\t\t\tany_match = True\n\n\t\tif any_match:\n\t\t\tcontinue\n\n\t\tfor db, pkg_type in ((portdb, \"ebuild\"), (bindb, \"binary\")):\n\t\t\tif pkg_type == \"binary\" and \"--usepkg\" not in myopts:\n\t\t\t\tcontinue\n\n\t\t\t# Use match instead of cp_list, to account for old-style virtuals.\n\t\t\tif not cp_exists and db.match(x.cp):\n\t\t\t\tcp_exists = True\n\t\t\t# Search for masked packages too.\n\t\t\tif not cp_exists and hasattr(db, \"xmatch\") and \\\n\t\t\t\tdb.xmatch(\"match-all\", x.cp):\n\t\t\t\tcp_exists = True\n\n\t\t\tmatches = db.match(x)\n\t\t\tmatches.reverse()\n\t\t\tfor match in matches:\n\t\t\t\tif pkg_type == \"binary\":\n\t\t\t\t\tif db.bintree.isremote(match):\n\t\t\t\t\t\tcontinue\n\t\t\t\tauxkeys = [\"EAPI\", \"DEFINED_PHASES\"]\n\t\t\t\tmetadata = dict(zip(auxkeys, db.aux_get(match, auxkeys)))\n\t\t\t\tif metadata[\"EAPI\"] not in (\"0\", \"1\", \"2\", \"3\") and \\\n\t\t\t\t\t\"info\" in metadata[\"DEFINED_PHASES\"].split():\n\t\t\t\t\tmypkgs.append((match, pkg_type))\n\t\t\t\t\tbreak\n\n\t\tif not cp_exists:\n\t\t\txinfo = '\"%s\"' % x.unevaluated_atom\n\t\t\t# Discard null\/ from failed cpv_expand category expansion.\n\t\t\txinfo = xinfo.replace(\"null\/\", \"\")\n\t\t\tif settings[\"ROOT\"] != \"\/\":\n\t\t\t\txinfo = \"%s for %s\" % (xinfo, eroot)\n\t\t\twritemsg(\"\\nemerge: there are no ebuilds to satisfy %s.\\n\" %\n\t\t\t\tcolorize(\"INFORM\", xinfo), noiselevel=-1)\n\n\t\t\tif myopts.get(\"--misspell-suggestions\", \"y\") != \"n\":\n\n\t\t\t\twritemsg(\"\\nemerge: searching for similar names...\"\n\t\t\t\t\t, noiselevel=-1)\n\n\t\t\t\tsearch_index = myopts.get(\"--search-index\", \"y\") != \"n\"\n\t\t\t\tdbs = [IndexedVardb(vardb) if search_index else vardb]\n\t\t\t\t#if \"--usepkgonly\" not in myopts:\n\t\t\t\tdbs.append(IndexedPortdb(portdb) if search_index else portdb)\n\t\t\t\tif \"--usepkg\" in myopts:\n\t\t\t\t\tdbs.append(bindb)\n\n\t\t\t\tmatches = similar_name_search(dbs, x)\n\n\t\t\t\tif len(matches) == 1:\n\t\t\t\t\twritemsg(\"\\nemerge: Maybe you meant \" + matches[0] + \"?\\n\"\n\t\t\t\t\t\t, noiselevel=-1)\n\t\t\t\telif len(matches) > 1:\n\t\t\t\t\twritemsg(\n\t\t\t\t\t\t\"\\nemerge: Maybe you meant any of these: %s?\\n\" % \\\n\t\t\t\t\t\t(\", \".join(matches),), noiselevel=-1)\n\t\t\t\telse:\n\t\t\t\t\t# Generally, this would only happen if\n\t\t\t\t\t# all dbapis are empty.\n\t\t\t\t\twritemsg(\" nothing similar found.\\n\"\n\t\t\t\t\t\t, noiselevel=-1)\n\n\t\t\treturn 1\n\n\toutput_buffer = []\n\tappend = output_buffer.append\n\troot_config = trees[settings['EROOT']]['root_config']\n\tchost = settings.get(\"CHOST\")\n\n\tappend(getportageversion(settings[\"PORTDIR\"], None,\n\t\tsettings.profile_path, chost,\n\t\ttrees[settings['EROOT']][\"vartree\"].dbapi))\n\n\theader_width = 65\n\theader_title = \"System Settings\"\n\tif myfiles:\n\t\tappend(header_width * \"=\")\n\t\tappend(header_title.rjust(int(header_width\/2 + len(header_title)\/2)))\n\tappend(header_width * \"=\")\n\tappend(\"System uname: %s\" % (platform.platform(aliased=1),))\n\n\tvm_info = get_vm_info()\n\tif \"ram.total\" in vm_info:\n\t\tline = \"%-9s %10d total\" % (\"KiB Mem:\", vm_info[\"ram.total\"] \/\/ 1024)\n\t\tif \"ram.free\" in vm_info:\n\t\t\tline += \",%10d free\" % (vm_info[\"ram.free\"] \/\/ 1024,)\n\t\tappend(line)\n\tif \"swap.total\" in vm_info:\n\t\tline = \"%-9s %10d total\" % (\"KiB Swap:\", vm_info[\"swap.total\"] \/\/ 1024)\n\t\tif \"swap.free\" in vm_info:\n\t\t\tline += \",%10d free\" % (vm_info[\"swap.free\"] \/\/ 1024,)\n\t\tappend(line)\n\n\tfor repo in repos:\n\t\tlast_sync = portage.grabfile(os.path.join(repo.location, \"metadata\", \"timestamp.chk\"))\n\t\tif last_sync:\n\t\t\tappend(\"Timestamp of repository %s: %s\" % (repo.name, last_sync[0]))\n\n\t# Searching contents for the \/bin\/sh provider is somewhat\n\t# slow. Therefore, use the basename of the symlink target\n\t# to locate the package. If this fails, then only the\n\t# basename of the symlink target will be displayed. So,\n\t# typical output is something like \"sh bash 4.2_p53\". Since\n\t# realpath is used to resolve symlinks recursively, this\n\t# approach is also able to handle multiple levels of symlinks\n\t# such as \/bin\/sh -> bb -> busybox. Note that we do not parse\n\t# the output of \"\/bin\/sh --version\" because many shells\n\t# do not have a --version option.\n\tbasename = os.path.basename(os.path.realpath(os.path.join(\n\t\tos.sep, portage.const.EPREFIX, \"bin\", \"sh\")))\n\ttry:\n\t\tAtom(\"null\/%s\" % basename)\n\texcept InvalidAtom:\n\t\tmatches = None\n\telse:\n\t\ttry:\n\t\t\t# Try a match against the basename, which should work for\n\t\t\t# busybox and most shells.\n\t\t\tmatches = (trees[trees._running_eroot][\"vartree\"].dbapi.\n\t\t\t\tmatch(basename))\n\t\texcept portage.exception.AmbiguousPackageName:\n\t\t\t# If the name is ambiguous, then restrict our match\n\t\t\t# to the app-shells category.\n\t\t\tmatches = (trees[trees._running_eroot][\"vartree\"].dbapi.\n\t\t\t\tmatch(\"app-shells\/%s\" % basename))\n\n\tif matches:\n\t\tpkg = matches[-1]\n\t\tname = pkg.cp\n\t\tversion = pkg.version\n\t\t# Omit app-shells category from the output.\n\t\tif name.startswith(\"app-shells\/\"):\n\t\t\tname = name[len(\"app-shells\/\"):]\n\t\tsh_str = \"%s %s\" % (name, version)\n\telse:\n\t\tsh_str = basename\n\n\tappend(\"sh %s\" % sh_str)\n\n\tld_names = []\n\tif chost:\n\t\tld_names.append(chost + \"-ld\")\n\tld_names.append(\"ld\")\n\tfor name in ld_names:\n\t\ttry:\n\t\t\tproc = subprocess.Popen([name, \"--version\"],\n\t\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\texcept OSError:\n\t\t\tpass\n\t\telse:\n\t\t\toutput = _unicode_decode(proc.communicate()[0]).splitlines()\n\t\t\tproc.wait()\n\t\t\tif proc.wait() == os.EX_OK and output:\n\t\t\t\tappend(\"ld %s\" % (output[0]))\n\t\t\t\tbreak\n\n\ttry:\n\t\tproc = subprocess.Popen([\"distcc\", \"--version\"],\n\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\texcept OSError:\n\t\toutput = (1, None)\n\telse:\n\t\toutput = _unicode_decode(proc.communicate()[0]).rstrip(\"\\n\")\n\t\toutput = (proc.wait(), output)\n\tif output[0] == os.EX_OK:\n\t\tdistcc_str = output[1].split(\"\\n\", 1)[0]\n\t\tif \"distcc\" in settings.features:\n\t\t\tdistcc_str += \" [enabled]\"\n\t\telse:\n\t\t\tdistcc_str += \" [disabled]\"\n\t\tappend(distcc_str)\n\n\ttry:\n\t\tproc = subprocess.Popen([\"ccache\", \"-V\"],\n\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\texcept OSError:\n\t\toutput = (1, None)\n\telse:\n\t\toutput = _unicode_decode(proc.communicate()[0]).rstrip(\"\\n\")\n\t\toutput = (proc.wait(), output)\n\tif output[0] == os.EX_OK:\n\t\tccache_str = output[1].split(\"\\n\", 1)[0]\n\t\tif \"ccache\" in settings.features:\n\t\t\tccache_str += \" [enabled]\"\n\t\telse:\n\t\t\tccache_str += \" [disabled]\"\n\t\tappend(ccache_str)\n\n\tmyvars  = [\"sys-devel\/autoconf\", \"sys-devel\/automake\", \"virtual\/os-headers\",\n\t           \"sys-devel\/binutils\", \"sys-devel\/libtool\",  \"dev-lang\/python\"]\n\tmyvars += portage.util.grabfile(settings[\"PORTDIR\"]+\"\/profiles\/info_pkgs\")\n\tatoms = []\n\tfor x in myvars:\n\t\ttry:\n\t\t\tx = Atom(x)\n\t\texcept InvalidAtom:\n\t\t\tappend(\"%-20s %s\" % (x+\":\", \"[NOT VALID]\"))\n\t\telse:\n\t\t\tfor atom in expand_new_virt(vardb, x):\n\t\t\t\tif not atom.blocker:\n\t\t\t\t\tatoms.append((x, atom))\n\n\tmyvars = sorted(set(atoms))\n\n\tcp_map = {}\n\tcp_max_len = 0\n\n\tfor orig_atom, x in myvars:\n\t\t\tpkg_matches = vardb.match(x)\n\n\t\t\tversions = []\n\t\t\tfor cpv in pkg_matches:\n\t\t\t\tmatched_cp = portage.versions.cpv_getkey(cpv)\n\t\t\t\tver = portage.versions.cpv_getversion(cpv)\n\t\t\t\tver_map = cp_map.setdefault(matched_cp, {})\n\t\t\t\tprev_match = ver_map.get(ver)\n\t\t\t\tif prev_match is not None:\n\t\t\t\t\tif prev_match.provide_suffix:\n\t\t\t\t\t\t# prefer duplicate matches that include\n\t\t\t\t\t\t# additional virtual provider info\n\t\t\t\t\t\tcontinue\n\n\t\t\t\tif len(matched_cp) > cp_max_len:\n\t\t\t\t\tcp_max_len = len(matched_cp)\n\t\t\t\trepo = vardb.aux_get(cpv, [\"repository\"])[0]\n\t\t\t\tif repo:\n\t\t\t\t\trepo_suffix = _repo_separator + repo\n\t\t\t\telse:\n\t\t\t\t\trepo_suffix = _repo_separator + \"<unknown repository>\"\n\n\t\t\t\tif matched_cp == orig_atom.cp:\n\t\t\t\t\tprovide_suffix = \"\"\n\t\t\t\telse:\n\t\t\t\t\tprovide_suffix = \" (%s)\" % (orig_atom,)\n\n\t\t\t\tver_map[ver] = _info_pkgs_ver(ver, repo_suffix, provide_suffix)\n\n\tfor cp in sorted(cp_map):\n\t\tversions = sorted(cp_map[cp].values())\n\t\tversions = \", \".join(ver.toString() for ver in versions)\n\t\tappend(\"%s %s\" % \\\n\t\t\t((cp + \":\").ljust(cp_max_len + 1), versions))\n\n\tappend(\"Repositories:\\n\")\n\tfor repo in repos:\n\t\tappend(repo.info_string())\n\n\tinstalled_sets = sorted(s for s in\n\t\troot_config.sets['selected'].getNonAtoms() if s.startswith(SETPREFIX))\n\tif installed_sets:\n\t\tsets_line = \"Installed sets: \"\n\t\tsets_line += \", \".join(installed_sets)\n\t\tappend(sets_line)\n\n\tif \"--verbose\" in myopts:\n\t\tmyvars = list(settings)\n\telse:\n\t\tmyvars = ['GENTOO_MIRRORS', 'CONFIG_PROTECT', 'CONFIG_PROTECT_MASK',\n\t\t          'DISTDIR', 'PKGDIR', 'PORTAGE_TMPDIR',\n\t\t          'PORTAGE_BUNZIP2_COMMAND',\n\t\t          'PORTAGE_BZIP2_COMMAND',\n\t\t          'USE', 'CHOST', 'CFLAGS', 'CXXFLAGS',\n\t\t          'ACCEPT_KEYWORDS', 'ACCEPT_LICENSE', 'FEATURES',\n\t\t          'EMERGE_DEFAULT_OPTS']\n\n\t\tmyvars.extend(portage.util.grabfile(settings[\"PORTDIR\"]+\"\/profiles\/info_vars\"))\n\n\tmyvars_ignore_defaults = {\n\t\t'PORTAGE_BZIP2_COMMAND' : 'bzip2',\n\t}\n\n\tskipped_vars = ['PORTAGE_REPOSITORIES']\n\t# Deprecated variables\n\tskipped_vars.extend(('PORTDIR', 'PORTDIR_OVERLAY', 'SYNC'))\n\n\tmyvars = set(myvars)\n\tmyvars.difference_update(skipped_vars)\n\tmyvars = sorted(myvars)\n\n\tuse_expand = settings.get('USE_EXPAND', '').split()\n\tuse_expand.sort()\n\tunset_vars = []\n\n\tfor k in myvars:\n\t\tv = settings.get(k)\n\t\tif v is not None:\n\t\t\tif k != \"USE\":\n\t\t\t\tdefault = myvars_ignore_defaults.get(k)\n\t\t\t\tif default is not None and \\\n\t\t\t\t\tdefault == v:\n\t\t\t\t\tcontinue\n\t\t\t\tappend('%s=\"%s\"' % (k, v))\n\t\t\telse:\n\t\t\t\tuse = set(v.split())\n\t\t\t\tfor varname in use_expand:\n\t\t\t\t\tflag_prefix = varname.lower() + \"_\"\n\t\t\t\t\tfor f in list(use):\n\t\t\t\t\t\tif f.startswith(flag_prefix):\n\t\t\t\t\t\t\tuse.remove(f)\n\t\t\t\tuse = list(use)\n\t\t\t\tuse.sort()\n\t\t\t\tuse = ['USE=\"%s\"' % \" \".join(use)]\n\t\t\t\tfor varname in use_expand:\n\t\t\t\t\tmyval = settings.get(varname)\n\t\t\t\t\tif myval:\n\t\t\t\t\t\tuse.append('%s=\"%s\"' % (varname, myval))\n\t\t\t\tappend(\" \".join(use))\n\t\telse:\n\t\t\tunset_vars.append(k)\n\tif unset_vars:\n\t\tappend(\"Unset:  \"+\", \".join(unset_vars))\n\tappend(\"\")\n\tappend(\"\")\n\twritemsg_stdout(\"\\n\".join(output_buffer),\n\t\tnoiselevel=-1)\n\tdel output_buffer[:]\n\n\t# If some packages were found...\n\tif mypkgs:\n\t\t# Get our global settings (we only print stuff if it varies from\n\t\t# the current config)\n\t\tmydesiredvars = [ 'CHOST', 'CFLAGS', 'CXXFLAGS', 'LDFLAGS' ]\n\t\tauxkeys = mydesiredvars + list(vardb._aux_cache_keys)\n\t\tauxkeys.append('DEFINED_PHASES')\n\t\tpkgsettings = portage.config(clone=settings)\n\n\t\t# Loop through each package\n\t\t# Only print settings if they differ from global settings\n\t\theader_title = \"Package Settings\"\n\t\tappend(header_width * \"=\")\n\t\tappend(header_title.rjust(int(header_width\/2 + len(header_title)\/2)))\n\t\tappend(header_width * \"=\")\n\t\tappend(\"\")\n\t\twritemsg_stdout(\"\\n\".join(output_buffer),\n\t\t\tnoiselevel=-1)\n\t\tdel output_buffer[:]\n\n\t\tout = portage.output.EOutput()\n\t\tfor mypkg in mypkgs:\n\t\t\tcpv = mypkg[0]\n\t\t\tpkg_type = mypkg[1]\n\t\t\t# Get all package specific variables\n\t\t\tif pkg_type == \"installed\":\n\t\t\t\tmetadata = dict(zip(auxkeys, vardb.aux_get(cpv, auxkeys)))\n\t\t\telif pkg_type == \"ebuild\":\n\t\t\t\tmetadata = dict(zip(auxkeys, portdb.aux_get(cpv, auxkeys)))\n\t\t\telif pkg_type == \"binary\":\n\t\t\t\tmetadata = dict(zip(auxkeys, bindb.aux_get(cpv, auxkeys)))\n\n\t\t\tpkg = Package(built=(pkg_type!=\"ebuild\"), cpv=cpv,\n\t\t\t\tinstalled=(pkg_type==\"installed\"), metadata=zip(Package.metadata_keys,\n\t\t\t\t(metadata.get(x, '') for x in Package.metadata_keys)),\n\t\t\t\troot_config=root_config, type_name=pkg_type)\n\n\t\t\tif pkg_type == \"installed\":\n\t\t\t\tappend(\"\\n%s was built with the following:\" % \\\n\t\t\t\t\tcolorize(\"INFORM\", str(pkg.cpv + _repo_separator + pkg.repo)))\n\t\t\telif pkg_type == \"ebuild\":\n\t\t\t\tappend(\"\\n%s would be built with the following:\" % \\\n\t\t\t\t\tcolorize(\"INFORM\", str(pkg.cpv + _repo_separator + pkg.repo)))\n\t\t\telif pkg_type == \"binary\":\n\t\t\t\tappend(\"\\n%s (non-installed binary) was built with the following:\" % \\\n\t\t\t\t\tcolorize(\"INFORM\", str(pkg.cpv + _repo_separator + pkg.repo)))\n\n\t\t\tappend('%s' % pkg_use_display(pkg, myopts))\n\t\t\tif pkg_type == \"installed\":\n\t\t\t\tfor myvar in mydesiredvars:\n\t\t\t\t\tif metadata[myvar].split() != settings.get(myvar, '').split():\n\t\t\t\t\t\tappend(\"%s=\\\"%s\\\"\" % (myvar, metadata[myvar]))\n\t\t\tappend(\"\")\n\t\t\tappend(\"\")\n\t\t\twritemsg_stdout(\"\\n\".join(output_buffer),\n\t\t\t\tnoiselevel=-1)\n\t\t\tdel output_buffer[:]\n\n\t\t\tif metadata['DEFINED_PHASES']:\n\t\t\t\tif 'info' not in metadata['DEFINED_PHASES'].split():\n\t\t\t\t\tcontinue\n\n\t\t\twritemsg_stdout(\">>> Attempting to run pkg_info() for '%s'\\n\"\n\t\t\t\t% pkg.cpv, noiselevel=-1)\n\n\t\t\tif pkg_type == \"installed\":\n\t\t\t\tebuildpath = vardb.findname(pkg.cpv)\n\t\t\telif pkg_type == \"ebuild\":\n\t\t\t\tebuildpath = portdb.findname(pkg.cpv, myrepo=pkg.repo)\n\t\t\telif pkg_type == \"binary\":\n\t\t\t\ttbz2_file = bindb.bintree.getname(pkg.cpv)\n\t\t\t\tebuild_file_name = pkg.cpv.split(\"\/\")[1] + \".ebuild\"\n\t\t\t\tebuild_file_contents = portage.xpak.tbz2(tbz2_file).getfile(ebuild_file_name)\n\t\t\t\ttmpdir = tempfile.mkdtemp()\n\t\t\t\tebuildpath = os.path.join(tmpdir, ebuild_file_name)\n\t\t\t\tfile = open(ebuildpath, 'w')\n\t\t\t\tfile.write(ebuild_file_contents)\n\t\t\t\tfile.close()\n\n\t\t\tif not ebuildpath or not os.path.exists(ebuildpath):\n\t\t\t\tout.ewarn(\"No ebuild found for '%s'\" % pkg.cpv)\n\t\t\t\tcontinue\n\n\t\t\tif pkg_type == \"installed\":\n\t\t\t\tportage.doebuild(ebuildpath, \"info\", settings=pkgsettings,\n\t\t\t\t\tdebug=(settings.get(\"PORTAGE_DEBUG\", \"\") == 1),\n\t\t\t\t\tmydbapi=trees[settings['EROOT']][\"vartree\"].dbapi,\n\t\t\t\t\ttree=\"vartree\")\n\t\t\telif pkg_type == \"ebuild\":\n\t\t\t\tportage.doebuild(ebuildpath, \"info\", settings=pkgsettings,\n\t\t\t\t\tdebug=(settings.get(\"PORTAGE_DEBUG\", \"\") == 1),\n\t\t\t\t\tmydbapi=trees[settings['EROOT']]['porttree'].dbapi,\n\t\t\t\t\ttree=\"porttree\")\n\t\t\telif pkg_type == \"binary\":\n\t\t\t\tportage.doebuild(ebuildpath, \"info\", settings=pkgsettings,\n\t\t\t\t\tdebug=(settings.get(\"PORTAGE_DEBUG\", \"\") == 1),\n\t\t\t\t\tmydbapi=trees[settings['EROOT']][\"bintree\"].dbapi,\n\t\t\t\t\ttree=\"bintree\")\n\t\t\t\tshutil.rmtree(tmpdir)\n\ndef action_regen(settings, portdb, max_jobs, max_load):\n\txterm_titles = \"notitles\" not in settings.features\n\temergelog(xterm_titles, \" === regen\")\n\t#regenerate cache entries\n\tsys.stdout.flush()\n\n\tregen = MetadataRegen(portdb, max_jobs=max_jobs,\n\t\tmax_load=max_load, main=True)\n\n\tsignum = run_main_scheduler(regen)\n\tif signum is not None:\n\t\tsys.exit(128 + signum)\n\n\tportage.writemsg_stdout(\"done!\\n\")\n\treturn regen.returncode\n\ndef action_search(root_config, myopts, myfiles, spinner):\n\tif not myfiles:\n\t\tprint(\"emerge: no search terms provided.\")\n\telse:\n\t\tsearchinstance = search(root_config,\n\t\t\tspinner, \"--searchdesc\" in myopts,\n\t\t\t\"--quiet\" not in myopts, \"--usepkg\" in myopts,\n\t\t\t\"--usepkgonly\" in myopts,\n\t\t\tsearch_index=myopts.get(\"--search-index\", \"y\") != \"n\",\n\t\t\tsearch_similarity=myopts.get(\"--search-similarity\"),\n\t\t\tfuzzy=myopts.get(\"--fuzzy-search\") != \"n\",\n\t\t\t)\n\t\tfor mysearch in myfiles:\n\t\t\ttry:\n\t\t\t\tsearchinstance.execute(mysearch)\n\t\t\texcept re.error as comment:\n\t\t\t\tprint(\"\\n!!! Regular expression error in \\\"%s\\\": %s\" % ( mysearch, comment ))\n\t\t\t\tsys.exit(1)\n\t\t\tsearchinstance.output()\n\ndef action_sync(emerge_config, trees=DeprecationWarning,\n\tmtimedb=DeprecationWarning, opts=DeprecationWarning,\n\taction=DeprecationWarning):\n\n\tif not isinstance(emerge_config, _emerge_config):\n\t\twarnings.warn(\"_emerge.actions.action_sync() now expects \"\n\t\t\t\"an _emerge_config instance as the first parameter\",\n\t\t\tDeprecationWarning, stacklevel=2)\n\t\temerge_config = load_emerge_config(\n\t\t\taction=action, args=[], trees=trees, opts=opts)\n\n\tsyncer = SyncRepos(emerge_config)\n\n\n\tretvals = syncer.auto_sync(options={'return-messages': False})\n\n\tif retvals:\n\t\treturn retvals[0][1]\n\treturn os.EX_OK\n\n\ndef action_uninstall(settings, trees, ldpath_mtimes,\n\topts, action, files, spinner):\n\t# For backward compat, some actions do not require leading '='.\n\tignore_missing_eq = action in ('clean', 'rage-clean', 'unmerge')\n\troot = settings['ROOT']\n\teroot = settings['EROOT']\n\tvardb = trees[settings['EROOT']]['vartree'].dbapi\n\tvalid_atoms = []\n\tlookup_owners = []\n\n\t# Ensure atoms are valid before calling unmerge().\n\t# For backward compat, leading '=' is not required.\n\tfor x in files:\n\t\tif is_valid_package_atom(x, allow_repo=True) or \\\n\t\t\t(ignore_missing_eq and is_valid_package_atom('=' + x)):\n\n\t\t\ttry:\n\t\t\t\tatom = dep_expand(x, mydb=vardb, settings=settings)\n\t\t\texcept portage.exception.AmbiguousPackageName as e:\n\t\t\t\tmsg = \"The short ebuild name \\\"\" + x + \\\n\t\t\t\t\t\"\\\" is ambiguous.  Please specify \" + \\\n\t\t\t\t\t\"one of the following \" + \\\n\t\t\t\t\t\"fully-qualified ebuild names instead:\"\n\t\t\t\tfor line in textwrap.wrap(msg, 70):\n\t\t\t\t\twritemsg_level(\"!!! %s\\n\" % (line,),\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\tfor i in e.args[0]:\n\t\t\t\t\twritemsg_level(\"    %s\\n\" % colorize(\"INFORM\", i),\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\twritemsg_level(\"\\n\", level=logging.ERROR, noiselevel=-1)\n\t\t\t\treturn 1\n\t\t\telse:\n\t\t\t\tif atom.use and atom.use.conditional:\n\t\t\t\t\twritemsg_level(\n\t\t\t\t\t\t(\"\\n\\n!!! '%s' contains a conditional \" + \\\n\t\t\t\t\t\t\"which is not allowed.\\n\") % (x,),\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\twritemsg_level(\n\t\t\t\t\t\t\"!!! Please check ebuild(5) for full details.\\n\",\n\t\t\t\t\t\tlevel=logging.ERROR)\n\t\t\t\t\treturn 1\n\t\t\t\tvalid_atoms.append(atom)\n\n\t\telif x.startswith(os.sep):\n\t\t\tif not x.startswith(eroot):\n\t\t\t\twritemsg_level((\"!!! '%s' does not start with\" + \\\n\t\t\t\t\t\" $EROOT.\\n\") % x, level=logging.ERROR, noiselevel=-1)\n\t\t\t\treturn 1\n\t\t\t# Queue these up since it's most efficient to handle\n\t\t\t# multiple files in a single iter_owners() call.\n\t\t\tlookup_owners.append(x)\n\n\t\telif x.startswith(SETPREFIX) and action == \"deselect\":\n\t\t\tvalid_atoms.append(x)\n\n\t\telif \"*\" in x:\n\t\t\ttry:\n\t\t\t\text_atom = Atom(x, allow_repo=True, allow_wildcard=True)\n\t\t\texcept InvalidAtom:\n\t\t\t\tmsg = []\n\t\t\t\tmsg.append(\"'%s' is not a valid package atom.\" % (x,))\n\t\t\t\tmsg.append(\"Please check ebuild(5) for full details.\")\n\t\t\t\twritemsg_level(\"\".join(\"!!! %s\\n\" % line for line in msg),\n\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\treturn 1\n\n\t\t\tfor cpv in vardb.cpv_all():\n\t\t\t\tif portage.match_from_list(ext_atom, [cpv]):\n\t\t\t\t\trequire_metadata = False\n\t\t\t\t\tatom = portage.cpv_getkey(cpv)\n\t\t\t\t\tif ext_atom.operator == '=*':\n\t\t\t\t\t\tatom = \"=\" + atom + \"-\" + \\\n\t\t\t\t\t\t\tportage.versions.cpv_getversion(cpv)\n\t\t\t\t\tif ext_atom.slot:\n\t\t\t\t\t\tatom += _slot_separator + ext_atom.slot\n\t\t\t\t\t\trequire_metadata = True\n\t\t\t\t\tif ext_atom.repo:\n\t\t\t\t\t\tatom += _repo_separator + ext_atom.repo\n\t\t\t\t\t\trequire_metadata = True\n\n\t\t\t\t\tatom = Atom(atom, allow_repo=True)\n\t\t\t\t\tif require_metadata:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tcpv = vardb._pkg_str(cpv, ext_atom.repo)\n\t\t\t\t\t\texcept (KeyError, InvalidData):\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif not portage.match_from_list(atom, [cpv]):\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tvalid_atoms.append(atom)\n\n\t\telse:\n\t\t\tmsg = []\n\t\t\tmsg.append(\"'%s' is not a valid package atom.\" % (x,))\n\t\t\tmsg.append(\"Please check ebuild(5) for full details.\")\n\t\t\twritemsg_level(\"\".join(\"!!! %s\\n\" % line for line in msg),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\treturn 1\n\n\tif lookup_owners:\n\t\trelative_paths = []\n\t\tsearch_for_multiple = False\n\t\tif len(lookup_owners) > 1:\n\t\t\tsearch_for_multiple = True\n\n\t\tfor x in lookup_owners:\n\t\t\tif not search_for_multiple and os.path.isdir(x):\n\t\t\t\tsearch_for_multiple = True\n\t\t\trelative_paths.append(x[len(root)-1:])\n\n\t\towners = set()\n\t\tfor pkg, relative_path in \\\n\t\t\tvardb._owners.iter_owners(relative_paths):\n\t\t\towners.add(pkg.mycpv)\n\t\t\tif not search_for_multiple:\n\t\t\t\tbreak\n\n\t\tif owners:\n\t\t\tfor cpv in owners:\n\t\t\t\tpkg = vardb._pkg_str(cpv, None)\n\t\t\t\tatom = '%s:%s' % (pkg.cp, pkg.slot)\n\t\t\t\tvalid_atoms.append(portage.dep.Atom(atom))\n\t\telse:\n\t\t\twritemsg_level((\"!!! '%s' is not claimed \" + \\\n\t\t\t\t\"by any package.\\n\") % lookup_owners[0],\n\t\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\tif files and not valid_atoms:\n\t\treturn 1\n\n\tif action == 'unmerge' and \\\n\t\t'--quiet' not in opts and \\\n\t\t'--quiet-unmerge-warn' not in opts:\n\t\tmsg = \"This action can remove important packages! \" + \\\n\t\t\t\"In order to be safer, use \" + \\\n\t\t\t\"`emerge -pv --depclean <atom>` to check for \" + \\\n\t\t\t\"reverse dependencies before removing packages.\"\n\t\tout = portage.output.EOutput()\n\t\tfor line in textwrap.wrap(msg, 72):\n\t\t\tout.ewarn(line)\n\n\tif action == 'deselect':\n\t\treturn action_deselect(settings, trees, opts, valid_atoms)\n\n\t# Use the same logic as the Scheduler class to trigger redirection\n\t# of ebuild pkg_prerm\/postrm phase output to logs as appropriate\n\t# for options such as --jobs, --quiet and --quiet-build.\n\tmax_jobs = opts.get(\"--jobs\", 1)\n\tbackground = (max_jobs is True or max_jobs > 1 or\n\t\t\"--quiet\" in opts or opts.get(\"--quiet-build\") == \"y\")\n\tsched_iface = SchedulerInterface(global_event_loop(),\n\t\tis_background=lambda: background)\n\n\tif background:\n\t\tsettings.unlock()\n\t\tsettings[\"PORTAGE_BACKGROUND\"] = \"1\"\n\t\tsettings.backup_changes(\"PORTAGE_BACKGROUND\")\n\t\tsettings.lock()\n\n\tif action in ('clean', 'rage-clean', 'unmerge') or \\\n\t\t(action == 'prune' and \"--nodeps\" in opts):\n\t\t# When given a list of atoms, unmerge them in the order given.\n\t\tordered = action in ('rage-clean', 'unmerge')\n\t\trval = unmerge(trees[settings['EROOT']]['root_config'], opts, action,\n\t\t\tvalid_atoms, ldpath_mtimes, ordered=ordered,\n\t\t\tscheduler=sched_iface)\n\telse:\n\t\trval = action_depclean(settings, trees, ldpath_mtimes,\n\t\t\topts, action, valid_atoms, spinner,\n\t\t\tscheduler=sched_iface)\n\n\treturn rval\n\ndef adjust_configs(myopts, trees):\n\tfor myroot in trees:\n\t\tmysettings =  trees[myroot][\"vartree\"].settings\n\t\tmysettings.unlock()\n\t\tadjust_config(myopts, mysettings)\n\t\tmysettings.lock()\n\ndef adjust_config(myopts, settings):\n\t\"\"\"Make emerge specific adjustments to the config.\"\"\"\n\n\t# Kill noauto as it will break merges otherwise.\n\tif \"noauto\" in settings.features:\n\t\tsettings.features.remove('noauto')\n\n\tfail_clean = myopts.get('--fail-clean')\n\tif fail_clean is not None:\n\t\tif fail_clean is True and \\\n\t\t\t'fail-clean' not in settings.features:\n\t\t\tsettings.features.add('fail-clean')\n\t\telif fail_clean == 'n' and \\\n\t\t\t'fail-clean' in settings.features:\n\t\t\tsettings.features.remove('fail-clean')\n\n\tCLEAN_DELAY = 5\n\ttry:\n\t\tCLEAN_DELAY = int(settings.get(\"CLEAN_DELAY\", str(CLEAN_DELAY)))\n\texcept ValueError as e:\n\t\tportage.writemsg(\"!!! %s\\n\" % str(e), noiselevel=-1)\n\t\tportage.writemsg(\"!!! Unable to parse integer: CLEAN_DELAY='%s'\\n\" % \\\n\t\t\tsettings[\"CLEAN_DELAY\"], noiselevel=-1)\n\tsettings[\"CLEAN_DELAY\"] = str(CLEAN_DELAY)\n\tsettings.backup_changes(\"CLEAN_DELAY\")\n\n\tEMERGE_WARNING_DELAY = 10\n\ttry:\n\t\tEMERGE_WARNING_DELAY = int(settings.get(\n\t\t\t\"EMERGE_WARNING_DELAY\", str(EMERGE_WARNING_DELAY)))\n\texcept ValueError as e:\n\t\tportage.writemsg(\"!!! %s\\n\" % str(e), noiselevel=-1)\n\t\tportage.writemsg(\"!!! Unable to parse integer: EMERGE_WARNING_DELAY='%s'\\n\" % \\\n\t\t\tsettings[\"EMERGE_WARNING_DELAY\"], noiselevel=-1)\n\tsettings[\"EMERGE_WARNING_DELAY\"] = str(EMERGE_WARNING_DELAY)\n\tsettings.backup_changes(\"EMERGE_WARNING_DELAY\")\n\n\tbuildpkg = myopts.get(\"--buildpkg\")\n\tif buildpkg is True:\n\t\tsettings.features.add(\"buildpkg\")\n\telif buildpkg == 'n':\n\t\tsettings.features.discard(\"buildpkg\")\n\n\tif \"--quiet\" in myopts:\n\t\tsettings[\"PORTAGE_QUIET\"]=\"1\"\n\t\tsettings.backup_changes(\"PORTAGE_QUIET\")\n\n\tif \"--verbose\" in myopts:\n\t\tsettings[\"PORTAGE_VERBOSE\"] = \"1\"\n\t\tsettings.backup_changes(\"PORTAGE_VERBOSE\")\n\n\t# Set so that configs will be merged regardless of remembered status\n\tif (\"--noconfmem\" in myopts):\n\t\tsettings[\"NOCONFMEM\"]=\"1\"\n\t\tsettings.backup_changes(\"NOCONFMEM\")\n\n\t# Set various debug markers... They should be merged somehow.\n\tPORTAGE_DEBUG = 0\n\ttry:\n\t\tPORTAGE_DEBUG = int(settings.get(\"PORTAGE_DEBUG\", str(PORTAGE_DEBUG)))\n\t\tif PORTAGE_DEBUG not in (0, 1):\n\t\t\tportage.writemsg(\"!!! Invalid value: PORTAGE_DEBUG='%i'\\n\" % \\\n\t\t\t\tPORTAGE_DEBUG, noiselevel=-1)\n\t\t\tportage.writemsg(\"!!! PORTAGE_DEBUG must be either 0 or 1\\n\",\n\t\t\t\tnoiselevel=-1)\n\t\t\tPORTAGE_DEBUG = 0\n\texcept ValueError as e:\n\t\tportage.writemsg(\"!!! %s\\n\" % str(e), noiselevel=-1)\n\t\tportage.writemsg(\"!!! Unable to parse integer: PORTAGE_DEBUG='%s'\\n\" %\\\n\t\t\tsettings[\"PORTAGE_DEBUG\"], noiselevel=-1)\n\t\tdel e\n\tif \"--debug\" in myopts:\n\t\tPORTAGE_DEBUG = 1\n\tsettings[\"PORTAGE_DEBUG\"] = str(PORTAGE_DEBUG)\n\tsettings.backup_changes(\"PORTAGE_DEBUG\")\n\n\tif settings.get(\"NOCOLOR\") not in (\"yes\",\"true\"):\n\t\tportage.output.havecolor = 1\n\n\t# The explicit --color < y | n > option overrides the NOCOLOR environment\n\t# variable and stdout auto-detection.\n\tif \"--color\" in myopts:\n\t\tif \"y\" == myopts[\"--color\"]:\n\t\t\tportage.output.havecolor = 1\n\t\t\tsettings[\"NOCOLOR\"] = \"false\"\n\t\telse:\n\t\t\tportage.output.havecolor = 0\n\t\t\tsettings[\"NOCOLOR\"] = \"true\"\n\t\tsettings.backup_changes(\"NOCOLOR\")\n\telif settings.get('TERM') == 'dumb' or \\\n\t\tnot sys.stdout.isatty():\n\t\tportage.output.havecolor = 0\n\t\tsettings[\"NOCOLOR\"] = \"true\"\n\t\tsettings.backup_changes(\"NOCOLOR\")\n\n\tif \"--pkg-format\" in myopts:\n\t\tsettings[\"PORTAGE_BINPKG_FORMAT\"] = myopts[\"--pkg-format\"]\n\t\tsettings.backup_changes(\"PORTAGE_BINPKG_FORMAT\")\n\ndef display_missing_pkg_set(root_config, set_name):\n\n\tmsg = []\n\tmsg.append((\"emerge: There are no sets to satisfy '%s'. \" + \\\n\t\t\"The following sets exist:\") % \\\n\t\tcolorize(\"INFORM\", set_name))\n\tmsg.append(\"\")\n\n\tfor s in sorted(root_config.sets):\n\t\tmsg.append(\"    %s\" % s)\n\tmsg.append(\"\")\n\n\twritemsg_level(\"\".join(\"%s\\n\" % l for l in msg),\n\t\tlevel=logging.ERROR, noiselevel=-1)\n\ndef relative_profile_path(portdir, abs_profile):\n\trealpath = os.path.realpath(abs_profile)\n\tbasepath   = os.path.realpath(os.path.join(portdir, \"profiles\"))\n\tif realpath.startswith(basepath):\n\t\tprofilever = realpath[1 + len(basepath):]\n\telse:\n\t\tprofilever = None\n\treturn profilever\n\ndef getportageversion(portdir, _unused, profile, chost, vardb):\n\tpythonver = 'python %d.%d.%d-%s-%d' % sys.version_info[:]\n\tprofilever = None\n\trepositories = vardb.settings.repositories\n\tif profile:\n\t\tprofilever = relative_profile_path(portdir, profile)\n\t\tif profilever is None:\n\t\t\ttry:\n\t\t\t\tfor parent in portage.grabfile(\n\t\t\t\t\tos.path.join(profile, 'parent')):\n\t\t\t\t\tprofilever = relative_profile_path(portdir,\n\t\t\t\t\t\tos.path.join(profile, parent))\n\t\t\t\t\tif profilever is not None:\n\t\t\t\t\t\tbreak\n\t\t\t\t\tcolon = parent.find(\":\")\n\t\t\t\t\tif colon != -1:\n\t\t\t\t\t\tp_repo_name = parent[:colon]\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tp_repo_loc = \\\n\t\t\t\t\t\t\t\trepositories.get_location_for_name(p_repo_name)\n\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\tpass\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprofilever = relative_profile_path(p_repo_loc,\n\t\t\t\t\t\t\t\tos.path.join(p_repo_loc, 'profiles',\n\t\t\t\t\t\t\t\t\tparent[colon+1:]))\n\t\t\t\t\t\t\tif profilever is not None:\n\t\t\t\t\t\t\t\tbreak\n\t\t\texcept portage.exception.PortageException:\n\t\t\t\tpass\n\n\t\t\tif profilever is None:\n\t\t\t\ttry:\n\t\t\t\t\tprofilever = \"!\" + os.readlink(profile)\n\t\t\t\texcept (OSError):\n\t\t\t\t\tpass\n\n\tif profilever is None:\n\t\tprofilever = \"unavailable\"\n\n\tlibcver = []\n\tlibclist = set()\n\tfor atom in expand_new_virt(vardb, portage.const.LIBC_PACKAGE_ATOM):\n\t\tif not atom.blocker:\n\t\t\tlibclist.update(vardb.match(atom))\n\tif libclist:\n\t\tfor cpv in sorted(libclist):\n\t\t\tlibc_split = portage.catpkgsplit(cpv)[1:]\n\t\t\tif libc_split[-1] == \"r0\":\n\t\t\t\tlibc_split = libc_split[:-1]\n\t\t\tlibcver.append(\"-\".join(libc_split))\n\telse:\n\t\tlibcver = [\"unavailable\"]\n\n\tgccver = getgccversion(chost)\n\tunameout=platform.release()+\" \"+platform.machine()\n\n\treturn \"Portage %s (%s, %s, %s, %s, %s)\" % \\\n\t\t(portage.VERSION, pythonver, profilever, gccver, \",\".join(libcver), unameout)\n\n\nclass _emerge_config(SlotObject):\n\n\t__slots__ = ('action', 'args', 'opts',\n\t\t'running_config', 'target_config', 'trees')\n\n\t# Support unpack as tuple, for load_emerge_config backward compatibility.\n\tdef __iter__(self):\n\t\tyield self.target_config.settings\n\t\tyield self.trees\n\t\tyield self.target_config.mtimedb\n\n\tdef __getitem__(self, index):\n\t\treturn list(self)[index]\n\n\tdef __len__(self):\n\t\treturn 3\n\ndef load_emerge_config(emerge_config=None, **kargs):\n\n\tif emerge_config is None:\n\t\temerge_config = _emerge_config(**kargs)\n\n\tkwargs = {}\n\tfor k, envvar in ((\"config_root\", \"PORTAGE_CONFIGROOT\"), (\"target_root\", \"ROOT\"),\n\t\t\t(\"eprefix\", \"EPREFIX\")):\n\t\tv = os.environ.get(envvar, None)\n\t\tif v and v.strip():\n\t\t\tkwargs[k] = v\n\temerge_config.trees = portage.create_trees(trees=emerge_config.trees,\n\t\t\t\t**kwargs)\n\n\tfor root_trees in emerge_config.trees.values():\n\t\tsettings = root_trees[\"vartree\"].settings\n\t\tsettings._init_dirs()\n\t\tsetconfig = load_default_config(settings, root_trees)\n\t\troot_config = RootConfig(settings, root_trees, setconfig)\n\t\tif \"root_config\" in root_trees:\n\t\t\t# Propagate changes to the existing instance,\n\t\t\t# which may be referenced by a depgraph.\n\t\t\troot_trees[\"root_config\"].update(root_config)\n\t\telse:\n\t\t\troot_trees[\"root_config\"] = root_config\n\n\ttarget_eroot = emerge_config.trees._target_eroot\n\temerge_config.target_config = \\\n\t\temerge_config.trees[target_eroot]['root_config']\n\temerge_config.target_config.mtimedb = portage.MtimeDB(\n\t\tos.path.join(target_eroot, portage.CACHE_PATH, \"mtimedb\"))\n\temerge_config.running_config = emerge_config.trees[\n\t\temerge_config.trees._running_eroot]['root_config']\n\tQueryCommand._db = emerge_config.trees\n\n\treturn emerge_config\n\ndef getgccversion(chost=None):\n\t\"\"\"\n\trtype: C{str}\n\treturn:  the current in-use gcc version\n\t\"\"\"\n\n\tgcc_ver_command = ['gcc', '-dumpversion']\n\tgcc_ver_prefix = 'gcc-'\n\n\tgcc_not_found_error = red(\n\t\"!!! No gcc found. You probably need to 'source \/etc\/profile'\\n\" +\n\t\"!!! to update the environment of this terminal and possibly\\n\" +\n\t\"!!! other terminals also.\\n\"\n\t)\n\n\tif chost:\n\t\ttry:\n\t\t\tproc = subprocess.Popen([\"gcc-config\", \"-c\"],\n\t\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\texcept OSError:\n\t\t\tmyoutput = None\n\t\t\tmystatus = 1\n\t\telse:\n\t\t\tmyoutput = _unicode_decode(proc.communicate()[0]).rstrip(\"\\n\")\n\t\t\tmystatus = proc.wait()\n\t\tif mystatus == os.EX_OK and myoutput.startswith(chost + \"-\"):\n\t\t\treturn myoutput.replace(chost + \"-\", gcc_ver_prefix, 1)\n\n\t\ttry:\n\t\t\tproc = subprocess.Popen(\n\t\t\t\t[chost + \"-\" + gcc_ver_command[0]] + gcc_ver_command[1:],\n\t\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\t\texcept OSError:\n\t\t\tmyoutput = None\n\t\t\tmystatus = 1\n\t\telse:\n\t\t\tmyoutput = _unicode_decode(proc.communicate()[0]).rstrip(\"\\n\")\n\t\t\tmystatus = proc.wait()\n\t\tif mystatus == os.EX_OK:\n\t\t\treturn gcc_ver_prefix + myoutput\n\n\ttry:\n\t\tproc = subprocess.Popen(gcc_ver_command,\n\t\t\tstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\texcept OSError:\n\t\tmyoutput = None\n\t\tmystatus = 1\n\telse:\n\t\tmyoutput = _unicode_decode(proc.communicate()[0]).rstrip(\"\\n\")\n\t\tmystatus = proc.wait()\n\tif mystatus == os.EX_OK:\n\t\treturn gcc_ver_prefix + myoutput\n\n\tportage.writemsg(gcc_not_found_error, noiselevel=-1)\n\treturn \"[unavailable]\"\n\n# Warn about features that may confuse users and\n# lead them to report invalid bugs.\n_emerge_features_warn = frozenset(['keeptemp', 'keepwork'])\n\ndef validate_ebuild_environment(trees):\n\tfeatures_warn = set()\n\tfor myroot in trees:\n\t\tsettings = trees[myroot][\"vartree\"].settings\n\t\tsettings.validate()\n\t\tfeatures_warn.update(\n\t\t\t_emerge_features_warn.intersection(settings.features))\n\n\tif features_warn:\n\t\tmsg = \"WARNING: The FEATURES variable contains one \" + \\\n\t\t\t\"or more values that should be disabled under \" + \\\n\t\t\t\"normal circumstances: %s\" % \" \".join(features_warn)\n\t\tout = portage.output.EOutput()\n\t\tfor line in textwrap.wrap(msg, 65):\n\t\t\tout.ewarn(line)\n\n\tcheck_locale()\n\ndef check_procfs():\n\tprocfs_path = '\/proc'\n\tif platform.system() not in (\"Linux\",) or \\\n\t\tos.path.ismount(procfs_path):\n\t\treturn os.EX_OK\n\tmsg = \"It seems that %s is not mounted. You have been warned.\" % procfs_path\n\twritemsg_level(\"\".join(\"!!! %s\\n\" % l for l in textwrap.wrap(msg, 70)),\n\t\tlevel=logging.ERROR, noiselevel=-1)\n\treturn 1\n\ndef config_protect_check(trees):\n\tfor root, root_trees in trees.items():\n\t\tsettings = root_trees[\"root_config\"].settings\n\t\tif not settings.get(\"CONFIG_PROTECT\"):\n\t\t\tmsg = \"!!! CONFIG_PROTECT is empty\"\n\t\t\tif settings[\"ROOT\"] != \"\/\":\n\t\t\t\tmsg += \" for '%s'\" % root\n\t\t\tmsg += \"\\n\"\n\t\t\twritemsg_level(msg, level=logging.WARN, noiselevel=-1)\n\ndef apply_priorities(settings):\n\tionice(settings)\n\tnice(settings)\n\ndef nice(settings):\n\ttry:\n\t\tos.nice(int(settings.get(\"PORTAGE_NICENESS\", \"0\")))\n\texcept (OSError, ValueError) as e:\n\t\tout = portage.output.EOutput()\n\t\tout.eerror(\"Failed to change nice value to '%s'\" % \\\n\t\t\tsettings[\"PORTAGE_NICENESS\"])\n\t\tout.eerror(\"%s\\n\" % str(e))\n\ndef ionice(settings):\n\n\tionice_cmd = settings.get(\"PORTAGE_IONICE_COMMAND\")\n\tif ionice_cmd:\n\t\tionice_cmd = portage.util.shlex_split(ionice_cmd)\n\tif not ionice_cmd:\n\t\treturn\n\n\tvariables = {\"PID\" : str(os.getpid())}\n\tcmd = [varexpand(x, mydict=variables) for x in ionice_cmd]\n\n\ttry:\n\t\trval = portage.process.spawn(cmd, env=os.environ)\n\texcept portage.exception.CommandNotFound:\n\t\t# The OS kernel probably doesn't support ionice,\n\t\t# so return silently.\n\t\treturn\n\n\tif rval != os.EX_OK:\n\t\tout = portage.output.EOutput()\n\t\tout.eerror(\"PORTAGE_IONICE_COMMAND returned %d\" % (rval,))\n\t\tout.eerror(\"See the make.conf(5) man page for PORTAGE_IONICE_COMMAND usage instructions.\")\n\ndef setconfig_fallback(root_config):\n\tsetconfig = root_config.setconfig\n\tsetconfig._create_default_config()\n\tsetconfig._parse(update=True)\n\troot_config.sets = setconfig.getSets()\n\ndef get_missing_sets(root_config):\n\t# emerge requires existence of \"world\", \"selected\", and \"system\"\n\tmissing_sets = []\n\n\tfor s in (\"selected\", \"system\", \"world\",):\n\t\tif s not in root_config.sets:\n\t\t\tmissing_sets.append(s)\n\n\treturn missing_sets\n\ndef missing_sets_warning(root_config, missing_sets):\n\tif len(missing_sets) > 2:\n\t\tmissing_sets_str = \", \".join('\"%s\"' % s for s in missing_sets[:-1])\n\t\tmissing_sets_str += ', and \"%s\"' % missing_sets[-1]\n\telif len(missing_sets) == 2:\n\t\tmissing_sets_str = '\"%s\" and \"%s\"' % tuple(missing_sets)\n\telse:\n\t\tmissing_sets_str = '\"%s\"' % missing_sets[-1]\n\tmsg = [\"emerge: incomplete set configuration, \" + \\\n\t\t\"missing set(s): %s\" % missing_sets_str]\n\tif root_config.sets:\n\t\tmsg.append(\"        sets defined: %s\" % \", \".join(root_config.sets))\n\tglobal_config_path = portage.const.GLOBAL_CONFIG_PATH\n\tif portage.const.EPREFIX:\n\t\tglobal_config_path = os.path.join(portage.const.EPREFIX,\n\t\t\t\tportage.const.GLOBAL_CONFIG_PATH.lstrip(os.sep))\n\tmsg.append(\"        This usually means that '%s'\" % \\\n\t\t(os.path.join(global_config_path, \"sets\/portage.conf\"),))\n\tmsg.append(\"        is missing or corrupt.\")\n\tmsg.append(\"        Falling back to default world and system set configuration!!!\")\n\tfor line in msg:\n\t\twritemsg_level(line + \"\\n\", level=logging.ERROR, noiselevel=-1)\n\ndef ensure_required_sets(trees):\n\twarning_shown = False\n\tfor root_trees in trees.values():\n\t\tmissing_sets = get_missing_sets(root_trees[\"root_config\"])\n\t\tif missing_sets and not warning_shown:\n\t\t\twarning_shown = True\n\t\t\tmissing_sets_warning(root_trees[\"root_config\"], missing_sets)\n\t\tif missing_sets:\n\t\t\tsetconfig_fallback(root_trees[\"root_config\"])\n\ndef expand_set_arguments(myfiles, myaction, root_config):\n\tretval = os.EX_OK\n\tsetconfig = root_config.setconfig\n\n\tsets = setconfig.getSets()\n\n\t# In order to know exactly which atoms\/sets should be added to the\n\t# world file, the depgraph performs set expansion later. It will get\n\t# confused about where the atoms came from if it's not allowed to\n\t# expand them itself.\n\tdo_not_expand = myaction is None\n\tnewargs = []\n\tfor a in myfiles:\n\t\tif a in (\"system\", \"world\"):\n\t\t\tnewargs.append(SETPREFIX+a)\n\t\telse:\n\t\t\tnewargs.append(a)\n\tmyfiles = newargs\n\tdel newargs\n\tnewargs = []\n\n\t# separators for set arguments\n\tARG_START = \"{\"\n\tARG_END = \"}\"\n\n\tfor i in range(0, len(myfiles)):\n\t\tif myfiles[i].startswith(SETPREFIX):\n\t\t\tstart = 0\n\t\t\tend = 0\n\t\t\tx = myfiles[i][len(SETPREFIX):]\n\t\t\tnewset = \"\"\n\t\t\twhile x:\n\t\t\t\tstart = x.find(ARG_START)\n\t\t\t\tend = x.find(ARG_END)\n\t\t\t\tif start > 0 and start < end:\n\t\t\t\t\tnamepart = x[:start]\n\t\t\t\t\targpart = x[start+1:end]\n\n\t\t\t\t\t# TODO: implement proper quoting\n\t\t\t\t\targs = argpart.split(\",\")\n\t\t\t\t\toptions = {}\n\t\t\t\t\tfor a in args:\n\t\t\t\t\t\tif \"=\" in a:\n\t\t\t\t\t\t\tk, v  = a.split(\"=\", 1)\n\t\t\t\t\t\t\toptions[k] = v\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\toptions[a] = \"True\"\n\t\t\t\t\tsetconfig.update(namepart, options)\n\t\t\t\t\tnewset += (x[:start-len(namepart)]+namepart)\n\t\t\t\t\tx = x[end+len(ARG_END):]\n\t\t\t\telse:\n\t\t\t\t\tnewset += x\n\t\t\t\t\tx = \"\"\n\t\t\tmyfiles[i] = SETPREFIX+newset\n\n\tsets = setconfig.getSets()\n\n\t# display errors that occurred while loading the SetConfig instance\n\tfor e in setconfig.errors:\n\t\tprint(colorize(\"BAD\", \"Error during set creation: %s\" % e))\n\n\tunmerge_actions = (\"unmerge\", \"prune\", \"clean\", \"depclean\", \"rage-clean\")\n\n\tfor a in myfiles:\n\t\tif a.startswith(SETPREFIX):\n\t\t\t\ts = a[len(SETPREFIX):]\n\t\t\t\tif s not in sets:\n\t\t\t\t\tdisplay_missing_pkg_set(root_config, s)\n\t\t\t\t\treturn (None, 1)\n\t\t\t\tif s == \"installed\":\n\t\t\t\t\tmsg = (\"The @installed set is not recommended when \"\n\t\t\t\t\t\t\"updating packages because it will often \"\n\t\t\t\t\t\t\"introduce unsolved blocker conflicts. Please \"\n\t\t\t\t\t\t\"refer to bug #387059 for details.\")\n\t\t\t\t\tout = portage.output.EOutput()\n\t\t\t\t\tfor line in textwrap.wrap(msg, 57):\n\t\t\t\t\t\tout.ewarn(line)\n\t\t\t\tsetconfig.active.append(s)\n\n\t\t\t\tif do_not_expand:\n\t\t\t\t\t# Loading sets can be slow, so skip it here, in order\n\t\t\t\t\t# to allow the depgraph to indicate progress with the\n\t\t\t\t\t# spinner while sets are loading (bug #461412).\n\t\t\t\t\tnewargs.append(a)\n\t\t\t\t\tcontinue\n\n\t\t\t\ttry:\n\t\t\t\t\tset_atoms = setconfig.getSetAtoms(s)\n\t\t\t\texcept portage.exception.PackageSetNotFound as e:\n\t\t\t\t\twritemsg_level((\"emerge: the given set '%s' \" + \\\n\t\t\t\t\t\t\"contains a non-existent set named '%s'.\\n\") % \\\n\t\t\t\t\t\t(s, e), level=logging.ERROR, noiselevel=-1)\n\t\t\t\t\tif s in ('world', 'selected') and \\\n\t\t\t\t\t\tSETPREFIX + e.value in sets['selected']:\n\t\t\t\t\t\twritemsg_level((\"Use `emerge --deselect %s%s` to \"\n\t\t\t\t\t\t\t\"remove this set from world_sets.\\n\") %\n\t\t\t\t\t\t\t(SETPREFIX, e,), level=logging.ERROR,\n\t\t\t\t\t\t\tnoiselevel=-1)\n\t\t\t\t\treturn (None, 1)\n\t\t\t\tif myaction in unmerge_actions and \\\n\t\t\t\t\t\tnot sets[s].supportsOperation(\"unmerge\"):\n\t\t\t\t\twritemsg_level(\"emerge: the given set '%s' does \" % s + \\\n\t\t\t\t\t\t\"not support unmerge operations\\n\",\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\tretval = 1\n\t\t\t\telif not set_atoms:\n\t\t\t\t\twritemsg_level(\"emerge: '%s' is an empty set\\n\" % s,\n\t\t\t\t\t\tlevel=logging.INFO, noiselevel=-1)\n\t\t\t\telse:\n\t\t\t\t\tnewargs.extend(set_atoms)\n\t\t\t\tfor error_msg in sets[s].errors:\n\t\t\t\t\twritemsg_level(\"%s\\n\" % (error_msg,),\n\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\telse:\n\t\t\tnewargs.append(a)\n\treturn (newargs, retval)\n\ndef repo_name_check(trees):\n\tmissing_repo_names = set()\n\tfor root_trees in trees.values():\n\t\tporttree = root_trees.get(\"porttree\")\n\t\tif porttree:\n\t\t\tportdb = porttree.dbapi\n\t\t\tmissing_repo_names.update(portdb.getMissingRepoNames())\n\n\t# Skip warnings about missing repo_name entries for\n\t# \/usr\/local\/portage (see bug #248603).\n\ttry:\n\t\tmissing_repo_names.remove('\/usr\/local\/portage')\n\texcept KeyError:\n\t\tpass\n\n\tif missing_repo_names:\n\t\tmsg = []\n\t\tmsg.append(\"WARNING: One or more repositories \" + \\\n\t\t\t\"have missing repo_name entries:\")\n\t\tmsg.append(\"\")\n\t\tfor p in missing_repo_names:\n\t\t\tmsg.append(\"\\t%s\/profiles\/repo_name\" % (p,))\n\t\tmsg.append(\"\")\n\t\tmsg.extend(textwrap.wrap(\"NOTE: Each repo_name entry \" + \\\n\t\t\t\"should be a plain text file containing a unique \" + \\\n\t\t\t\"name for the repository on the first line.\", 70))\n\t\tmsg.append(\"\\n\")\n\t\twritemsg_level(\"\".join(\"%s\\n\" % l for l in msg),\n\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\treturn bool(missing_repo_names)\n\ndef repo_name_duplicate_check(trees):\n\tignored_repos = {}\n\tfor root, root_trees in trees.items():\n\t\tif 'porttree' in root_trees:\n\t\t\tportdb = root_trees['porttree'].dbapi\n\t\t\tif portdb.settings.get('PORTAGE_REPO_DUPLICATE_WARN') != '0':\n\t\t\t\tfor repo_name, paths in portdb.getIgnoredRepos():\n\t\t\t\t\tk = (root, repo_name, portdb.getRepositoryPath(repo_name))\n\t\t\t\t\tignored_repos.setdefault(k, []).extend(paths)\n\n\tif ignored_repos:\n\t\tmsg = []\n\t\tmsg.append('WARNING: One or more repositories ' + \\\n\t\t\t'have been ignored due to duplicate')\n\t\tmsg.append('  profiles\/repo_name entries:')\n\t\tmsg.append('')\n\t\tfor k in sorted(ignored_repos):\n\t\t\tmsg.append('  %s overrides' % \", \".join(k))\n\t\t\tfor path in ignored_repos[k]:\n\t\t\t\tmsg.append('    %s' % (path,))\n\t\t\tmsg.append('')\n\t\tmsg.extend('  ' + x for x in textwrap.wrap(\n\t\t\t\"All profiles\/repo_name entries must be unique in order \" + \\\n\t\t\t\"to avoid having duplicates ignored. \" + \\\n\t\t\t\"Set PORTAGE_REPO_DUPLICATE_WARN=\\\"0\\\" in \" + \\\n\t\t\t\"\/etc\/portage\/make.conf if you would like to disable this warning.\"))\n\t\tmsg.append(\"\\n\")\n\t\twritemsg_level(''.join('%s\\n' % l for l in msg),\n\t\t\tlevel=logging.WARNING, noiselevel=-1)\n\n\treturn bool(ignored_repos)\n\ndef run_action(emerge_config):\n\n\t# skip global updates prior to sync, since it's called after sync\n\tif emerge_config.action not in ('help', 'info', 'sync', 'version') and \\\n\t\temerge_config.opts.get('--package-moves') != 'n' and \\\n\t\t_global_updates(emerge_config.trees,\n\t\temerge_config.target_config.mtimedb[\"updates\"],\n\t\tquiet=(\"--quiet\" in emerge_config.opts)):\n\t\temerge_config.target_config.mtimedb.commit()\n\t\t# Reload the whole config from scratch.\n\t\tload_emerge_config(emerge_config=emerge_config)\n\n\txterm_titles = \"notitles\" not in \\\n\t\temerge_config.target_config.settings.features\n\tif xterm_titles:\n\t\txtermTitle(\"emerge\")\n\n\tif \"--digest\" in emerge_config.opts:\n\t\tos.environ[\"FEATURES\"] = os.environ.get(\"FEATURES\",\"\") + \" digest\"\n\t\t# Reload the whole config from scratch so that the portdbapi internal\n\t\t# config is updated with new FEATURES.\n\t\tload_emerge_config(emerge_config=emerge_config)\n\n\t# NOTE: adjust_configs() can map options to FEATURES, so any relevant\n\t# options adjustments should be made prior to calling adjust_configs().\n\tif \"--buildpkgonly\" in emerge_config.opts:\n\t\temerge_config.opts[\"--buildpkg\"] = True\n\n\tif \"getbinpkg\" in emerge_config.target_config.settings.features:\n\t\temerge_config.opts[\"--getbinpkg\"] = True\n\n\tif \"--getbinpkgonly\" in emerge_config.opts:\n\t\temerge_config.opts[\"--getbinpkg\"] = True\n\n\tif \"--getbinpkgonly\" in emerge_config.opts:\n\t\temerge_config.opts[\"--usepkgonly\"] = True\n\n\tif \"--getbinpkg\" in emerge_config.opts:\n\t\temerge_config.opts[\"--usepkg\"] = True\n\n\tif \"--usepkgonly\" in emerge_config.opts:\n\t\temerge_config.opts[\"--usepkg\"] = True\n\n\tif \"--buildpkgonly\" in emerge_config.opts:\n\t\t# --buildpkgonly will not merge anything, so\n\t\t# it cancels all binary package options.\n\t\tfor opt in (\"--getbinpkg\", \"--getbinpkgonly\",\n\t\t\t\"--usepkg\", \"--usepkgonly\"):\n\t\t\temerge_config.opts.pop(opt, None)\n\n\tadjust_configs(emerge_config.opts, emerge_config.trees)\n\tapply_priorities(emerge_config.target_config.settings)\n\n\tfor fmt in emerge_config.target_config.settings.get(\"PORTAGE_BINPKG_FORMAT\", \"\").split():\n\t\tif not fmt in portage.const.SUPPORTED_BINPKG_FORMATS:\n\t\t\tif \"--pkg-format\" in emerge_config.opts:\n\t\t\t\tproblematic=\"--pkg-format\"\n\t\t\telse:\n\t\t\t\tproblematic=\"PORTAGE_BINPKG_FORMAT\"\n\n\t\t\twritemsg_level((\"emerge: %s is not set correctly. Format \" + \\\n\t\t\t\t\"'%s' is not supported.\\n\") % (problematic, fmt),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\treturn 1\n\n\tif emerge_config.action == 'version':\n\t\twritemsg_stdout(getportageversion(\n\t\t\temerge_config.target_config.settings[\"PORTDIR\"],\n\t\t\tNone,\n\t\t\temerge_config.target_config.settings.profile_path,\n\t\t\temerge_config.target_config.settings.get(\"CHOST\"),\n\t\t\temerge_config.target_config.trees['vartree'].dbapi) + '\\n',\n\t\t\tnoiselevel=-1)\n\t\treturn 0\n\telif emerge_config.action == 'help':\n\t\temerge_help()\n\t\treturn 0\n\n\tspinner = stdout_spinner()\n\tif \"candy\" in emerge_config.target_config.settings.features:\n\t\tspinner.update = spinner.update_scroll\n\n\tif \"--quiet\" not in emerge_config.opts:\n\t\tportage.deprecated_profile_check(\n\t\t\tsettings=emerge_config.target_config.settings)\n\t\trepo_name_check(emerge_config.trees)\n\t\trepo_name_duplicate_check(emerge_config.trees)\n\t\tconfig_protect_check(emerge_config.trees)\n\tcheck_procfs()\n\n\tfor mytrees in emerge_config.trees.values():\n\t\tmydb = mytrees[\"porttree\"].dbapi\n\t\t# Freeze the portdbapi for performance (memoize all xmatch results).\n\t\tmydb.freeze()\n\n\t\tif emerge_config.action in ('search', None) and \\\n\t\t\t\"--usepkg\" in emerge_config.opts:\n\t\t\t# Populate the bintree with current --getbinpkg setting.\n\t\t\t# This needs to happen before expand_set_arguments(), in case\n\t\t\t# any sets use the bintree.\n\t\t\ttry:\n\t\t\t\tmytrees[\"bintree\"].populate(\n\t\t\t\t\tgetbinpkgs=\"--getbinpkg\" in emerge_config.opts)\n\t\t\texcept ParseError as e:\n\t\t\t\twritemsg(\"\\n\\n!!!%s.\\nSee make.conf(5) for more info.\\n\"\n\t\t\t\t\t\t % e, noiselevel=-1)\n\t\t\t\treturn 1\n\n\tdel mytrees, mydb\n\n\tfor x in emerge_config.args:\n\t\tif x.endswith((\".ebuild\", \".tbz2\")) and \\\n\t\t\tos.path.exists(os.path.abspath(x)):\n\t\t\tprint(colorize(\"BAD\", \"\\n*** emerging by path is broken \"\n\t\t\t\t\"and may not always work!!!\\n\"))\n\t\t\tbreak\n\n\tif emerge_config.action == \"list-sets\":\n\t\twritemsg_stdout(\"\".join(\"%s\\n\" % s for s in\n\t\t\tsorted(emerge_config.target_config.sets)))\n\t\treturn os.EX_OK\n\telif emerge_config.action == \"check-news\":\n\t\tnews_counts = count_unread_news(\n\t\t\temerge_config.target_config.trees[\"porttree\"].dbapi,\n\t\t\temerge_config.target_config.trees[\"vartree\"].dbapi)\n\t\tif any(news_counts.values()):\n\t\t\tdisplay_news_notifications(news_counts)\n\t\telif \"--quiet\" not in emerge_config.opts:\n\t\t\tprint(\"\", colorize(\"GOOD\", \"*\"), \"No news items were found.\")\n\t\treturn os.EX_OK\n\n\tensure_required_sets(emerge_config.trees)\n\n\tif emerge_config.action is None and \\\n\t\t\"--resume\" in emerge_config.opts and emerge_config.args:\n\t\twritemsg(\"emerge: unexpected argument(s) for --resume: %s\\n\" %\n\t\t   \" \".join(emerge_config.args), noiselevel=-1)\n\t\treturn 1\n\n\t# only expand sets for actions taking package arguments\n\toldargs = emerge_config.args[:]\n\tif emerge_config.action in (\"clean\", \"config\", \"depclean\",\n\t\t\"info\", \"prune\", \"unmerge\", \"rage-clean\", None):\n\t\tnewargs, retval = expand_set_arguments(\n\t\t\temerge_config.args, emerge_config.action,\n\t\t\temerge_config.target_config)\n\t\tif retval != os.EX_OK:\n\t\t\treturn retval\n\n\t\t# Need to handle empty sets specially, otherwise emerge will react\n\t\t# with the help message for empty argument lists\n\t\tif oldargs and not newargs:\n\t\t\tprint(\"emerge: no targets left after set expansion\")\n\t\t\treturn 0\n\n\t\temerge_config.args = newargs\n\n\tif \"--tree\" in emerge_config.opts and \\\n\t\t\"--columns\" in emerge_config.opts:\n\t\tprint(\"emerge: can't specify both of \\\"--tree\\\" and \\\"--columns\\\".\")\n\t\treturn 1\n\n\tif '--emptytree' in emerge_config.opts and \\\n\t\t'--noreplace' in emerge_config.opts:\n\t\twritemsg_level(\"emerge: can't specify both of \" + \\\n\t\t\t\"\\\"--emptytree\\\" and \\\"--noreplace\\\".\\n\",\n\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\treturn 1\n\n\tif (\"--quiet\" in emerge_config.opts):\n\t\tspinner.update = spinner.update_quiet\n\t\tportage.util.noiselimit = -1\n\n\tif \"--fetch-all-uri\" in emerge_config.opts:\n\t\temerge_config.opts[\"--fetchonly\"] = True\n\n\tif \"--skipfirst\" in emerge_config.opts and \\\n\t\t\"--resume\" not in emerge_config.opts:\n\t\temerge_config.opts[\"--resume\"] = True\n\n\t# Allow -p to remove --ask\n\tif \"--pretend\" in emerge_config.opts:\n\t\temerge_config.opts.pop(\"--ask\", None)\n\n\t# forbid --ask when not in a terminal\n\t# note: this breaks `emerge --ask | tee logfile`, but that doesn't work anyway.\n\tif (\"--ask\" in emerge_config.opts) and (not sys.stdin.isatty()):\n\t\tportage.writemsg(\"!!! \\\"--ask\\\" should only be used in a terminal. Exiting.\\n\",\n\t\t\tnoiselevel=-1)\n\t\treturn 1\n\n\tif emerge_config.target_config.settings.get(\"PORTAGE_DEBUG\", \"\") == \"1\":\n\t\tspinner.update = spinner.update_quiet\n\t\tportage.util.noiselimit = 0\n\t\tif \"python-trace\" in emerge_config.target_config.settings.features:\n\t\t\tportage.debug.set_trace(True)\n\n\tif not (\"--quiet\" in emerge_config.opts):\n\t\tif '--nospinner' in emerge_config.opts or \\\n\t\t\temerge_config.target_config.settings.get('TERM') == 'dumb' or \\\n\t\t\tnot sys.stdout.isatty():\n\t\t\tspinner.update = spinner.update_basic\n\n\tif \"--debug\" in emerge_config.opts:\n\t\tprint(\"myaction\", emerge_config.action)\n\t\tprint(\"myopts\", emerge_config.opts)\n\n\tif not emerge_config.action and not emerge_config.args and \\\n\t\t\"--resume\" not in emerge_config.opts:\n\t\temerge_help()\n\t\treturn 1\n\n\tpretend = \"--pretend\" in emerge_config.opts\n\tfetchonly = \"--fetchonly\" in emerge_config.opts or \\\n\t\t\"--fetch-all-uri\" in emerge_config.opts\n\tbuildpkgonly = \"--buildpkgonly\" in emerge_config.opts\n\n\t# check if root user is the current user for the actions where emerge needs this\n\tif portage.data.secpass < 2:\n\t\t# We've already allowed \"--version\" and \"--help\" above.\n\t\tif \"--pretend\" not in emerge_config.opts and \\\n\t\t\temerge_config.action not in (\"search\", \"info\"):\n\t\t\tneed_superuser = emerge_config.action in ('clean', 'depclean',\n\t\t\t\t'deselect', 'prune', 'unmerge', \"rage-clean\") or not \\\n\t\t\t\t(fetchonly or \\\n\t\t\t\t(buildpkgonly and portage.data.secpass >= 1) or \\\n\t\t\t\temerge_config.action in (\"metadata\", \"regen\", \"sync\"))\n\t\t\tif portage.data.secpass < 1 or \\\n\t\t\t\tneed_superuser:\n\t\t\t\tif need_superuser:\n\t\t\t\t\taccess_desc = \"superuser\"\n\t\t\t\telse:\n\t\t\t\t\taccess_desc = \"portage group\"\n\t\t\t\t# Always show portage_group_warning() when only portage group\n\t\t\t\t# access is required but the user is not in the portage group.\n\t\t\t\tif \"--ask\" in emerge_config.opts:\n\t\t\t\t\twritemsg_stdout(\"This action requires %s access...\\n\" % \\\n\t\t\t\t\t\t(access_desc,), noiselevel=-1)\n\t\t\t\t\tif portage.data.secpass < 1 and not need_superuser:\n\t\t\t\t\t\tportage.data.portage_group_warning()\n\t\t\t\t\tuq = UserQuery(emerge_config.opts)\n\t\t\t\t\tif uq.query(\"Would you like to add --pretend to options?\",\n\t\t\t\t\t\t\"--ask-enter-invalid\" in emerge_config.opts) == \"No\":\n\t\t\t\t\t\treturn 128 + signal.SIGINT\n\t\t\t\t\temerge_config.opts[\"--pretend\"] = True\n\t\t\t\t\temerge_config.opts.pop(\"--ask\")\n\t\t\t\telse:\n\t\t\t\t\tsys.stderr.write((\"emerge: %s access is required\\n\") \\\n\t\t\t\t\t\t% access_desc)\n\t\t\t\t\tif portage.data.secpass < 1 and not need_superuser:\n\t\t\t\t\t\tportage.data.portage_group_warning()\n\t\t\t\t\treturn 1\n\n\t# Disable emergelog for everything except build or unmerge operations.\n\t# This helps minimize parallel emerge.log entries that can confuse log\n\t# parsers like genlop.\n\tdisable_emergelog = False\n\tfor x in (\"--pretend\", \"--fetchonly\", \"--fetch-all-uri\"):\n\t\tif x in emerge_config.opts:\n\t\t\tdisable_emergelog = True\n\t\t\tbreak\n\tif disable_emergelog:\n\t\tpass\n\telif emerge_config.action in (\"search\", \"info\"):\n\t\tdisable_emergelog = True\n\telif portage.data.secpass < 1:\n\t\tdisable_emergelog = True\n\n\timport _emerge.emergelog\n\t_emerge.emergelog._disable = disable_emergelog\n\n\tif not disable_emergelog:\n\t\temerge_log_dir = \\\n\t\t\temerge_config.target_config.settings.get('EMERGE_LOG_DIR')\n\t\tif emerge_log_dir:\n\t\t\ttry:\n\t\t\t\t# At least the parent needs to exist for the lock file.\n\t\t\t\tportage.util.ensure_dirs(emerge_log_dir)\n\t\t\texcept portage.exception.PortageException as e:\n\t\t\t\twritemsg_level(\"!!! Error creating directory for \" + \\\n\t\t\t\t\t\"EMERGE_LOG_DIR='%s':\\n!!! %s\\n\" % \\\n\t\t\t\t\t(emerge_log_dir, e),\n\t\t\t\t\tnoiselevel=-1, level=logging.ERROR)\n\t\t\t\tportage.util.ensure_dirs(_emerge.emergelog._emerge_log_dir)\n\t\t\telse:\n\t\t\t\t_emerge.emergelog._emerge_log_dir = emerge_log_dir\n\t\telse:\n\t\t\t_emerge.emergelog._emerge_log_dir = os.path.join(os.sep,\n\t\t\t\tportage.const.EPREFIX.lstrip(os.sep), \"var\", \"log\")\n\t\t\tportage.util.ensure_dirs(_emerge.emergelog._emerge_log_dir)\n\n\tif not \"--pretend\" in emerge_config.opts:\n\t\ttime_fmt = \"%b %d, %Y %H:%M:%S\"\n\t\tif sys.hexversion < 0x3000000:\n\t\t\ttime_fmt = portage._unicode_encode(time_fmt)\n\t\ttime_str = time.strftime(time_fmt, time.localtime(time.time()))\n\t\t# Avoid potential UnicodeDecodeError in Python 2, since strftime\n\t\t# returns bytes in Python 2, and %b may contain non-ascii chars.\n\t\ttime_str = _unicode_decode(time_str,\n\t\t\tencoding=_encodings['content'], errors='replace')\n\t\temergelog(xterm_titles, \"Started emerge on: %s\" % time_str)\n\t\tmyelogstr=\"\"\n\t\tif emerge_config.opts:\n\t\t\topt_list = []\n\t\t\tfor opt, arg in emerge_config.opts.items():\n\t\t\t\tif arg is True:\n\t\t\t\t\topt_list.append(opt)\n\t\t\t\telif isinstance(arg, list):\n\t\t\t\t\t# arguments like --exclude that use 'append' action\n\t\t\t\t\tfor x in arg:\n\t\t\t\t\t\topt_list.append(\"%s=%s\" % (opt, x))\n\t\t\t\telse:\n\t\t\t\t\topt_list.append(\"%s=%s\" % (opt, arg))\n\t\t\tmyelogstr=\" \".join(opt_list)\n\t\tif emerge_config.action:\n\t\t\tmyelogstr += \" --\" + emerge_config.action\n\t\tif oldargs:\n\t\t\tmyelogstr += \" \" + \" \".join(oldargs)\n\t\temergelog(xterm_titles, \" *** emerge \" + myelogstr)\n\n\toldargs = None\n\n\tdef emergeexitsig(signum, frame):\n\t\tsignal.signal(signal.SIGTERM, signal.SIG_IGN)\n\t\tportage.util.writemsg(\n\t\t\t\"\\n\\nExiting on signal %(signal)s\\n\" % {\"signal\":signum})\n\t\tsys.exit(128 + signum)\n\n\tsignal.signal(signal.SIGTERM, emergeexitsig)\n\n\tdef emergeexit():\n\t\t\"\"\"This gets out final log message in before we quit.\"\"\"\n\t\tif \"--pretend\" not in emerge_config.opts:\n\t\t\temergelog(xterm_titles, \" *** terminating.\")\n\t\tif xterm_titles:\n\t\t\txtermTitleReset()\n\tportage.atexit_register(emergeexit)\n\n\tif emerge_config.action in (\"config\", \"metadata\", \"regen\", \"sync\"):\n\t\tif \"--pretend\" in emerge_config.opts:\n\t\t\tsys.stderr.write((\"emerge: The '%s' action does \" + \\\n\t\t\t\t\"not support '--pretend'.\\n\") % emerge_config.action)\n\t\t\treturn 1\n\n\tif \"sync\" == emerge_config.action:\n\t\treturn action_sync(emerge_config)\n\telif \"metadata\" == emerge_config.action:\n\t\taction_metadata(emerge_config.target_config.settings,\n\t\t\temerge_config.target_config.trees['porttree'].dbapi,\n\t\t\temerge_config.opts)\n\telif emerge_config.action==\"regen\":\n\t\tvalidate_ebuild_environment(emerge_config.trees)\n\t\treturn action_regen(emerge_config.target_config.settings,\n\t\t\temerge_config.target_config.trees['porttree'].dbapi,\n\t\t\temerge_config.opts.get(\"--jobs\"),\n\t\t\temerge_config.opts.get(\"--load-average\"))\n\t# HELP action\n\telif \"config\" == emerge_config.action:\n\t\tvalidate_ebuild_environment(emerge_config.trees)\n\t\taction_config(emerge_config.target_config.settings,\n\t\t\temerge_config.trees, emerge_config.opts, emerge_config.args)\n\n\t# SEARCH action\n\telif \"search\" == emerge_config.action:\n\t\tvalidate_ebuild_environment(emerge_config.trees)\n\t\taction_search(emerge_config.target_config,\n\t\t\temerge_config.opts, emerge_config.args, spinner)\n\n\telif emerge_config.action in \\\n\t\t('clean', 'depclean', 'deselect', 'prune', 'unmerge', 'rage-clean'):\n\t\tvalidate_ebuild_environment(emerge_config.trees)\n\t\trval = action_uninstall(emerge_config.target_config.settings,\n\t\t\temerge_config.trees, emerge_config.target_config.mtimedb[\"ldpath\"],\n\t\t\temerge_config.opts, emerge_config.action,\n\t\t\temerge_config.args, spinner)\n\t\tif not (emerge_config.action == 'deselect' or\n\t\t\tbuildpkgonly or fetchonly or pretend):\n\t\t\tpost_emerge(emerge_config.action, emerge_config.opts,\n\t\t\t\temerge_config.args, emerge_config.target_config.root,\n\t\t\t\temerge_config.trees, emerge_config.target_config.mtimedb, rval)\n\t\treturn rval\n\n\telif emerge_config.action == 'info':\n\n\t\t# Ensure atoms are valid before calling unmerge().\n\t\tvardb = emerge_config.target_config.trees['vartree'].dbapi\n\t\tportdb = emerge_config.target_config.trees['porttree'].dbapi\n\t\tbindb = emerge_config.target_config.trees['bintree'].dbapi\n\t\tvalid_atoms = []\n\t\tfor x in emerge_config.args:\n\t\t\tif is_valid_package_atom(x, allow_repo=True):\n\t\t\t\ttry:\n\t\t\t\t\t#look at the installed files first, if there is no match\n\t\t\t\t\t#look at the ebuilds, since EAPI 4 allows running pkg_info\n\t\t\t\t\t#on non-installed packages\n\t\t\t\t\tvalid_atom = dep_expand(x, mydb=vardb)\n\t\t\t\t\tif valid_atom.cp.split(\"\/\")[0] == \"null\":\n\t\t\t\t\t\tvalid_atom = dep_expand(x, mydb=portdb)\n\n\t\t\t\t\tif valid_atom.cp.split(\"\/\")[0] == \"null\" and \\\n\t\t\t\t\t\t\"--usepkg\" in emerge_config.opts:\n\t\t\t\t\t\tvalid_atom = dep_expand(x, mydb=bindb)\n\n\t\t\t\t\tvalid_atoms.append(valid_atom)\n\n\t\t\t\texcept portage.exception.AmbiguousPackageName as e:\n\t\t\t\t\tmsg = \"The short ebuild name \\\"\" + x + \\\n\t\t\t\t\t\t\"\\\" is ambiguous.  Please specify \" + \\\n\t\t\t\t\t\t\"one of the following \" + \\\n\t\t\t\t\t\t\"fully-qualified ebuild names instead:\"\n\t\t\t\t\tfor line in textwrap.wrap(msg, 70):\n\t\t\t\t\t\twritemsg_level(\"!!! %s\\n\" % (line,),\n\t\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\tfor i in e.args[0]:\n\t\t\t\t\t\twritemsg_level(\"    %s\\n\" % colorize(\"INFORM\", i),\n\t\t\t\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\t\t\twritemsg_level(\"\\n\", level=logging.ERROR, noiselevel=-1)\n\t\t\t\t\treturn 1\n\t\t\t\tcontinue\n\t\t\tmsg = []\n\t\t\tmsg.append(\"'%s' is not a valid package atom.\" % (x,))\n\t\t\tmsg.append(\"Please check ebuild(5) for full details.\")\n\t\t\twritemsg_level(\"\".join(\"!!! %s\\n\" % line for line in msg),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\treturn 1\n\n\t\treturn action_info(emerge_config.target_config.settings,\n\t\t\temerge_config.trees, emerge_config.opts, valid_atoms)\n\n\t# \"update\", \"system\", or just process files:\n\telse:\n\t\tvalidate_ebuild_environment(emerge_config.trees)\n\n\t\tfor x in emerge_config.args:\n\t\t\tif x.startswith(SETPREFIX) or \\\n\t\t\t\tis_valid_package_atom(x, allow_repo=True):\n\t\t\t\tcontinue\n\t\t\tif x[:1] == os.sep:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tos.lstat(x)\n\t\t\t\tcontinue\n\t\t\texcept OSError:\n\t\t\t\tpass\n\t\t\tmsg = []\n\t\t\tmsg.append(\"'%s' is not a valid package atom.\" % (x,))\n\t\t\tmsg.append(\"Please check ebuild(5) for full details.\")\n\t\t\twritemsg_level(\"\".join(\"!!! %s\\n\" % line for line in msg),\n\t\t\t\tlevel=logging.ERROR, noiselevel=-1)\n\t\t\treturn 1\n\n\t\t# GLEP 42 says to display news *after* an emerge --pretend\n\t\tif \"--pretend\" not in emerge_config.opts:\n\t\t\tuq = UserQuery(emerge_config.opts)\n\t\t\tif display_news_notification(emerge_config.target_config,\n\t\t\t\t\t\t\t\temerge_config.opts) \\\n\t\t\t\tand \"--ask\" in emerge_config.opts \\\n\t\t\t\tand \"--read-news\" in emerge_config.opts \\\n\t\t\t\tand uq.query(\"Would you like to read the news items while \" \\\n\t\t\t\t\t\t\"calculating dependencies?\",\n\t\t\t\t\t\t'--ask-enter-invalid' in emerge_config.opts) == \"Yes\":\n\t\t\t\ttry:\n\t\t\t\t\tsubprocess.call(['eselect', 'news', 'read'])\n\t\t\t\t# If eselect is not installed, Python <3.3 will throw an\n\t\t\t\t# OSError. >=3.3 will throw a FileNotFoundError, which is a\n\t\t\t\t# subclass of OSError.\n\t\t\t\texcept OSError:\n\t\t\t\t\twritemsg(\"Please install eselect to use this feature.\\n\",\n\t\t\t\t\t\t\tnoiselevel=-1)\n\t\tretval = action_build(emerge_config, spinner=spinner)\n\t\tpost_emerge(emerge_config.action, emerge_config.opts,\n\t\t\temerge_config.args, emerge_config.target_config.root,\n\t\t\temerge_config.trees, emerge_config.target_config.mtimedb, retval)\n\n\t\treturn retval\n","license":"gpl-2.0","hash":-2494895931116796229,"line_mean":31.6491120637,"line_max":93,"alpha_frac":0.6702959712,"autogenerated":false},
{"repo_name":"transifex\/openformats","path":"openformats\/tests\/formats\/common\/__init__.py","copies":"1","size":"3781","content":"import fnmatch\nimport six\nfrom io import open\n\nfrom os import listdir, path\nfrom os.path import isfile, join\n\nfrom openformats.exceptions import ParseError\nfrom openformats.tests.utils import translate_stringset\n\n\nclass CommonFormatTestMixin(object):\n    \"\"\"\n    Define a set of tests to be run by every file format.\n\n    The class that inherits from this must define the following:\n\n    * ``HANDLER_CLASS``, eg: PlaintextHandler\n    * ``TESTFILE_BASE``, eg: `openformats\/tests\/formats\/plaintext\/files`\n    \"\"\"\n\n    TESTFILE_BASE = None\n    HANDLER_CLASS = None\n\n    def __init__(self, *args, **kwargs):\n        self.data = {}\n        super(CommonFormatTestMixin, self).__init__(*args, **kwargs)\n\n    def read_files(self, ftypes=('en', 'el', 'tpl')):\n        \"\"\"\n        Read test data files into variables.\n\n        Example: 1_en.txt stored into self.data[\"1_en\"]\n        \"\"\"\n\n        # Find source files to use as a base to read all others\n        en_files = []\n        for f in listdir(self.TESTFILE_BASE):\n            if (isfile(join(self.TESTFILE_BASE, f)) and\n                    fnmatch.fnmatch(f, '[!.]*_en.*')):\n                en_files.append(f)\n\n        file_nums = set([f.split(\"_\")[0] for f in en_files])\n        for num in file_nums:\n            for ftype in ftypes:\n                name = \"%s_%s\" % (num, ftype)  # 1_en, 1_fr etc.\n                filepath = path.join(self.TESTFILE_BASE, \"%s.%s\" % (\n                    name, self.HANDLER_CLASS.extension))\n                if not isfile(filepath):\n                    self.fail(\"Bad test files: Expected to find %s\" % filepath)\n                with open(filepath, \"r\", encoding='utf-8') as myfile:\n                    self.data[name] = myfile.read()\n\n    def setUp(self):\n        self.handler = self.HANDLER_CLASS()\n        self.read_files()\n        self.tmpl, self.strset = self.handler.parse(self.data[\"1_en\"])\n        super(CommonFormatTestMixin, self).setUp()\n\n    def test_extracts_raw(self):\n        if self.HANDLER_CLASS.EXTRACTS_RAW:\n            self.assertTrue(hasattr(self.HANDLER_CLASS, 'escape'))\n            self.assertTrue(hasattr(self.HANDLER_CLASS, 'unescape'))\n\n    def test_template(self):\n        \"\"\"Test that the template created is the same as static one.\"\"\"\n        # FIXME: Test descriptions should have the handler's name prefixed to\n        # be able to differentiate between them.\n        self.assertEqual(self.tmpl, self.data[\"1_tpl\"])\n\n    def test_no_empty_strings_in_handler_stringset(self):\n        for s in self.strset:\n            self.assertFalse(s.string == '')\n\n    def test_compile(self):\n        \"\"\"Test that import-export is the same as the original file.\"\"\"\n        remade_orig_content = self.handler.compile(self.tmpl, self.strset)\n        self.assertEqual(remade_orig_content, self.data[\"1_en\"])\n\n    def test_translate(self):\n        \"\"\"Test that translate + export is the same as the precompiled file.\"\"\"\n        translated_strset = translate_stringset(self.strset)\n        translated_content = self.handler.compile(self.tmpl, translated_strset)\n        self.assertEqual(translated_content, self.data[\"1_el\"])\n\n    def _test_parse_error(self, source, error_msg, parse_kwargs=None):\n        \"\"\"\n        Test that trying to parse 'source' raises an error with a message\n        exactly like 'error_msg'\n        \"\"\"\n        parse_kwargs = parse_kwargs if parse_kwargs is not None else {}\n        exception = None\n        try:\n            self.handler.parse(source, **parse_kwargs)\n        except ParseError as e:\n            exception = six.text_type(e)\n        except Exception:\n            raise AssertionError(\"Did not raise ParseError\")\n        else:\n            raise AssertionError(\"Did not raise '{}'\".format(error_msg))\n        self.assertEqual(exception, error_msg)\n","license":"gpl-3.0","hash":4147000938901387546,"line_mean":36.4356435644,"line_max":79,"alpha_frac":0.6085691616,"autogenerated":false},
{"repo_name":"corso-python-prato\/share-system-team2","path":"client\/test_connection_manager.py","copies":"1","size":"21870","content":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nfrom connection_manager import ConnectionManager\nimport os\nimport json\nimport httpretty\nimport time\nimport shutil\nimport urllib\n\n# API:\n# - GET \/diffs, con parametro timestamp\n#\n# files:\n# - GET \/files\/<path> - scarica un file\n# - POST \/files\/<path> - crea un file\n# - PUT \/files\/<path> - modifica un file\n# actions:\n#  - POST \/actions\/copy - parametri src, dest\n#  - POST \/actions\/delete - parametro path\n#  - POST \/actions\/move - parametri src, dest\n# ---------\n# shares:\n#  - POST \/shares\/<root_path>\/<user> - crea (se necessario) lo share, e l\u2019utente che \u201cvede\u201d la condivisione\n#  - DELETE \/shares\/<root_path> - elimina del tutto lo share\n#  - DELETE \/shares\/<root_path>\/<user> - elimina l\u2019utente dallo share\n\nTEST_DIR = os.path.join(os.environ['HOME'], 'daemon_test')\nCONFIG_DIR = os.path.join(TEST_DIR, '.PyBox')\nCONFIG_FILEPATH = os.path.join(CONFIG_DIR, 'daemon_config')\nLOCAL_DIR_STATE_FOR_TEST = os.path.join(CONFIG_DIR, 'local_dir_state')\nTEST_SHARING_FOLDER = os.path.join(TEST_DIR, 'test_sharing_folder')\nTEST_SERVER_ADDRESS = 'http:\/\/www.pyboxtest.com'\n\nTEST_CFG = {\n    'local_dir_state_path': LOCAL_DIR_STATE_FOR_TEST,\n    'sharing_path': TEST_SHARING_FOLDER,\n    'cmd_address': 'localhost',\n    'cmd_port': 60001,\n    'api_suffix': '\/API\/V1\/',\n    # no server_address to be sure\n    'server_address': TEST_SERVER_ADDRESS,\n    'user': 'user',\n    'pass': 'pass',\n    'activate': True\n}\n\n\ndef create_environment():\n    if not os.path.exists(TEST_DIR):\n        os.makedirs(CONFIG_DIR)\n\n    with open(CONFIG_FILEPATH, 'w') as f:\n            json.dump(TEST_CFG, f, skipkeys=True, ensure_ascii=True, indent=4)\n\n# Test-user account details\nUSR, PW = 'client_user@mail.com', 'Mail_85'\n\n\ndef make_fake_dir():\n    if os.path.exists(TEST_SHARING_FOLDER):\n        shutil.rmtree(TEST_SHARING_FOLDER)\n    os.makedirs(TEST_SHARING_FOLDER)\n\n    fake_file = os.path.join(TEST_SHARING_FOLDER, 'foo.txt')\n    with open(fake_file, 'w') as f:\n        f.write('foo.txt :)')\n\n\ndef remove_fake_dir():\n    shutil.rmtree(TEST_SHARING_FOLDER)\n\n\nclass TestConnectionManager(unittest.TestCase):\n\n    def setUp(self):\n        httpretty.enable()\n        create_environment()\n        make_fake_dir()\n        with open(CONFIG_FILEPATH, 'r') as fo:\n            self.cfg = json.load(fo)\n\n        self.auth = (self.cfg['user'], self.cfg['pass'])\n        self.base_url = ''.join([self.cfg['server_address'], self.cfg['api_suffix']])\n        self.files_url = ''.join([self.base_url, 'files\/'])\n        self.actions_url = ''.join([self.base_url, 'actions\/'])\n        self.shares_url = ''.join([self.base_url, 'shares\/'])\n        self.user_url = ''.join([self.base_url, 'users\/'])\n\n        self.cm = ConnectionManager(self.cfg)\n\n    def tearDown(self):\n        httpretty.disable()\n        httpretty.reset()\n        remove_fake_dir()\n\n    @httpretty.activate\n    def test_register_user(self):\n        \"\"\"\n        Test register user api:\n        method = POST\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = (USR, PW)\n        url = ''.join((self.user_url, USR))\n        content = 'user created'\n        content_jsoned = json.dumps(content)\n        httpretty.register_uri(httpretty.POST, url, status=200, body=content_jsoned)\n        response = self.cm.do_register(data)\n        self.assertIn('content', response)\n        self.assertEqual(response['content'], content)\n        self.assertTrue(response['successful'])\n\n    @httpretty.activate\n    def test_register_user_with_weak_password(self):\n        \"\"\"\n        Test register user api with weak password:\n        method = POST\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        weak_password = 'Password'\n        data = (USR, weak_password)\n        url = ''.join((self.user_url, USR))\n        content = {'type_of_improvement': 'improvement suggested'}\n        content_jsoned = json.dumps(content)\n        httpretty.register_uri(httpretty.POST, url, status=403, body=content_jsoned)\n        response = self.cm.do_register(data)\n        self.assertIn('improvements', response)\n        self.assertEqual(response['improvements'], content)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_register_user_with_already_existent_user(self):\n        \"\"\"\n        Test register user api with already existent user:\n        method = POST\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = (USR, PW)\n        url = ''.join((self.user_url, USR))\n        # This is the only case where server doesn't send data with the message error\n        httpretty.register_uri(httpretty.POST, url, status=409)\n        response = self.cm.do_register(data)\n        self.assertIn('content', response)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_fail_to_register_user(self):\n        \"\"\"\n        Test failed register request\n        Test activate user api:\n        method = POST\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = (USR, PW)\n        url = ''.join((self.user_url, USR))\n        httpretty.register_uri(httpretty.POST, url, status=500)\n\n        response = self.cm.do_register(data)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_activate_user(self):\n        \"\"\"\n        Test successful activation\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        token = '6c9fb345c317ad1d31ab9d6445d1a820'\n        data = (user, token)\n        url = ''.join((self.user_url, user))\n        answer = 'user activated'\n        answer_jsoned = json.dumps(answer)\n        httpretty.register_uri(httpretty.PUT, url, status=201, body=answer_jsoned)\n\n        response = self.cm.do_activate(data)\n        self.assertIsInstance(response['content'], unicode)\n        self.assertTrue(response['successful'])\n\n    @httpretty.activate\n    def test_activate_user_already_existent(self):\n        \"\"\"\n        Test activate user already existent\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        token = 'bad_token'\n        data = (user, token)\n        url = ''.join((self.user_url, user))\n        httpretty.register_uri(httpretty.PUT, url, status=409)\n\n        response = self.cm.do_activate(data)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_activate_user_not_existent(self):\n        \"\"\"\n        Test activate user not existent\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        token = 'bad_token'\n        data = (user, token)\n        url = ''.join((self.user_url, user))\n        httpretty.register_uri(httpretty.PUT, url, status=404)\n\n        response = self.cm.do_activate(data)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_fail_to_activate_user(self):\n        \"\"\"\n        Test failed activation request\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        token = 'bad_token'\n        data = (user, token)\n        url = ''.join((self.user_url, user))\n        httpretty.register_uri(httpretty.PUT, url, status=500)\n\n        response = self.cm.do_activate(data)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_login_user(self):\n        \"\"\"\n        Test login user api:\n        method = get\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = (USR, PW)\n        url = self.files_url\n        content = {'file1': 'foo.txt', 'file2': 'dir\/foo.txt'}\n        content_jsoned = json.dumps(content)\n        httpretty.register_uri(httpretty.GET, url, status=200, body=content_jsoned)\n        response = self.cm.do_login(data)\n        self.assertIn('content', response)\n        self.assertIsInstance(response['content'], str)\n        self.assertTrue(response['successful'])\n\n    @httpretty.activate\n    def test_login_user_failed(self):\n        \"\"\"\n        Test login user api with weak password:\n        method = GET\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = ('bad_user', 'bad_pass')\n        url = self.files_url\n        httpretty.register_uri(httpretty.GET, url, status=401)\n        response = self.cm.do_login(data)\n        self.assertIn('content', response)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_login_fail_connection(self):\n        \"\"\"\n        Test login user api:\n        method = get\n        resource = <user>\n        data = password=<password>\n        \"\"\"\n        data = (USR, PW)\n        url = self.files_url\n        httpretty.register_uri(httpretty.GET, url, status=400)\n        response = self.cm.do_login(data)\n        self.assertIsInstance(response['content'], str)\n        self.assertFalse(response['successful'])\n\n    @httpretty.activate\n    def test_post_recover_password_not_found(self):\n        \"\"\"\n        Test that if \/users\/<email>\/reset POST == 404 then cm return None\n        \"\"\"\n        # An unknown user (neither registered nor pending) is a resource not found for the server...\n        email = 'unknown.user@gmail.com'\n        url = self.user_url + email + '\/reset'\n        # ...so the server should return a 404:\n        httpretty.register_uri(httpretty.POST, url, status=404)\n        # and the command manager must return None in this case\n        response = self.cm.do_reqrecoverpass(email)\n        self.assertIsNone(response)\n\n    @httpretty.activate\n    def test_post_recover_password_accept(self):\n        \"\"\"\n        Test that if \/users\/<email>\/reset POST == 202 then cm return True\n        \"\"\"\n        email = 'pippo@gmail.com'\n        url = self.user_url + email + '\/reset'\n        httpretty.register_uri(httpretty.POST, url, status=202)\n        response = self.cm.do_reqrecoverpass(email)\n        self.assertTrue(response)\n\n    @httpretty.activate\n    def test_put_recover_password_not_found(self):\n        \"\"\"\n        Test that if \/users\/<email> PUT == 404 then cm return None\n        \"\"\"\n        email = 'pippo@gmail.com'\n        recoverpass_code = os.urandom(16).encode('hex')\n        new_password = 'mynewpass'\n        url = self.user_url + email\n        httpretty.register_uri(httpretty.PUT, url, status=404)\n        data = email, recoverpass_code, new_password\n        response = self.cm.do_recoverpass(data)\n        self.assertFalse(response)\n\n    @httpretty.activate\n    def test_put_recover_password_ok(self):\n        \"\"\"\n        Test that if \/users\/<email> PUT == 200 then cm return True\n        \"\"\"\n        email = 'pippo@gmail.com'\n        recoverpass_code = os.urandom(16).encode('hex')\n        new_password = 'mynewpass'\n        url = self.user_url + email\n        httpretty.register_uri(httpretty.PUT, url, status=200)\n        data = email, recoverpass_code, new_password\n        response = self.cm.do_recoverpass(data)\n        self.assertTrue(response)\n\n    @httpretty.activate\n    def test_addshare(self):\n        \"\"\"\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        shared_folder = 'folder'\n        data = (shared_folder, user)\n        url = ''.join([self.shares_url, shared_folder, '\/', user])\n\n        httpretty.register_uri(httpretty.POST, url, status=200, body='added shared folder')\n        response = self.cm.do_addshare(data)\n        self.assertNotEqual(response, False)\n        self.assertIsInstance(response, unicode)\n\n        httpretty.register_uri(httpretty.POST, url, status=404)\n        self.assertFalse(self.cm.do_addshare(data))\n\n        httpretty.register_uri(httpretty.POST, url, status=409)\n        self.assertFalse(self.cm.do_addshare(data))\n\n    @httpretty.activate\n    def test_removeshare(self):\n        \"\"\"\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        shared_folder = 'folder'\n        data = (shared_folder, )\n        url = ''.join([self.shares_url, shared_folder])\n\n        httpretty.register_uri(httpretty.DELETE, url, status=200, body='share removed')\n        response = self.cm.do_removeshare(data)\n        self.assertNotEqual(response, False)\n        self.assertIsInstance(response, unicode)\n\n        httpretty.register_uri(httpretty.DELETE, url, status=404)\n        self.assertFalse(self.cm.do_removeshare(data))\n\n        httpretty.register_uri(httpretty.DELETE, url, status=409)\n        self.assertFalse(self.cm.do_removeshare(data))\n\n    @httpretty.activate\n    def test_removeshareduser(self):\n        \"\"\"\n        Test activate user api:\n        method = PUT\n        resource = <user>\n        data = activation_code=<token>\n        \"\"\"\n        user = 'mail@mail.it'\n        shared_folder = 'folder'\n        data = (shared_folder, user)\n        url = ''.join([self.shares_url, shared_folder, '\/', user])\n\n        httpretty.register_uri(httpretty.DELETE, url, status=200, body='removed user from share')\n        response = self.cm.do_removeshareduser(data)\n        self.assertNotEqual(response, False)\n        self.assertIsInstance(response, unicode)\n\n        httpretty.register_uri(httpretty.DELETE, url, status=404)\n        self.assertFalse(self.cm.do_removeshareduser(data))\n\n        httpretty.register_uri(httpretty.DELETE, url, status=409)\n        self.assertFalse(self.cm.do_removeshareduser(data))\n\n    # files:\n    @httpretty.activate\n    def test_download_normal_file(self):\n        url = ''.join((self.files_url, 'file.txt'))\n\n        httpretty.register_uri(httpretty.GET, url, status=201)\n        data = {'filepath': 'file.txt'}\n        response = self.cm.do_download(data)\n        self.assertEqual(response['successful'], True)\n\n    @httpretty.activate\n    def test_download_file_not_exists(self):\n        url = ''.join((self.files_url, 'file.tx'))\n\n        httpretty.register_uri(httpretty.GET, url, status=404)\n        data = {'filepath': 'file.tx'}\n        response = self.cm.do_download(data)\n        self.assertEqual(response['successful'], False)\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_do_upload_success(self):\n\n        # prepare fake server\n        url = ''.join((self.files_url, 'foo.txt'))\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.POST, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        # call api\n        response = self.cm.do_upload({'filepath': 'foo.txt', 'md5': 'test_md5'})\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_do_upload_fail(self):\n\n        # prepare fake server\n        url = ''.join((self.files_url, 'foo.txt'))\n        httpretty.register_uri(httpretty.POST, url, status=404,\n                               content_type=\"application\/json\")\n\n        # call api\n        response = self.cm.do_upload({'filepath': 'foo.txt', 'md5': 'test_md5'})\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_encode_of_url_with_strange_char(self):\n        \"\"\"\n        Test the url encode of filename with strange char.\n        I use upload method for example and i expect that httpretty answer at the right URL.\n        \"\"\"\n        # Create the file with strange name\n        strange_filename = 'name%with#strange~char'\n        strange_filepath = os.path.join(TEST_SHARING_FOLDER, strange_filename)\n        with open(strange_filepath, 'w') as f:\n            f.write('file with strange name content')\n\n        # prepare fake server\n        encoded_filename = urllib.quote(strange_filename, self.cm.ENCODER_FILTER)\n        url = ''.join((self.files_url, encoded_filename))\n        print url\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.POST, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        # call api\n        response = self.cm.do_upload({'filepath': strange_filename, 'md5': 'test_md5'})\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    # actions:\n    @httpretty.activate\n    def test_do_move(self):\n        url = ''.join((self.actions_url, 'move'))\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.POST, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_move({'src': 'foo.txt', 'dst': 'folder\/foo.txt'})\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_do_move_fail(self):\n        url = ''.join((self.actions_url, 'move'))\n        httpretty.register_uri(httpretty.POST, url, status=404,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_move({'src': 'foo.txt', 'dst': 'folder\/foo.txt'})\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_do_delete(self):\n        url = ''.join((self.actions_url, 'delete'))\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.POST, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n        d = {'filepath': 'foo.txt'}\n\n        response = self.cm.do_delete(d)\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_do_delete_fail(self):\n        url = ''.join((self.actions_url, 'delete'))\n        httpretty.register_uri(httpretty.POST, url, status=404,\n                               content_type=\"application\/json\")\n        d = {'filepath': 'foo.txt'}\n\n        response = self.cm.do_delete(d)\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_do_modify(self):\n        url = ''.join((self.files_url, 'foo.txt'))\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.PUT, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_modify({'filepath': 'foo.txt', 'md5': 'test_md5'})\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_do_modify_fail(self):\n        url = ''.join((self.files_url, 'foo.txt'))\n        httpretty.register_uri(httpretty.PUT, url, status=404,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_modify({'filepath': 'foo.txt', 'md5': 'test_md5'})\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_do_copy(self):\n        url = ''.join([self.actions_url, 'copy'])\n        d = {'src': 'foo.txt', 'dst': 'folder\/foo.txt'}\n        msg = {'server_timestamp': time.time()}\n        js = json.dumps(msg)\n        httpretty.register_uri(httpretty.POST, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_copy(d)\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_do_copy_fail(self):\n        url = ''.join([self.actions_url, 'copy'])\n        d = {'src': 'foo.txt', 'dst': 'folder\/foo.txt'}\n        httpretty.register_uri(httpretty.POST, url, status=404,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_copy(d)\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\n    @httpretty.activate\n    def test_get_server_snapshot(self):\n        url = self.files_url\n        msg = {'files': 'foo.txt'}\n        js = json.dumps(msg)\n\n        httpretty.register_uri(httpretty.GET, url, status=201,\n                               body=js,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_get_server_snapshot('')\n        self.assertTrue(response['successful'])\n        self.assertEqual(response['content'], msg)\n\n    @httpretty.activate\n    def test_get_server_snapshot_fail(self):\n        url = self.files_url\n\n        httpretty.register_uri(httpretty.GET, url, status=404,\n                               content_type=\"application\/json\")\n\n        response = self.cm.do_get_server_snapshot('')\n        self.assertFalse(response['successful'])\n        self.assertIsInstance(response['content'], str)\n\nif __name__ == '__main__':\n    unittest.main()\n","license":"apache-2.0","hash":3992381976422252841,"line_mean":34.2612903226,"line_max":107,"alpha_frac":0.5960570854,"autogenerated":false},
{"repo_name":"DPRL\/MathSymbolRecognizer","path":"src\/dataset_info.py","copies":"1","size":"4484","content":"\"\"\"\n    DPRL Math Symbol Recognizers \n    Copyright (c) 2012-2014 Kenny Davila, Richard Zanibbi\n\n    This file is part of DPRL Math Symbol Recognizers.\n\n    DPRL Math Symbol Recognizers is free software: you can redistribute it and\/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    DPRL Math Symbol Recognizers is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with DPRL Math Symbol Recognizers.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n    Contact:\n        - Kenny Davila: kxd7282@rit.edu\n        - Richard Zanibbi: rlaz@cs.rit.edu \n\"\"\"\n\nimport sys\nimport math\nimport numpy as np\nfrom dataset_ops import *\n#=====================================================================\n#  Simple script that loads an existing dataset from a file and\n#  outputs the most general information.\n#\n#  Created by:\n#      - Kenny Davila (Feb 7, 2012-2014)\n#  Modified By:\n#      - Kenny Davila (Feb 7, 2012-2014)\n#      - Kenny Davila (Feb 7, 2012-2014)\n#\n#=====================================================================\n\n\ndef main():\n    #usage check\n    if len(sys.argv) < 2:\n        print(\"Usage: python dataset_info.py datset [n_bins]\")\n        print(\"Where\")\n        print(\"\\tdataset\\t= Path to file that contains the data set\")\n        print(\"\\tn_bins\\t= Optional, number of bins for histogram of class representation\")\n        return\n\n    input_filename = sys.argv[1]\n\n    if len(sys.argv) >= 3:\n        try:\n            n_bins = int(sys.argv[2])\n            if n_bins < 1:\n                print(\"Invalid n_bins value\")\n                return\n        except:\n            print(\"Invalid n_bins value\")\n            return\n    else:\n        n_bins = 10\n\n    print(\"Loading data....\")\n    training, labels_l, att_types = load_dataset(input_filename);\n    print(\"Data loaded!\")\n\n    print(\"Getting information...\")\n    n_samples = np.size(training, 0)\n    n_atts = np.size(training, 1)\n\n    #...counts per class...\n    count_per_class = {}\n    for idx in range(n_samples):\n        #s_label = labels_train[idx, 0]\n        s_label = labels_l[idx]\n\n        if s_label in count_per_class:\n            count_per_class[s_label] += 1\n        else:\n            count_per_class[s_label] = 1\n\n    #...distribution...\n    #...first pass, compute minimum and maximum...\n    smallest_class_size = n_samples\n    smallest_class_label = \"\"\n    largest_class_size = 0\n    largest_class_label = \"\"\n    for label in count_per_class:\n        #...check minimum\n        if count_per_class[label] < smallest_class_size:\n            smallest_class_size = count_per_class[label]\n            smallest_class_label = label\n\n        #...check maximum\n        if count_per_class[label] > largest_class_size:\n            largest_class_size = count_per_class[label]\n            largest_class_label = label\n\n        print(\"Class\\t\" + label + \"\\tCount\\t\" + str(count_per_class[label]))\n\n    #...second pass... create histogram...\n    count_bins = [0 for x in range(n_bins)]\n    samples_bins = [0 for x in range(n_bins)]\n    size_per_bin = int(math.ceil(float(largest_class_size + 1) \/ float(n_bins)))\n    for label in count_per_class:\n        current_bin = int(count_per_class[label] \/ size_per_bin)\n        count_bins[current_bin] += 1\n        samples_bins[current_bin] += count_per_class[label]\n\n    #...print... bins...\n    print(\"Class sizes distribution\")\n    for i in range(n_bins):\n        start = i * size_per_bin + 1\n        end = (i + 1) * size_per_bin\n        percentage = (float(samples_bins[i]) \/ float(n_samples)) * 100.0\n        print(\"... From \" + str(start) + \"\\t to \" + str(end) + \"\\t : \" +\n              str(count_bins[i]) + \"\\t (\" + str(percentage) + \" of data)\")\n\n    n_classes = len(count_per_class.keys())\n\n    print(\"Total Samples: \" + str(n_samples))\n    print(\"Total Attributes: \" + str(n_atts))\n    print(\"Total Classes: \" + str(n_classes))\n    print(\"-> Largest Class: \" + largest_class_label + \"\\t: \" + str(largest_class_size) + \" samples\")\n    print(\"-> Smallest Class: \" + smallest_class_label + \"\\t: \" + str(smallest_class_size) + \" samples\")\n    print(\"Finished...\")\n\nmain()\n","license":"gpl-3.0","hash":7726680428416640942,"line_mean":33.4923076923,"line_max":104,"alpha_frac":0.589206066,"autogenerated":false},
{"repo_name":"longmazhanfeng\/interface_web","path":"interface_platform\/model\/testcase.py","copies":"1","size":"1586","content":"# -*- coding: UTF-8 -*-\nfrom interfacebuilder import InterfaceBuilder\n\n\n# \u63a5\u53e3\u6d4b\u8bd5\u7528\u4f8b\u7c7b\nclass TestCase(object):\n    def __init__(self, testcase, logger, its):\n        print \"TestCase.__init__()\"\n        self._testcase = testcase\n        self._logger = logger\n        self._its = its\n        self._status = \"Unexecuted\"\n        self._msg = None\n\n    @property\n    def status(self):\n        return self._status\n\n    @property\n    def msg(self):\n        return self._msg\n\n    # \u8fd0\u884c\u7528\u4f8b\uff0c\u8bb0\u5f55\u6d4b\u8bd5\u65e5\u5fd7\n    # \u4e00\u4e2a\u63a5\u53e3\u51fa\u9519\u5219\u4e2d\u65ad\u6267\u884c\n    def run(self):\n        print \"TestCase.run()\"\n        # self._logger.info(\"\u5f00\u59cb\u6267\u884c\u7528\u4f8b: id=\" + str(self._testcase.id) + \" name=\" + self._testcase.name)\n        for it in self._its:\n            interface_builder = InterfaceBuilder(it, self._logger)\n            interface = interface_builder.build()\n            interface.run()\n            interface.validate()\n            # \u5982\u679c\u63a5\u53e3\u6267\u884c\u5931\u8d25\uff0c\u5219\u7ec8\u6b62\u7528\u4f8b\u6267\u884c\n            if interface.status == \"Fail\":\n                self._status = \"Fail\"\n                break\n            print \"it:\", it.id, \"execute result: \", interface.status\n            # \u8bb0\u5f55\u501f\u53e3\u9519\u8bef\u4fe1\u606f\n            self._msg = interface.error_msg\n            self._logger.info(\"\u63a5\u53e3\u6267\u884c\u72b6\u6001: \" + interface.status)\n            self._logger.info(\"\u63a5\u53e3\u6784\u5efa\u5b8c\u6210\uff01\")\n        if self._status != \"Fail\":\n            self._status = \"Pass\"\n        self._logger.info(\"\u7528\u4f8b\u6267\u884c\u72b6\u6001: \" + self._status)\n        self._logger.info(\"\u7528\u4f8b\u6267\u884c\u5b8c\u6210\uff01\")\n        return self._status\n","license":"mit","hash":214188544792553164,"line_mean":29.7826086957,"line_max":100,"alpha_frac":0.5423728814,"autogenerated":false},
{"repo_name":"centaurialpha\/ninja-ide","path":"ninja_ide\/gui\/theme.py","copies":"1","size":"3170","content":"# -*- coding: utf-8 -*-\n#\n# This file is part of NINJA-IDE (http:\/\/ninja-ide.org).\n#\n# NINJA-IDE is free software; you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# any later version.\n#\n# NINJA-IDE is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with NINJA-IDE; If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n# import yaml\nimport json\nimport os\n\nfrom PyQt5.QtWidgets import QApplication\nfrom PyQt5.QtGui import (\n    QPalette,\n    QColor\n)\nfrom PyQt5.QtCore import QObject\nfrom ninja_ide import resources\nfrom ninja_ide.core.file_handling import file_manager\n\nPALETTE = {}\n\n\nclass ThemeManager(QObject):\n\n    def __init__(self):\n        QObject.__init__(self)\n        self._themes_dir = (\n            resources.NINJA_THEMES,\n            resources.NINJA_THEMES_DOWNLOAD\n        )\n        self.__themes = {}\n\n    def discover_themes(self):\n        for dir_ in self._themes_dir:\n            themes = file_manager.get_files_from_folder(dir_, '.ninjatheme')\n            if themes:\n                for theme in themes:\n                    theme_filename = os.path.join(dir_, theme)\n                    with open(theme_filename) as json_f:\n                        theme_content = json.load(json_f)\n                        theme_name = theme_content['name']\n                        ninja_theme = NTheme(theme_content)\n                        self.__themes[theme_name] = ninja_theme\n\n    def load(self, theme_name):\n        theme = self.__themes[theme_name]\n        pal = theme.get_palette()\n        theme.initialize_colors()\n        QApplication.setPalette(pal)\n\n\nclass NTheme(object):\n\n    __COLORS = {}\n\n    def __init__(self, theme_content_dict):\n        self.name = theme_content_dict['name']\n        self.palette = theme_content_dict['palette']\n        self.colors = theme_content_dict['colors']\n        self._flags = theme_content_dict['flags']\n        self._derive_palette_from_theme = self._flags['PaletteFromTheme']\n\n    def initialize_colors(self):\n        for role, color in self.colors.items():\n            self.__COLORS[role] = QColor(color)\n\n    @classmethod\n    def get_color(cls, role):\n        return cls.__COLORS.get(role)\n\n    @classmethod\n    def get_colors(cls):\n        return cls.__COLORS\n\n    def get_palette(self):\n        if not self._derive_palette_from_theme:\n            return QApplication.palette()\n        palette = QPalette()\n        for role, color in self.palette.items():\n            qcolor = QColor(color)\n            color_group = QPalette.All\n            if role.endswith('Disabled'):\n                role = role.split('Disabled')[0]\n                color_group = QPalette.Disabled\n            color_role = getattr(palette, role, None)\n            palette.setBrush(color_group, color_role, qcolor)\n            PALETTE[role] = color\n        return palette\n","license":"gpl-3.0","hash":-5442366628796231446,"line_mean":31.0202020202,"line_max":76,"alpha_frac":0.614511041,"autogenerated":false},
{"repo_name":"Rumel\/berniemetrics","path":"private\/scrapers\/realclearpolitics-scraper\/realclearpolitics\/spiders\/spider.py","copies":"3","size":"1294","content":"import scrapy\nfrom realclearpolitics.items import TableItem\n\nclass RcpSpider(scrapy.Spider):\n    name = \"realclearpoliticsSpider\"\n    start_urls = []\n    columns = ['Poll','Date', 'Sample', 'Spread']\n\n    def __init__(self, url, extra_fields = {}):\n        self.url = url\n        self.extra_fields = extra_fields\n\n    def start_requests(self):\n        return [scrapy.FormRequest(self.url,\n                               callback=self.parse)]\n\n\n    def parse(self, response):\n        table = response.css('.data').pop()\n        legend = table.css('tr')[0]\n        fieldNames = legend.css('th::text').extract()\n        nb_fields = len(fieldNames)\n        items = []\n\n        contentLines = table.css('tr')[1::]\n\n        for line in contentLines:\n            item = TableItem()\n            item['field'] = {}\n            values = line.css('td::text, td span::text, td a::text').extract()\n            for i in range(nb_fields):\n                if fieldNames[i] in RcpSpider.columns:\n                    item[fieldNames[i]] = values[i]\n                elif values[i] != '--':\n                    item['field'][fieldNames[i]] = values[i]\n\n            for fieldName, value in self.extra_fields.iteritems():\n                item[fieldName] = value\n\n            items.append(item)\n\n        return items\n","license":"mit","hash":6904448572680763380,"line_mean":29.8095238095,"line_max":78,"alpha_frac":0.5363214838,"autogenerated":false},
{"repo_name":"jscontreras\/learning-gae","path":"pgae-examples-master\/2e\/python\/datastore\/commentform\/main.py","copies":"1","size":"1335","content":"from google.appengine.ext import webapp\nfrom google.appengine.ext.webapp.util import run_wsgi_app\nfrom google.appengine.ext import db\nfrom google.appengine.api import users\nimport cgi\nimport datetime\nimport webapp2\n\nclass Comment(db.Expando):\n    pass\n\nclass CommentHandler(webapp2.RequestHandler):\n    def post(self):\n        c = Comment()\n        c.commenter = users.get_current_user()\n        c.message = db.Text(self.request.get('message'))\n        c.date = datetime.datetime.now()\n        c.put()\n\n        self.redirect('\/')\n\nclass CommentFormHandler(webapp2.RequestHandler):\n    def get(self):\n        self.response.out.write('<p>Comments:<\/p><ul>')\n        # Borrowing a bit from chapter 5...\n        for c in Comment.all().order('-date'):\n            self.response.out.write('<li>%s<br \/>posted by %s on %s<\/li>'\n                                    % (cgi.escape(c.message),\n                                       cgi.escape(c.commenter.nickname()),\n                                       c.date))\n        self.response.out.write('<\/ul>')\n\n        self.response.out.write('''<p>Post a comment:<\/p>\n<form action=\"\/post\" method=\"POST\">\n<textarea name=\"message\"><\/textarea><br \/>\n<input type=\"submit\" \/>\n<\/form>\n''')\n\napp = webapp2.WSGIApplication(\n    [('\/', CommentFormHandler),\n     ('\/post', CommentHandler)],\n    debug=True)\n","license":"lgpl-3.0","hash":1374123423848744515,"line_mean":30.0465116279,"line_max":74,"alpha_frac":0.5925093633,"autogenerated":false}]
]